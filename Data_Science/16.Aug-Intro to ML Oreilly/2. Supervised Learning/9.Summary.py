'''
                            Summary and Outlook
'''

'''
우리는 다음과 같은 개념들을 배웠다.
    1. model Complexity

    2. generalization

    3. underfitting

    4. overfitting


또한, 분류와 회귀를 위한 머신 러닝 알고리즘을 배우고, 다음과 같은 사항도 알게 되었다.
    1. 파라미터의 설정이 중요하다.

    2. input data를 잘 선정하는 것이 중요하다.

    3. input data의 전처리(preprocessing, rescaling)도 중요하다.


각 알고리즘을 언제 적용해야할까? 아래 요약이 있다.
    1. Nearest neighbors
        가까운 k개의 이웃으로부터 가장 많은 vote를 얻거나(분류), 그들의 평균(회귀)을 구한다.
        k개에게 동등한 weight을 줄 수도 있지만, 가중치를 둘 수도 있다.

        작은 데이터셋에 좋다.
        기초 알고리즘으로서 좋고 설명하기 쉽다.


    2. Linear models
        처음으로 시도할 모델로서 좋다.
        아주 큰 데이터셋에 좋고, 아주 큰 차원의 데이터에 좋다.


        1) Linear Regression (Ordinary Least Squares, OLS)
        종속 변수 y와 한 개 이상의 독립 변수 X와의 선형 상관 관계를 모델링하는 회귀 분석 기법이다. 선형 회귀는 선형 예측 함수를 사용해 회귀식을 모델링하며, 알려지지 않은 파라미터는 데이터로부터 추정한다.
        OLS는 각 데이터로부터의 거리의 제곱이 최소화되는 선이 모델이 된다.
        || Ax -b || ^2

        그런데, 해가 없거나 유일하지 않은 경우, 그 문제는 불량조건이 되고, OLS는 overfit하거나 underfit한 모델을 만들어낸다. 따라서, 좋은 특성을 지닌 특정한 해를 찾기위해, 정규화 항이 추가된다.
        || Ax -b || ^2 + ||Tx||^2
        여기서 Tikhonov 매트릭스인 T는 identity에 스칼라배를 한 것이다.
        이것을 더해주면 놈이 더 작은 해에게 우선권이 간다. 이것을 L2 정규화라고 한다.

        추가 설명)
        p개의 독립변수를 갖는 데이터에서 회귀분석을 수행할 때 우리는 2^p개의 회귀모형(부분집합)을 고려해보아야 한다. 하지만 이는 시간이 너무 많이 걸린다. 대안으로, 계수추정치를 제한(constraint, regularize)하여 추정치가 0에 가까워지게 함으로써 p개 독립 변수를 모두 포함하는 모형을 만들 수 있다. 이렇게 회귀계수를 수축하는 유명한 방법이 ridge와 lasso이다.
        
        2) Ridge Regression



    3. Naive Bayes
        분류를 위해서만 쓰인다.
        선형 모델보다도 빠르고, 아주 큰 데이터셋과 아주 높은 차원의 데이터에 좋다.
        종종 선형 모델보다 덜 정확하다.

    4. Decision Trees
        아주 빠르고 데이터의 가공이 필요하지 않다.
        시각화와 설명이 쉽다.

    5. Random Forests
        거의 대부분 결정 트리보다 성능이 좋다.
        데이터의 가공이 필요하지 않다.
        고차원의 sparse 데이터셋에 부적합하다.

    6. Gradient Boosted decision trees
        종종 랜덤 포레스트보다 약간 더 성능이 좋다.
        학습이 느리지만 예측이 더 빠르다. (랜덤 포레스트에 비해)
        랜덤 포레스트보다 파라미터 조율이 더 필요하다.

    7. Support Vector Machines
        중간 사이즈의 데이터셋에 적합하다.
        feature들이 비슷한 성격을 띨 때 좋다.
        데이터의 가공이 필요하고, 파라미터에 민감하다.

    8. Neural networks
        아주 복잡한 모델을 만들 수 있다. (특히 큰 데이터셋에)
        데이터의 가공에 민감하고, 파라미터의 선택에 민감하다.
        큰 모델은 훈련에 시간이 오래 걸린다.

이 모델들을 가지고 하나의 데이터셋에 이리저리 맞춰보며 실험해보는 것이 공부하는 데 도움이 된다. scikit-learn에는 자체 데이터셋이 많으므로 해보도록 하자.


새로운 데이터셋을 마주했을 때, 다음의 방법을 따르는 것이 좋다.
    1. 간단한 모델부터 시작한다. 그리고 어느 정도 성능을 내는지 지켜본다.
        -선형 모델, 나이브 베이즈, kNN

    2. 데이터에 대한 이해를 바탕으로, 더 복잡한 모델로 나아간다.
        -랜덤 포레스트, 그래디언트부스팅, SVM, 뉴럴 네트워크

    3. 파라미터와 그 외 변수들을 조율해본다.
'''
