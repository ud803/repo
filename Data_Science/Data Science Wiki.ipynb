{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기 있는 내용들은 모두 직접 번역하고 수집한 자료입니다.\n",
    "# ud803da@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is data/big data?\n",
    "\n",
    "#### `\"데이터는 '정성적'이거나 '정량적'인 변수이며, Set of items에 속하는 것들이다.\"`\n",
    "\n",
    "Set of items : 종종 모집단이라고도 불리며, 아이템의 특성을 의미한다.\n",
    "\n",
    "정성적 데이터 : 출생지, 성별, 종교, ...\n",
    "정량적 데이터 : 체중, 키, 혈압, 나이, ...\n",
    "\n",
    "#### `\"데이터는 2번째로 중요한 것이다\"`\n",
    "- 데이터 사이언스에서 가장 중요한 것은 올바른 질문을 하는 것\n",
    "- 두 번째가  데이터이다\n",
    "- 종종 데이터는 그 질문에 제한을 주거나, 가능하게 해준다\n",
    "- 하지만 올바른 질문이 없으면 데이터가 그 무엇도 해결해주지 않는다\n",
    "\n",
    "#### `\"빅데이터에 대한 정의는 사용자의 관점에 따라 다르다\"`\n",
    "- 기술이 발달함에 따라 빅데이터의 정의는 달라진다\n",
    "\n",
    "#### `\"왜 이제서야 화두로 올랐을까?\"`\n",
    "- 훨씬 많은 데이터를 훨씬 저렴하게 수집하는게 가능해졌다\n",
    "\n",
    "#### `\"하지만 빅데이터 시대에도 그렇게 '큰' 데이터가 아닐 수 있다\"`\n",
    "- http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html\n",
    "\n",
    "#### `\"중요한 사실은 올바른 질문과 올바른 데이터가 필요하다는 것\"`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Not HERE) Experimental Design - 별도의 카테고리로 다룰 \n",
    "\n",
    "\n",
    "#### 한 가지 예를 먼저 살펴보자\n",
    "이는 버락 오바마의 선거 캠프에서 홈페이지의 화면이 후원자들에게 미치는 영향을 측정하 위해 고안된 실험이었다.\n",
    "\n",
    "알고자 하는 것 : 웹사이트의 텍스트를 바꾸는 것이 후원금을 늘리는 데 효과가 있을까?\n",
    "\n",
    "실험 단계 :\n",
    "1. 웹사이트 방문자들에게 임의로 A, B 버전을 보여준다.\n",
    "2. 각 방문자들이 얼마를 후원하는지 측정한다.\n",
    "3. A와 B 중 어떤 것이 나은지 결정한다.\n",
    "\n",
    "실험 방법 :\n",
    "1. 여기서는 통계적 추론 방법이 사용되었다. 모든 사용자들에게 사이트를 보여주면 좋겠지만, 현실적으로 불가능하므로 확률적으로 샘플을 뽑아낸다.\n",
    "\n",
    "2. 그 샘플로부터 기술적 통계량을 추출하는데, 예를 들면 한 후원자가 후원한 금액이나 방문한 횟수, 등이다.\n",
    "\n",
    "3. 그리고 추론적 통계를 이용하여 이러한 통계가 전체 모집단에게도 적용되겠는가를 판단한다.\n",
    "\n",
    "\n",
    "아래는 가능한 결과이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1의 경우, 평균적으로 후원금은 비슷하기 때문에 어떤 결론을 내리기 힘들다.\n",
    "\n",
    "2의 경우, A와 B의 차이가 있긴 하지만, 큰 효과는 없어 보인다.\n",
    "\n",
    "3의 경우, A와 B의 차이가 매우 크기 때문에, 효과가 있다고 말할 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "### Confounding Effect (혼동효과)\n",
    "\n",
    "A와 B 사이의 상관관계를 분석한다고 가정하자.\n",
    "\n",
    "하지만 이 둘 사이에 Confoun Effect를 주는 C 변수가 또 존재한다면, 이는 올바른 설계가 아니다.\n",
    "\n",
    "### Randomization and blocking\n",
    "잠재적 혼동 요소들을 처리하는 방법들이 있다.\n",
    "\n",
    "1. 가능하다면 변수를 고정해라\n",
    "    - A, B 모두 \"같은 텍스트\"를 가져야 한다.\n",
    "2. 변수가 고정되어있지 않다면, 계층화하라\n",
    "    - 만약 두 개의 웹사이트 후보 색상이 있다면, A와 B 모두 사용하되, 두 색상 모두 사용하라.\n",
    "3. 변수를 고정할 수 없고, 계층화도 할 수 없다면, 랜덤화하라 (**Randomization**)\n",
    "\n",
    "\n",
    "### Correlation is not causation\n",
    "\n",
    "인구별 초콜릿 소비량과 노벨 수상자 숫자 간의 선형 관계가 있다는 연구가 있었다.\n",
    "하지만 초콜릿을 더 먹는다는 것은 더 나은 경제 상황과 나아가 교육 상황을 의미할 수 있는 거이다. 따라서 선형 상관관계가 존재하더라도 이는 인과관계가 아니다.\n",
    "http://www.nejm.org/doi/full/10.1056/NEJMon1211064\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction vs. Inference\n",
    "\n",
    "\n",
    "\n",
    "### Prediction Key Quantities\n",
    "\n",
    "Sensitivity -> Pr ( Positive | Disease)\n",
    "Specificity -> Pr ( Negative | No Disease)\n",
    "Positive Predictive -> Pr ( Disease | Positive )\n",
    "Negative Predictive -> Pr ( No Disease | Negative )\n",
    "Accuracy -> Pr ( Correct Outcome )\n",
    "\n",
    "\n",
    "\n",
    "#### 요약\n",
    "\n",
    "1. 좋은 실험은\n",
    "    - 반복하여도 같은 결과가 나와야 하고\n",
    "    - 변동성 (오차의 가능성)을 측정해야 하고\n",
    "    - 일반화 할 수 있어야 하고\n",
    "    - 투명해야 한다\n",
    "2. 예측(Prediction)이 추론(Inference)는 아니다.\n",
    "3. Data Dredging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do data scientists do?\n",
    "\n",
    "- 질문을 정의한다\n",
    "- 이상적인 데이터셋을 정의한다\n",
    "- 어떤 데이터에 접근할 수 있는지 결정한다\n",
    "- 데이터를 얻고\n",
    "- 데이터 클렌징을 하고\n",
    "- 탐험적 데이터 분석을 한다\n",
    "- 통계적 추론 / 모델링을 한다\n",
    "- 결과를 해석하고\n",
    "- 결과를 반박도 해보고\n",
    "- 결과를 취합하며\n",
    "- 재사용가능하도록 코드를 짜고\n",
    "- 결과를 사람들에게 공유한다\n",
    "\n",
    "\n",
    "# Types of Data Science Questions\n",
    "\n",
    "- 기술적 분석(Descriptive Analysis) : 주어진 데이터 셋을 설명(기술)하는 분석\n",
    "    - 데이터가 주어졌을 때 가장 먼저 하는 분석이다\n",
    "    - 인구 통계 데이터에 많이 쓰인다\n",
    "    - 하지만 '기술'과 '해석'은 서로 다른 단계이다\n",
    "    - 추가적인 통계적 모델링 없이 기술은 일반화 될 수 없다\n",
    "    - ex) U.S. Census data, Google books Ngram Viewer\n",
    "<br><br>\n",
    "- 탐험적 분석(Exploratory Analysis) : 이전에 몰랐던 관계를 찾아낸다\n",
    "    - 탐험적 모델은 새로운 연결고리를 찾는 데 유용하다\n",
    "    - 또한 미래의 연구를 찾는 데도 유용하다\n",
    "    - 탐험적 분석이 마지막 단계는 아니다\n",
    "    - 탐험적 분석 자체로는 일반화/예측에 쓰일 수 없다\n",
    "    - 상관관계가 인과관계를 의미하는 것은 아니다\n",
    "    - ex) brain damages and regions of stimulus / sdss (sloan digital sky survey) \n",
    "<br><br>\n",
    "- 추론적 분석(Inferential Analysis) : 비교적 작은 샘플을 이용해 그 모집단에 대해 이야기 하는 것\n",
    "    - 추론은 보통 통계적 모델의 목표이다\n",
    "    - 추론은 알고자 하는 '수량'과 그 추정의 '불확실성' 둘다 포함한다\n",
    "    - 추론은 모집단과 샘플링 방법에 강하게 의존한다\n",
    "    - ex) Effect of airpollution to ~ \n",
    "<br><br>\n",
    "- 예측 분석(Predictive Analysis) : 어떤 것에 관한 데이터를 이용해 다른 것을 예측한다\n",
    "    - X가 Y를 예측한다고 해서 둘 사이에 인과관계가 있는 것은 아니다\n",
    "    - 정확한 예측은 올바른 변수를 측정하는 것에 의존한다\n",
    "    - 좋은 모델이 있고 나쁜 모델이 있지만, 데이터가 많고 단순한 모형일수록 좋다\n",
    "    - 예측은 매우 힘들다. 특히 미래의 일에 관해서는 더더욱.\n",
    "    - ex) U.S. election forecast / Target figuring out a teen girl's pregnancy\n",
    "<br><br>\n",
    "- 인과관계 분석(Causal Analysis) : 다른 변수에 변화를 주었을 때 특정 변수에 어떤 결과를 미치는지 분석한다\n",
    "    - 보통 인과관계 분석을 위해 랜덤화된 연구가 필요하다\n",
    "    - 랜덤화되지 않은 연구에서 인과관계를 추론하는 방법들이 있지만, 그 방법들은 가정에 민감하고 복잡하다\n",
    "    - 인과관계 모델은 데이터 분석의 '금본위제(기준)'과도 같다ta analysis\n",
    "    - ex) Causation btw symptoms and disease \n",
    "<br><br>\n",
    "- 기계적 분석(Mechanistic Analysis) : 개별 대상에 대해 다른 변수의 변화를 만들어내는 특정 변수의 정확한 변화를 이해한다\n",
    "    - 단순한 몇몇 경우를 제외하고는 추론이 매우 어렵다\n",
    "    - 보통 결정론적 방정식에 의해 모델이 만들어진다\n",
    "    - 일반적으로 데이터의 랜덤한 부분은 측정의 오류이다\n",
    "    - 방정식은 알려져있지만 파라미터가 알려져 있지 않을 때, 데이터 분석으로 추론될 수 있다\n",
    "<br>    \n",
    "![title](./images/Statistical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining\n",
    "\n",
    "데이터 마이닝은 KDD(Knowledge Discovery in Database)라고 불리우는 대용량의 자료, 혹은 데이터 웨어 하우스로부터 쉽게 드러나지 않는 유용한 정보들을 찾아내는 과정을 말한다.\n",
    "\n",
    "즉, 대용량 관측 데이터를 기반으로 숨겨진 지식, 기대하지 못했던 패턴, 새로운 법칙과 관계를 발견하고 이를 의사결정등을 위한 정보로 활용하는 것이다.\n",
    "\n",
    "실제 데이터 마이닝이 적용되는 과정은\n",
    "1) 탐색(Exploration)을 통해 평균, 이상치, 결측치 등을 발견하고\n",
    "2) 변형(Modification)으로 자료를 변환하며\n",
    "3) 모형화(Modeling)와 모델평가(Assessment) 단계를 거치게 된다.\n",
    "\n",
    "대표적인 기법들은 아래와 같다.\n",
    "\n",
    "- 군집 분석(Cluster Analysis)\n",
    "- 연결 분석(Link Analysis)\n",
    "- 판별 분석(Discrimination Analysis)\n",
    "- 연관성규칙(Association Rule)\n",
    "- 의사결정나무(Decision Tree)\n",
    "- 신경망모형(Neural Network)\n",
    "- OLAP(On-Line Analytic Processing)\n",
    "\n",
    "\n",
    "### 1. Logit Regression\n",
    "로짓 회귀 모형은 목표변수(종속변수)가 **순서형 명목 척도**로 측정되어 있는 경우, 목표변수와 설명변수간의 인과 관계를 분석하기 위하여 적용되는 통계 분석 기법이다.\n",
    "\n",
    "로짓 회귀는 판별분석과 마찬가지로 두 집단 이상으로 구분된 개체에 대해 각 개체가 속하는 집단을 예측하거나 집단의 구분에서는 어느 설명변수가 유의한지를 알아보는데 사용된다.\n",
    "\n",
    "여기서 로짓 변환을 이용하게 되는데, 모형은 다음과 같다.\n",
    "\n",
    "$$ log\\frac{p(y=1\\vert x_1, ..., x_p)}{1 -p(y=1\\vert x_1, ..., x_p)} = \\alpha + \\beta_1 x_1 + ... + \\beta_p x_p $$\n",
    "\n",
    "이는 입력변수 $x_1, x_2, ... x_p $에 대해서 다중 로지스틱 회귀모형화하여, 식의 좌변과 우변이 모두 실수 값을 가지도록 하는 것이다. \n",
    "\n",
    "로짓 회귀의 목적은 추정된 로짓 모형을 이용하여 자료를 분류하기 위한 것이기 때문에 일반적인 판별분석과 비교하여 로지스틱 판별분석(Logistic Discrimination)이라고 불린다.\n",
    "\n",
    "위 식으로부터 아래와 같은 사후 확률에 대한 추정식을 얻을 수 있다.\n",
    "\n",
    "$$ \\widehat{p}(y=1\\vert x_1, ..., x_p) = \\frac{ exp(\\widehat{a} + \\widehat{b_1}x_1 + ... + \\widehat{b_p}x_p}{ 1 + exp(\\widehat{a} + \\widehat{b_1}x_1 + ... + \\widehat{b_p}x_p)} $$\n",
    "\n",
    "이렇게 얻어진 각 개체에 대한 추정 사후 확률은 개체를 분류하기 위해 사용될 수 있다. 즉, 사후 확률은 0과 1사이의 값을 가지게 되므로, 적절한 절단값(**Cutoff value**)를 정하여 이 값을 기준으로 각 개체를 분류하는 것이다.\n",
    "\n",
    "입력변수가 분류 결정에 미치는 영향의 정도는 **오즈비(Odds Ratio)**로 계량화 하는데, 다른 모든 입력변수가 동일한 상태에서 $x_i$가 1단위 증가하는데 따른 오즈비는 다음으로 계산한다.\n",
    "\n",
    "$$ \\frac{ exp(\\alpha + \\beta_1 x_1 + ... + \\beta_i (x_i + 1) + ... \\beta_p x_p) }{ exp(\\alpha + \\beta_1 x_1 + ...+ \\beta_i (x_i) + ... \\beta_p x_p ) } = exp(\\beta_i) $$\n",
    "\n",
    "오즈비가 1보다 작다는 것은 입력변수 $x_i$가 감소방향의 영향을 미침을 의미하고, 1보다 크면 증가방향의 영향을 미침을 의미한다.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Neural Network\n",
    "신경망(Neural Network)에는 여러 가지 다양한 모형이 있으나, 자료분석을 위해 가장 널리 사용되는 모형은 **MLP**(Multilayer Perceptron, 다층인식자) 신경망이다. \n",
    "\n",
    "MLP모형은 1) 입력층(input layer) 2) 은닉층(hidden layer) 3) 출력층(output layer)로 구성된 전방향(feed-forware) 신경망이다.\n",
    "\n",
    "\n",
    "입력층은 각 입력변수에 대응되는 마디들로 구성되어 있다. 명목형(nominal) 변수에 대해서는 각 수준에 대응하는 입력마디를 가지게 되는데, 이는 통계적 선형모형에서 가변수(dummy variable)를 사용하는 것과 같다. \n",
    "\n",
    "은닉층은 여러계의 은닉마디(hidden node)로 구성되어 있는데, 각 마디는 입력층으로부터 전달되는 *변수값들의 선형결합(l-comb)을 비선형함수(NL func)로 처리*하여 출력층 혹은 또다른 은닉층에 전달한다.\n",
    "\n",
    "출력층은 목표변수에 대응하는 마디들을 갖는다. 여러개의 목표변수 또는 세 개이상의 수준을 가지는 명목형 목표변수가 있을 경우에는 여러 개의 출력마디들이 존재하게 된다.\n",
    "\n",
    "![title](./MLP.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림의 구조를 수식으로 도식화하면 다음과 같다.\n",
    "\n",
    "$$ H_1 = f_1(b_1 + w_{11}X_1 + w_{21}X_2 + ... + w_{p1}X_p ) $$\n",
    "$$ H_2 = f_2(b_2 + w_{12}X_1 + + w_{22}X_2 + ... + w_{p2}X_p ) $$\n",
    "$$ Y = g(b_0 + w_{10}H_1 + w_{20}H_2 ) $$\n",
    "\n",
    "이처럼 신경망에서 사용되는 함수는 크게 결합함수(Comb func)와 활성함수(Activation func)가 있다.\n",
    "\n",
    "결합함수는 입력층 또는 은닉층의 마디들을 결합하는 형태를 의미한다. 각 은닉마디 $H_1$과 $H_2$는 입력변수들을 선형결합, 즉 \n",
    "\n",
    "$$ b_j + w_{1j}X_1 + w_{2j}X_2 + ... + w_{pj}X_p $$  하여 이를 변환한다.\n",
    "\n",
    "대부분의 신경망에서는 결합함수로 이와 같은 선형 함수를 사용하지만, **RBF 신경망**은 원형 기준 함수(Radial Basis Func)을 사용하기도 한다.\n",
    "\n",
    "활성함수는 *입력변수 또는 은닉마디의 결합을 변환하는 함수*를 의미한다. 위 식에서 $f_1, f_2, g$는 각각 활성함수와 출력 활성 함수라고 불리며, 선형결합함수를 S자 형태의 곡면형태의 출력을 갖도록 만든다.\n",
    "\n",
    "활성함수와 출력활성함수는 동일한 함수를 사용하는 것이 일반적인데, 활성함수는 통계적 선형모형에서의 *연결함수의 역함수와 유사한 의미*를 가진다.\n",
    "\n",
    "가장 보편적인 활성함수는 **로지스틱 함수**와 **쌍곡 탄젠트(Hyperbolic Tangent)** 함수이다. \n",
    "\n",
    "한편, 목표 변수가 연속형 변수인 경우에는 출력활성함수로 **항등 함수(Identity Func)**를 사용하여\n",
    "\n",
    "$$ Y=b_0 + w_{01}h_1 + w_{02}H_2 $$와 같이 출력마디가 생성되도록 하는 경우도 있다.\n",
    "\n",
    "신경망은 다양한 모형을 포함하는 매우 유연한 모형이나 데이터로부터 계수를 추정해야 하기 때문에 은닉층과 마디가 많을수록 신경망은 복잡해지며 계수의 수가 급격히 증가하기 때문에 최적화가 다른 회귀분석이나 의사결정나무 분석보다 어렵다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Decision Tree\n",
    "\n",
    "의사결정나무(Decision Tree)는 의사결정규칙을 나무구조로 도표화하여 관심대상이 되는 집단을 몇 개의 소집단으로 분류하거나 예측을 수행하는 분석방법이다.\n",
    "\n",
    "나무구조에 의해 분석과정이 표현되기 때문에 분류 또는 예측을 목적으로 하는 방법들 즉, *회귀분석, 신경망, 판별분석*에 비해 연구자는 분석과정을 쉽게 이해하고 설명할 수 있다.\n",
    "\n",
    "의사결정나무분석을 수행하기 위한 다양한 **분리 기준, 정지 규칙, 가지치기 방법**들이 제안되어 있으며, 이들을 어떻게 결합하느냐에 따라서 서로 다른 의사결정나무 형성방법이 만들어진다. \n",
    "\n",
    "대표적인 알고리즘으로는 **CHAID, CART, C4.5**등이 있다.\n",
    "\n",
    "1) CHAID(Chi-squared Automatic Interaction Detection) \n",
    "\n",
    "CHAID는 카이제곱 검정(범주형 목표변수) 또는 F검정(연속형 목표변수)를 이용하여 다지분리(Multiway split)를 수행하는 알고리즘이다. 다지분리란 부모마디에서 자식마디들이 생성될 때, 2개 이상의 분리를 허용함을 의미한다.\n",
    "\n",
    "CHAID 목표변수가 이산형일 때,**피어슨 카이제곱 통계량** 또는 **우도비 카이제곱 통계량(Likelihood Ratio -)**을 분리기준으로 사용한다. \n",
    "\n",
    "여기서 목표변수가 순서형 또는 사전 그룹화된 연속형인 경우에는 우도비 카이제곱 통계량이 사용된다. 카이제곱 통계량은 관측도수로 이루어진 분할표로부터 계산된다.\n",
    "\n",
    "\n",
    "![title](./Contingency.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 분할표로부터, 피어슨 카이제곱 통계량은 다음과 같다.\n",
    "$$ \\chi^2 = \\sum_{i,j}\\frac{ (f_{ij}-e_{ij})^2}{e_{ij}} $$ \n",
    "\n",
    "우도비 카이제곱 통계량은 다음과 같다.\n",
    "\n",
    "$$ \\chi^2 = 2\\sum_{i,j} f_{ij} log(\\frac{f_{ij}}{e_{ij}}) $$\n",
    "\n",
    "이 때, 두 통계량의 자유도(df)는 `(r-1)(c-1)` 로서 동일하다.\n",
    "\n",
    "여기서 $e_{ij}$는 분포의 동일성 또는 독립성의 가설 하에서 계산된 기대도수(Expected freq)를 말하며, 다음 식으로 계산한다.\n",
    "\n",
    "$$ e_{ij} = \\frac{ f_{i\\cdot} f_{\\cdot j}}{f_{\\cdot \\cdot}}$$\n",
    "\n",
    "\n",
    "카이제곱 통계량이 자유도에 비해서 매우 작다는 것은 예측변수의 각 범주에 따른 목표변수의 분포가 서로 동일하다는 것을 의미한다. \n",
    "따라서 예측변수가 목표변수의 분류에 영향을 주지 않는다고 결론지을 수 있다. \n",
    "\n",
    "자유도에 대한 카이제곱 통계량 값의 크고 작음은 P-값으로 표현될 수 있는데, 카이제곱 통계량 값이 자유도에 비해서 작으면 P-값은 커지게 된\n",
    "다. 결국 분리기준을 카이제곱 통계량으로 한다는 것은 P-값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 형성시킨다는 것을 의\n",
    "미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pattern Recognition**은 머신 러닝의 한 분야로서, 패턴의 인식과 데이터의 규칙성에 주목하는 학문이다. (사실 머신 러닝이라는 용어와 거의 유사하게 쓰인다.)\n",
    "\n",
    "Pattern Recognition systems are in many cases trained from labeled \"training\" data (**supervised learning**).\n",
    "<br> But when no labeled data are available, other algorithms can be used to discover previously unknown patterns (**unsupervised learning**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pattern Recognition, Machine Learning, Data Mining, Knowledge discovery in database (KDD)**는 많은 분야에서 겹치기 때문에 구분 짓기가 힘들다.\n",
    "'머신 러닝'은 지도 학습 방법에서 자주 쓰이는 용어이고, 인공지능에서 유래했다.\n",
    "반면, KDD와 데이터 마이닝은 비지도 학습에 더 큰 초점이 있으며 상업적 목적과 연관되어 있다.\n",
    "\n",
    "머신 러닝에서, 패턴 분석은 주어진 입력 값에 라벨을 할당하는 작업이다.\n",
    "통계학에서, 이와 같은 목적을 가진 '판별 분석'이 1936년 소개되었다.\n",
    "\n",
    "패턴 분석의 종류는 다음과 같다.\n",
    "- 'Classification' : 각 입력 값에 주어진 클래스 중 하나를 할당하는 작업이다.\n",
    "- 'Regression' : 각 입력값에 실수 값을 할당해주는 작업이다.\n",
    "- 'Sequence labeling' : 순서가 있는 연속된 값들에 클래스를 할당해주는 작업을 한다. (예를 들어 POS 태깅 같은 경우, 하나의 문장에서 각 단어에 POS를 할당한다.)\n",
    "- 'Parsing' : 입력된 문장에 'Parse Tree'를 할당하여 문장의 구문 구조를 표현한다.\n",
    "\n",
    "패턴 분석은 일반적으로 모든 가능한 입력 값에 대해 합리적이고 \"가장 그럴듯한\" 결과 값을 내놓는 것을 목표로 한다.\n",
    "이는 입력 값에서 주어진 패턴과 정확히 일치하는 것을 찾는 'Pattern Matching'과는 다르다고 할 수 있다. (매칭의 대표적인 예가 Regular Expression 이다.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패턴 분석은 결과 값을 산출해내는 학습 과정에 따라 분류 된다.\n",
    "<br><br>**'Supervised Learning'**은 트레이닝 셋이 제공되어 있고, 그 셋은 정확한 결과값으로 라벨링 되어 있다는 것을 가정한다.\n",
    "그럼 학습 과정은 트레이닝 셋에서도 좋은 성능을 보이고, 새로운 데이터에서도 일반화가 잘되는 언뜻 보면 상충되는 두 개의 목적을 모두 달성하는 'Model'을 만들어낸다.\n",
    "(사실 가장 단순한 모형을 의미한다. 오컴의 면도날 원리와 유사하다)\n",
    "<br><br>**'Unsupervised Learning'**은 반대로 트레이닝 셋이 라벨링 되어있지 않은 상태를 가정한다. 그리고 새로운 데이터에 대해 결과값을 줄 수 있는 데이터 속의 잠재적인 패턴을 찾으려고 노력한다. (이 경우, 트레이닝 셋이 곧 테스트 셋일 수 있다.)\n",
    "<br><br>이 둘의 조합이 **'Semi-supervised Learning'**이고, 라벨링 된 데이터와 그렇지 않은 데이터를 모두 이용한다. (이 경우, 작은 규모의 라벨 데이터와 큰 언라벨 데이터를 함께 사용한다.)\n",
    "\n",
    "<br> *주의할 점은, 앞서 말한 지도 학습과 비지도 학습을 지칭하는 또다른 용어들이 많이 있다는 점이다.\n",
    "예를 들어, 'Unsupervised equivalent of Classification'은 보통 'Clustering'이라고 알려져 있다.*\n",
    "<br><br>**'Clustering'**은 입력 값들을 내재된 '유사도 측정 방식'에 의해서 클러스터로 그룹 짓는 방법이다. 그리고 여기에 사전에 정의된 클래스는 따로 존재하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Terminology\n",
    "\n",
    "결과 값이 산출되는 입력 값의 한 조각을 **'Instance'**라고 부른다.\n",
    "\n",
    "'Instance'는 보통 **'vector of features'**로 표현된다.\n",
    "이 feature 벡터가 모여 instance의 모든 특성을 설명하는 것이다.\n",
    "\n",
    "이 Feature Vector는 '다차원 공간'에서 의 한 점으로서 표현될 수 있고, '벡터 공간'에서 벡터를 조작하는 방법들은 똑같이 이 Feature Vector에도 적용될 수 있다.\n",
    "\n",
    "일반적으로, Feature는\n",
    "<br>**1) Categorical** (혹은 Nominal, e.g., male, female) 데이터 이거나, \n",
    "<br>**2) Ordinal** (e.g., large, medium, small) 이거나, \n",
    "<br>**3) Real-Value** (e.g., measurement of blood pressure)일 수 있다.  \n",
    "\n",
    "일반적으로는 Category와 Ordinal을 함께 묶고, Integer와 Real value를 함께 묶는다.\n",
    "\n",
    "*많은 알고리즘이 오직 Categorical Data에서만 작동하고, 실수형 데이터들이 그룹으로 묶이는 것을 요구한다. (e.g., 5이하, 5초과 10미만, 10이상)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Classifiers\n",
    "\n",
    "많은 패턴 분석 알고리즘은 본질적으로 **'probabilistic'**하다. 왜냐하면 주어진 인스턴스에 대해 최선의 라벨을 산출하기 위해 통계적 추론을 사용하기 때문이다.\n",
    "\n",
    "단순하게 \"최선의\" 라벨만을 내놓는 다른 알고리즘들과 달리, 확률 알고리즘들은 해당 인스턴스가 해당 라벨일 '확률'도 산출해 낸다.\n",
    "\n",
    "게다가, 많은 확률 알고리즘들은 'N개의 최선의 라벨' 목록을 그 확률과 함께 보여주기도 한다.\n",
    "\n",
    "확률 알고리즘은 비확률 알고리즘에 비해 장점이 많다.\n",
    "- 산출값과 관련된 'Confidence Value'를 산출해낸다. (비확률 알고리즘도 이를 산출해낼 수 있지만, 일반적으로 확률 알고리즘만이 확률 이론에 수학적인 근거를 두고 있다.)\n",
    "- 따라서, 해당 값의 'Confidence' 값이 너무 낮다면 그 결과를 지양할 수 있다.\n",
    "- 확률론적 결과값 때문에, 확률 패턴 분석 알고리즘은 더 큰 머신 러닝 기법에 사용될 수 있다. (**'Error Propagation'**을 부분적으로, 혹은 전반적으로 무시할 수 있다.)\n",
    "\n",
    "\n",
    "### Number of important feature variables\n",
    "**'Feature Selection'** 알고리즘은 불필요하거나 관계가 없는 feature를 잘라내는 데 그 목적이 있다.\n",
    "*'Feature Selection'*의 복잡함은 *n 개의 feature가 주어졌을 때, $2^n - 1$ 개 만큼의 Feature의 부분집합을 탐색해야 한다는 'Optimization Problem'에 있다.*\n",
    "\n",
    "**'Branch-and-Bound'** 알고리즘이 이러한 복잡도를 줄여 주긴 하지만, 어느 정도 큰 숫자의 feature에는 적용하기 어렵다는 문제가 있다.\n",
    "\n",
    "**'Feature Extraction'**은 Feature Vector를 변형시키는 작업이다. 종종 패턴 매칭 알고리즘을 적용하기 전에 사용된다.\n",
    "예를 들어, 'feature extraction' 알고리즘은 고차원의 feature vector를 낮은 차원으로 바꿔 작업을 용이하게 하고 불필요한 데이터를 'encode'할 필요가 적어진다.\n",
    "\n",
    "대표적인 수학적 기법으로는 **'Principal Components Analysis(PCA)'**가 있다.\n",
    "\n",
    "Feature Selection과 Extraction의 차이는, Extraction 이후의 Feature는 기존의 Feature와 다른 종류이고, 쉽게 해석하기 힘들 수 있다는 점이다. 이와 반대로 Selection 이후의 Feature는 단순히 원래 집합의 부분집합일 뿐이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement (Supervised Version)\n",
    "\n",
    "일반적으로, 지도(Supervised) 패턴 인식은 다음의 순서대로 이루어진다.\n",
    "1. 미지의 함수 $g : \\mathrm{X} \\to \\mathrm{Y}$ 가 주어진다. 이 때, g는 입력 값 $ x \\in \\mathrm{X} $ 를 라벨값 $ y \\in \\mathrm{Y} $ 로 매핑시키는 함수이다.\n",
    "2. Training Data $ \\mathrm{D} = \\{(x_1,y_1),...,(x_n,y_n)\\}$는 g에 들어가 있다.\n",
    "3. 함수 g와 최대한 비슷한 결과를 내는 함수 $ h : \\mathrm{X} \\to \\mathrm{Y}$ 를 만들어 낸다.\n",
    "4. 여기서 '최대한 비슷한 결과'를 내는 것은 **'Loss Function', 'Cost Function'**의 선택을 통해 정의된다.\n",
    "5. 이 함수들은 잘못된 라벨의 결과로서 나오는 손실에 특정 값을 부여한다. 여기서 목적은 'Expected Loss'를 최소화하는 것이다.\n",
    "6. 손실 함수는 예측되는 라벨의 종류에 따라 달라지는데, 예를 들어 Classification의 경우, 간단한 **'Zero-one loss function'**이 알맞다.\n",
    "7. \n",
    "    1. 여기서 Expectation은 $\\mathrm{X}$의 확률 분포에서 기대값을 구하는 것이다.\n",
    "    2. 확률적 패턴 인식에서는, 이 기대값 대신에 가능한 라벨의 확률을 구한다.\n",
    "<br><br>$$ p(label | x, \\theta) = f(x;\\theta) $$<br>\n",
    "여기서 x는 Feature Vector이고, 함수 f는 보통 파라미터 $\\theta$에 의해 결정된다.\n",
    "\n",
    "        1. **'Discriminative Approach'**에서, f는 직접적으로 추정된다.\n",
    "        2. **'Generative Approach'**에서, $p(x|label)$ 이 추정되고, Prior Prob인 $p(label|\\theta)$와 함께 베이즈 정리를 이용한다.\n",
    "<br><br> $$ p(label | x, \\theta) = \\frac{p(x|label,\\theta)p(label|\\theta)}{\\sum_{L\\in{all,labels}}p(x|L)p(L|\\theta)} $$\n",
    "\n",
    "이 때, $\\theta$의 값은 **'maximum a posteriori (MAP)'**을 이용하여 계산된다.\n",
    "베이지안의 관점에서 보면, 정규화 과정은 Prior Prob인 $\\theta$를 서로 다른 값으로 바꿔주는 과정이라고 할 수 있다.\n",
    "\n",
    "수학적으로,\n",
    "$$ \\theta^{*} = argmax_\\theta p(\\theta|D) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Frequentist or Bayesian approach?\n",
    "피셔가 소개한 최초의 패턴 분석 알고리즘인 \"Linear Discriminant\"는 빈도론에서 탄생하였다. 빈도론자의 접근은 모형의 파라미터가 알려져있지 않지만, 객관적이라고 말한다. 따라서 파라미터는 수집된 데이터로부터 계산된다.\n",
    "\n",
    "베이즈 통계는 고대 그리스 철학에 그 기원이 있다. 그 당시부터 'a priori'와 'a posteriori' 사이에 구분이 있었다. 그 후, 칸트 또한 그 경계를 구분하였다. 베이지안 패턴 분석에서, 클래스의 확률은 사용자에 의해 정해질 수 있다. (이는 a priori이다) 그리고 이러한 베이지안 접근은 전문가의 견해를 주관적인 확률로서 깔끔하게 조화를 이룬다는 사실에 있다.\n",
    "\n",
    "확률론적 패턴 분석은 빈도론자 관점이나 베이지안의 관점에서 사용될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "\n",
    "1. Classification Algorithms (supervised algo predicting categorical labels)\n",
    "    1. Parametric \n",
    "        1. Linear Discriminant Analysis\n",
    "        2. Quadratic Discriminant Analysis\n",
    "        3. Maximum entrophy Classifier (logistic regression)\n",
    "    2. Nonparametric\n",
    "        1. Decision Trees, Decision Lists\n",
    "        2. Kernel Estimation, KNN algo\n",
    "        3. Naive Bayes Classifier\n",
    "        4. Neural Networks (multi-layer perceptrons)\n",
    "        5. Perceptrons\n",
    "        6. Support Vector Machine\n",
    "        7. Gene Expression Programming\n",
    "2. Clustering Algorithms (unsupervised algo predicting categorical labels)\n",
    "    1. Categorical mixture models\n",
    "    2. Deep Learning Methods\n",
    "    3. Hierarchical Clustering\n",
    "    4. K-means Clustering\n",
    "    5. Correlation Clustering\n",
    "    6. Kernel Principal Component Analysis (Kernel PCA)\n",
    "3. Ensemble Learning Algorithms (supervised meta-algorithms for combining mtp learning algo together)\n",
    "    1. Boosting (meta-algorithm)\n",
    "    2. Bootstrap Aggregating(\"bagging\")\n",
    "    3. Ensemble Averaging\n",
    "    4. Mixture of experts, hierarchical mixture of experts\n",
    "4. General Algo for predicting arbitrarily-structured labels\n",
    "    1. Bayesian Networks\n",
    "    2. Markov Random Fields\n",
    "5. Real-valued sequence labeling Algorithms (predicting sequences of real-valued labels)\n",
    "    1. Kalman filters\n",
    "    2. Particle filters\n",
    "6. Sequence labeling algo (predicting sequences of categorical labels)\n",
    "    1. Supervised\n",
    "        1. Conditional random fields (CRFs)\n",
    "        2. Hidden Markov models (HMMs)\n",
    "        3  Maximum entropy Markov models (MEMMs)\n",
    "        4. Recurrent Neural Networks\n",
    "    2. Unsupervised\n",
    "        1. Hidden Markov Models (HMMs)\n",
    "7. Regression Algorithms (predicting real-valued labels)\n",
    "    1. Supervised\n",
    "        1. Gaussian process regression (kriging)\n",
    "        2. Linear Regression and extensions\n",
    "        3. Neural networks and Deep learning methods\n",
    "    2. Unsupervised\n",
    "        1. Independent Component Analysis (ICA)\n",
    "        2. Principal Component Analysis (PCA)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Learning Theory\n",
    "\n",
    "(Computational) [Learning theory](https://en.wikipedia.org/wiki/Computational_learning_theory)는 인공 지능의 하위 분야이며, 머신 러닝 알고리즘의 디자인과 분석이 주된 분야이다.\n",
    "\n",
    "Computational Learning Theory에서는 시간 복잡도와 학습의 가용성에 대해서도 연구한다.\n",
    "\n",
    "연산의 가능성은 \"[Polynomial Time](https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time)\"내에 가능한 지에 달려있다.\n",
    "    \n",
    "다음 두 가지의 시간 복잡도 결과가 있다.\n",
    "    - Positive Result : Polynomial Time 내에 특정 함수가 실행 가능할 경우\n",
    "    - Negative Result : 불가능할 경우\n",
    "        - Negative Result는 보통 옳다고 받아들여지지만, 증명되지는 않은 가정을 따른다\n",
    "            - Computational Complexity - P versus NP problem\n",
    "            - Cryptographic - One-way functions exist\n",
    "Learning Theory에는 또 다른 여러 접근들이 있다. 이들의 차이점은 제한된 데이터로부터 일반화 할 때 추론하는 원칙에 있다.\n",
    "이 때, 확률에 대한 서로 다른 정의, 샘플의 제너레이션에서의 서로 다른 가정들이 포함된다.\n",
    "    - Exact learning (proposed by Dana Angluin)\n",
    "    - Probably Approximately Correct Learning (PAC learning)\n",
    "        - It inspired boosting\n",
    "    - VC Theory\n",
    "        - It led to support vector machine\n",
    "    - Bayesian Inference\n",
    "        - It led to Belief Networks\n",
    "    - Algorithmic Learning Theory\n",
    "    - Online Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propagation of uncertainty\n",
    "\n",
    "통계학에서, 'Propagation of Uncertainty'는 '변수의 불확실성'이 '함수의 불확실성'에 영향을 미치는 것을 의미한다.\n",
    "변수들이 실험 측정의 값들일 경우, 그 변수들은 측정의 한계로 인해 불확실성을 가지고 있다. 그리고 그 불확실성은 함수에서 변수의 결합에까지 퍼져나간다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "## Overall\n",
    "**Feature Selection은 Variable S-, Attribute S-, Variable Subset S- 으로도 알려져 있다. **\n",
    "이는 모형을 만드는 데 사용할 feature (or equivalently, variables, predictors)의 부분집합을 선택하는 과정이다.\n",
    "\n",
    "Feature Selection은 크게 4가지 이유로 사용된다.\n",
    "- 모형의 단순화를 통해 사용자들의 해석을 더 용이하게 만든다.\n",
    "- 트레이닝 시간이 줄어든다.\n",
    "- 차원의 저주를 피하기 위해서.\n",
    "- 오버피팅을 줄여서 일반화를 용이하게 한다. (혹은, **\"reduction of variance\"**)\n",
    "    \n",
    "Feature Selection의 대전제는 '데이터가 중복되거나 무관한 Feature들을 가지고 있고, 정보의 큰 손실 없이 그러한 Feature들이 제거될 수 있다'는 것이다. \n",
    "*Redundancy와 Irrelevancy는 서로 다른 개념이다. 한 예로, 관계가 있는 한 특성은 또다른 관계가 있는 한 특성으로 인해 redundant 해 질 수 있다. *\n",
    "\n",
    "Feature Selection은 Feature Extraction과는 다르다. Extraction은 기존 Feature의 함수로서 새로운 Feature를 만들어내는 작업이다. \n",
    "Feature Selection은 Feature는 많지만 비교적 데이터가 적은 경우 사용된다.\n",
    "\n",
    "Feature Selection이 주로 사용되는 분야는 많은 Feature가 존재하지만, 데이터는 적은 written text와 DNA microarray 데이터 분야이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "### 1. Introduction\n",
    "Feature Selection은 \n",
    "<br>1) 새로운 Feature 부분집합을 검색하는 부분과 \n",
    "<br>2) 서로다른 Feature 부분집합을 평가하는 부분으로 나눌 수 있다.\n",
    "\n",
    "각 부분집합을 평가하는 가장 단순한 알고리즘은 \"**error rate**\"을 최소화하는 것을 찾는 것이다. 하지만 이는 \"exhaustive\"한 방법이고, 작은 집합을 제외하고는 처리하기가 힘들다.\n",
    "\n",
    "따라서 이러한 측정 방법이 알고리즘에 가장 큰 영향을 미치고, 알고리즘을 크게 세 부분으로 분류해준다.\n",
    "\n",
    "1. Wrapper\n",
    "<br>Wrapper는 \"**predictive model**\"을 사용하여 부분집합에 점수를 매긴다. \n",
    "<br>각 새로운 부분집합은 모형을 트레이닝하는 데 사용되고, 그 모형은 'hold-out set'에서 테스트된다. \n",
    "그 셋에서 발생한 error rate는 그 subset의 점수가 된다. \n",
    "<br> Wrapper 방법은 모든 부분집합에 대해 새로운 모형을 트레이닝 하기 때문에, 연산 작업이 매우 많지만 일반적으로 '주어진 모형에 대해' 최고의 성능을 낸다.\n",
    "\n",
    "2. Filter \n",
    "<br>Filter는 error rate이 아닌 \"**proxy measure**\"를 사용한다. 이 방법은 계산이 빠르지만, Feature set의 유용함은 그대로 간직한다. \n",
    "<br>방법으로는 \"**mutual information, pointwise mutual information, pearson product-moment correlation coefficient, inter/intra class distance or the scores of significance tests**\" 등이 있다. \n",
    "<br>Filter는 Wrapper에 비해 연산이 많지는 않지만, *특정 Predictive Model에 맞춰지지 않은 Feature Set을 제공한다.*\n",
    "<br>이러한 성질은 Filter의 Feature 집합이 Wrapper보다 더 일반적이라는 것을 의미한다.  따라서 보통 더 낮은 예측력을 갖는다.\n",
    "<br>하지만 모형에 대한 가정이 없기에 Feature 사이의 관계를 밝히는 데 더 유용하다. \n",
    "<br>일반적으로, Filter는 최고의 부분집합을 보여주기 보다는 Feature들의 순위를 보여준다. \n",
    "<br>그리고 그 cutoff point는 \"**Cross Validation**\"을 통해 정해진다.\n",
    "Filter 방법은 Wrapper 방법을 사용하기 전 전처리 단계에서도 쓰이는데, Wrapper가 더 큰 문제에 쓰일 수 있게 도와준다.\n",
    "\n",
    "3. Embedded Method\n",
    "<br>임베디드 메서드는 \"catch-all group of technique\" 이고, 모형을 설계하는 과정에서 Feature Selection을 실행한다. \n",
    "<br>이 방식의 한 예는 선형 모형을 만드는 \"**LASSO**\" 방법인데, 회귀 계수를 \"**L1 Penalty**\"로서 제약을 주고, 그 중 대다수를 0으로 축소시킨다.\n",
    "<br> 0이 아닌 회귀 계수를 갖는 Feature는 LASSO 알고리즘에 의해 '선택'된다. \n",
    "<br> LASSO의 개량 버전은 \"**Bolasso, Elastic net regularization, FeaLect**\" 등이 있다.\n",
    "<br>또 다른 임베디드 메서드로는 \"**Recursive Feature Elimination**\" 알고리즘이 있는데, SVM과 함께 모형을 세우고 가중치가 낮은 Feature를 제거하는 것을 반복한다.\n",
    "<br> 이 방법들은 계산의 복잡도를 놓고 볼때 Filter와 Wrapper 중간 지점에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Subset Selection\n",
    "Feature의 부분집합을 고르는 과정은 Wrapper, Filter, Embedded 방법으로 분류될 수 있다.\n",
    "<br>Wrapper는 탐색 알고리즘을 사용하여 가능한 Feature 공간을 검색하고, 각 부분집합에 대해 모형을 돌려서 평가를 한다.\n",
    "<br>Wrapper는 계산 비용이 높으며 모형의 오버피팅 가능성이 있다.\n",
    "<br>Filter는 검색 방식은 Wrapper와 비슷하지만, 모형에 대해 평가하는 대신 단ㄷ순한 필터를 사용한다.\n",
    "<br>Embedded는 모형에 내재되어 있고, 그 모형에 국한되어있다.\n",
    "\n",
    "많은 검색 알고리즘이 \"**Greedy Hill Climbing**\"을 사용하는데, 반복적으로 후보 집합을 평가한 뒤, 그 집합에 약간의 변형을 주어 그 이전보다 더 나아졌는지를 평가하는 과정을 반복한다. \n",
    "<br>이 때, 집합을 평가하는 scoring metric이 필요하다.\n",
    "<br>이러한 완전한 탐색(Exhaustive Search)은 비효율적이기 때문에, 사전에 정해놓은 지점까지 가장 높은 점수를 받은 후보 집합이 적당한 Feature Subset으로 결정된다.\n",
    "<br>이 때, 사정에 정해놓은 지점은 알고리즘마다 다르지만 다음과 같은 방법이 있다 : subset score가 기준점을 넘을 때, 프로그램의 허용된 런타임이 초과되었을 때 등..\n",
    "<br> 또 다른 검색 알고리즘으로는 \"**Targeted Projection Pursuit**\"에 기초한 방법이 있는데, 가장 점수가 높은 저차원의 데이터를 찾는 작업이다. 즉, 저차원 공간에 가장 큰 Projection을 갖는 Feature들이 선택된다.\n",
    "\n",
    "- 여러 서치 알고리즘\n",
    "    - Exhausitve\n",
    "    - Best first\n",
    "    - Simulated Annealing\n",
    "    - Genetic Algorithm\n",
    "    - Greedy Forward Selection\n",
    "    - Greedy Backward Elimination\n",
    "    - Particle Swarm Optimization\n",
    "    - Targeted Projection Pursuit\n",
    "    - Scatter Search\n",
    "    - Variable Neighborhood Search\n",
    "    \n",
    "- 필터 메트릭\n",
    "<br>Classification에서 사용되는 가장 유명한 필터 두 가지는 \"**Correlation**\"과 \"**Mutual Information**\"이다. 하지만 이 둘 모두 실제 측정기준(metric)이라고 할 수 없다, 이들은 삼각부등식을 만족시키지 못하기 때문이다. 따라서 distance라는 개념보다는 score가 더 어울린다.\n",
    "그 외에도 아래와 같은 필터들이 있다.\n",
    "    - Class Separability\n",
    "        - Error Probability\n",
    "        - Inter-class distance\n",
    "        - Probabilistic distance\n",
    "        - Entropy\n",
    "    - Consistency-based feature selection\n",
    "    - Correlation-based feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic\n",
    "\n",
    "단순히 Heuristic이라고 불리는 \"**Heuristic Technique**\"은 문제 해결, 학습, 발견에서 사용되는 어떠한 접근법이라도 해당된다. 이는 최적화 되어있거나 완벽한 방법들은 아니지만, 단기적인 목표를 위해선 충분하다.\n",
    "<br>최적해를 찾는 것이 불가능하거나 실용적이지 않을 때, 휴리스틱은 만족할 만한 결과를 찾는 데 쓰일 수 있다.\n",
    "<br> 휴리스틱의 예로는 \"**a rule of thumb, an educated guess, an intuitive judgment, guesstimate, stereotyping, profiling, common sense**\" 등이 있다. \n",
    "\n",
    "- 그 외 Heuristics\n",
    "    - Anchoring and adjustment\n",
    "    - Availability Heuristic\n",
    "    - Representativeness Heuristic\n",
    "    - Naive Diversificatiion\n",
    "\n",
    "##### - Rule of Thumb\n",
    "정확하거나 매 상황에 맞지는 않지만 광범위하게 쓰일 수 있는 원칙을 의미한다. \n",
    "<br>이론에 근거하기 보다는 경험에 기반하여 얻어진 방법이며, 쉽게 적용할 수 있는 방법을 의미한다.\n",
    "\n",
    "##### - Guesstimate\n",
    "영단어 'guess'와 'estimate'의 'portmanteau (여행용 짐가방, 둘 이상의 혼성어)' 이다. \n",
    "<br>적절한 정보 없이, 추측에 의해 평가하는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill Climbing\n",
    "\n",
    "Hill Climbing은 \"**Local Search**\"에 속하는 수학적 최적화 기법이다.\n",
    "<br> \"**Iterative Algorithm**\"이며, 임의의 해에서 시작하여 점차적으로 한 가지 원소를 변경하여 더 나은 해를 찾으려는 시도이다.\n",
    "<br> 변경으로 인한 해가 더 좋은 결과를 만들다가, 더 이상 변경할 점이 없게되면 알고리즘은 종료된다.\n",
    "\n",
    "이 방법은 '[Travelling Salesman Problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem)'에 적용될 수 있다. 이 때, 알고리즘은 임의의 한 방법에서 시작하여 순서를 바꾸는 등 조금씩 개선해 나간다.\n",
    "\n",
    "\n",
    "\n",
    "##### - Convex Problem\n",
    "Convex Minimazation은 최적화의 한 분야이며, \"**Convex Set**\"에서 \"**Convex Function**\"을 최소화시키는 문제를 다룬다. \n",
    "\n",
    "Convexity는 최적화 문제를 일반적인 상황보다 더 쉽게 만드는데, 극소값(local minimum)이 최소값(global minimum)이 되고, 이는 최적성에 충분 조건이기 때문이다. \n",
    "\n",
    "Convex Minimization은 여러 학문에서 쓰이고 있다 ; \"automatic control system\", \"signal processing\", \"circuit design\", finance, statistics, structual optimization.\n",
    "\n",
    "** Def **\n",
    "Given a real vector space $ \\mathrm{X}$ together with a convex, real-valued function defined on a convex subset $\\chi$ of $\\mathrm{X}$\n",
    "\n",
    "$$ f : \\chi \\to \\mathrm{R}; \\forall{x_1, x_2} \\in \\chi, \\forall{t} \\in [0, 1] : f(tx_1 + (1-t)x_2) <= tf(x_1) + (1-t)f(x_2) $$\n",
    "\n",
    "여기서 해결하고자 하는 것은 $f(x)$값을 최소화시키는 $\\chi$에 속하는 점 $x$를 찾는 것이다.\n",
    "\n",
    "$$ f(x^*) <= f(x),  \\forall{x}\\in\\chi$$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Mutual_information\n",
    "https://en.wikipedia.org/wiki/Hill_climbing\n",
    "https://en.wikipedia.org/wiki/Local_search_(optimization)\n",
    "https://en.wikipedia.org/wiki/Triangle_inequality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "`\"the Collection and manipulation of items of data to produce meaningful information.\"`\n",
    "\n",
    "Data Processing may involve various processes, including :\n",
    "    - Validation\n",
    "    - Sorting\n",
    "    - Summarization\n",
    "    - Aggregation\n",
    "    - Analysis\n",
    "    - Reporting\n",
    "    - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data\n",
    "\n",
    "`\"Big data is data sets that are so voluminous and complex that traditional data processing application software are inadequate to deal with them.\"`\n",
    "\n",
    "빅데이터가 직면한 과제는 다음과 같다 : **Capturing data, data storgae, data analysis, search, sharing, transfer, visualization, querying, dupating and information privacy.**\n",
    "\n",
    "빅데이터에는 5개의 차원이 있다 : 기존의 3Vs(Volume, Variety, Velocity) + 새로운 2Vs(Veracity, Value)\n",
    "    - Volume : 만들어지고 저장된 데이터의 양\n",
    "    - Variety : 데이터의 종류와 성질\n",
    "    - Velocity : 데이터가 생성되고 가공되는 속도\n",
    "    - Variability : 데이터셋의 변동성은 데이터를 가공하고 조작하는 과정을 망칠 수 있다\n",
    "    - Veracity(정확도) : 얻어진 데이터의 품질은 다양하며, 분석의 품질에 영향을 미친다.\n",
    "    \n",
    "요즘 들어, 빅데이터라는 단어는 예측 분석, 사용자 분석 등 다른 데이터 분석 모형을 의미하게 되었다. (데이터셋의 크기에 집중하기 보다는) \n",
    "\n",
    "요즘 세상에서 데이터의 양이 정말로 'big'하다는 것에는 이견이 없지만, 새로운 데이터 생태계에서 그것은 그렇게 중요하지 않다.\n",
    "\n",
    "데이터셋의 분석은 \"사업 기회를 찾고, 질병을 예방하고, 범죄를 막는 등\" 새로운 연관성을 찾을 수 있다.\n",
    "\n",
    "\"**Relational Database Management System**\"과 기존의 통계, 시각화 소프트웨어는 빅데이터를 다루는 데 적합하지 않다. 빅데이터의 업무는 보통 수십, 수백, 수천개의 서버에서 광범위하게, 동시에 작동할 수 있는 소프트웨어를 필요로 하기 때문이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDBMS\n",
    "Relational Database Management System은 관계형 모형에 기반한 데이터베이스 관리 시스템이다.\n",
    "RDBMS는 1980년대부터 새로운 데이터베이스로 쓰이기 시작했으며, 그 전에 쓰이던 \"**Hierarchical Database**\"와 \"**Network Database**\"를 대체하기 시작했다.\n",
    "\n",
    "하지만 1980년대, 1990년대에 들어 \"**Object DBMS**\"에 도전을 받았다. (RDBMS와 관계지향형 프로그램 사이의 *Object-Relational Impedance Mismatch* 문제를 해결하기 위해 도입됨). 또한 \"**XML DBMS**\"으로부터도 위협을 받았다. 그럼에도 불구하고 RDBMS는 시장에서 권위를 지킨다.\n",
    "\n",
    "### Market Share\n",
    "2017년 5월 기준, 가장 널리 사용되는 RDBMS는 Oracle과 MySQL(오픈소스), Microsoft SQL Server, PostgreSQL (오픈소스), IBM DB2, Microsoft Access, SQLite (오픈소스) 이다. \n",
    "\n",
    "상업적 5대 기업은 오라클 ,IBM, Microsoft, SAP, Teradata 이다.\n",
    "\n",
    "\n",
    "### Relational Model\n",
    "관계형 모형은 \n",
    "https://en.wikipedia.org/wiki/Relational_model\n",
    "https://en.wikipedia.org/wiki/Object_database\n",
    "\n",
    "\n",
    "\n",
    "- First-order Logic (First-order predicate logic)\n",
    "\n",
    "\n",
    "- Hierarchical Database Model\n",
    "데이터가 트리의 형태로 저장되어 있는 데이터 모형이다. 데이터는 **record** 형태로 저장되며, 다른 데이터와 **link**를 통해 연결되어 있다. \n",
    "*Record*는 필드의 집합이며, 각 필드는 오직 하나의 값을 가진다.\n",
    "레코드의 **Entity Type**은 레코드가 어떤 필드를 갖는 지 결정한다. 계층 모형에서 레코드는 관계형 모형에서의 행(tuple)에 대응되고, entity type은 table (relation)에 해당된다.\n",
    "\n",
    "계층 모형에서 각 자식 레코드는 오직 하나의 부모 레코드를 갖고, 부모 레코드는 여러 자식 레코드를 가질 수 있다.\n",
    "\n",
    "계층 모형에서 데이터에 접근하려면, Root부터 시작하여 전체 트리를 경유해야 한다. 1960년대에 IBM에 의해 만들어진 최초의 데이터베이스 모형이다.\n",
    "\n",
    "\n",
    "- Network Database Model\n",
    "네트워크 모형은 오브젝트와 그 관계를 유연하게 나타내는 데이터베이스 모형이다. \n",
    "가장 큰 특징은 스키마(**Schema**)를 그래프로 표현했을 때, 계층이나 격자형 모형에 제한되지 않는다는 점이다.\n",
    "\n",
    "계층 모형이 데이터를 레코드의 트리로 구성하는 반면, 네트워크 모형에서 각 레코드는 여러 부모 레코드와 여러 자식 레코드를 가질 수 있다. \n",
    "\n",
    "이 특징은 다음 두 측면에서 효과가 있다\n",
    "    1. 스키마는 Relationship Type으로 연결된 레코드 타입이다.\n",
    "    2. 데이터베이스 자체가 Relationship으로 연결된 Record의 그래프이다.\n",
    "계층 모형에 비해 네트워크 모형이 갖는 장점은 entity 사이의 관계를 더 자연스럽게 모형화 할 수 있었다는 점이다.\n",
    "\n",
    "하지만 두 가지 이유로 주류가 되지 못했는데,\n",
    "    1. IBM이 semi-network extension을 고수했기 때문이다.\n",
    "    2. 결국 관계형 모형에 의해 대체되었기 때문이다.\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./images/Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering vs K-Nearest Neighbor Classifiers\n",
    "\n",
    "K-means Clustering은 \"**[Vector Quantization](https://en.wikipedia.org/wiki/Vector_quantization)**\"의 한 방법이다.\n",
    "\n",
    "본래 \"**[Cluster Analysis](https://en.wikipedia.org/wiki/Cluster_analysis)**\"로 유명한 \"**신호 처리(Signal Processing)**\"에서 유래했다.  \n",
    "\n",
    "`n`개의 관측치를 `k`개의 클러스터로 나누고, 각 관측치는 가장 평균이 가까운 클러스터에 속하게 된다. 결과적으로 데이터 공간은 \"**[Voronoi Cells](https://en.wikipedia.org/wiki/Voronoi_diagram)**\"이 된다.\n",
    "\n",
    "이 문제는 계산이 어렵지만, 부분 극값에 빠르게 수렴하도록 하는 휴리스틱 알고리즘이 존재한다. \n",
    "\n",
    "알고리즘 자체는 \"**k-nearest neighbor**\"와 관련은 적다. 다만 새로운 데이터를 기존의 클러스터로 분류하기 위해 k-means에 의해 얻어진 클러스터의 중심에 1-NN을 적용할 수는 있다. 이는 \"**[Nearest Centroid Classifier](https://en.wikipedia.org/wiki/Nearest_centroid_classifier)**\" 혹은 \"**[Rocchio Algorithm](https://en.wikipedia.org/wiki/Rocchio_algorithm)**\" 이라고 불리운다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Description\n",
    "\n",
    "d-dimension인 관측치 $(x_1, x_2, ..., x_n)$이 주어졌을 때, `k`개의 set인 $ S = {S_1, S_2, ... S_k}$ 로 분류하여 **within-cluster sum of squares (WCSS)**를 최소화 시키도록 한다.\n",
    "\n",
    "$$ arg min (s) \\sum_{i=1}^k\\sum_{x\\in S_i}\\Vert x-\\mu_i\\Vert^2$$\n",
    "\n",
    "하지만 이는 휴리스틱 알고리즘이기 때문에, 전역 최적값이라는 보장이 없고, 초기 클러스터에 따라 결과값이 달라질 수 있다. 알고리즘 자체의 속도가 빠르기 때문에, 서로 다른 초기값으로 여러 번 돌리는 게 일반적이다.\n",
    "\n",
    "최악의 경우, 아주 천천히 수렴할 수도 있는데, 실제로 관찰되지는 않고 이론적으로 재현할 수 있다.\n",
    "\n",
    "\"Assignment\" 단계는 \"**expectation step**\"이라고도 불리고, \"Update\" 단계는 \"**Maximization step**\" 이라고도 불린다.\n",
    "\n",
    "\n",
    "### 2. Algorithm\n",
    "\n",
    "###### 1) Initialization Step\n",
    "초기에 어떻게 k개의 클러스터를 만들 것인지를 결정하는 단계이다.\n",
    "주로 사용되는 방법으로는 **Forgy**와 **Random Partition**이 있다.\n",
    "- Forgy\n",
    "임의로 데이터로부터 `k`개의 관측치를 골라서 초기의 평균(means)으로 설정한다.\n",
    "<br>초기 평균값을 퍼트리는 경향이 있다.\n",
    "- Random Partition\n",
    "각 관측 데이터에 임의로 `k`개의 클러스터 중 하나를 배정한 후, 거기서 클러스터의 평균을 Centroid로 설정한다.\n",
    "<br>데이터셋의 중심에 모으는 경향이 있다.\n",
    "\n",
    "Hamerly에 의하면, 랜덤 파티션이 'k-harmonic means' 또는 'fuzzy k-means'에 적합하고, Forgy는 'expectation maximazation'이나 'standard k-means'에 적합하다. \n",
    "\n",
    "하지만 Celebi의 연구 결과에 따르면, \n",
    "1) 널리 쓰이는 초기 알고리즘인 Forgy, RF, Maximin은 성능이 좋지 않고, \n",
    "2) Bradley and Fayyad의 방법은 \"best group\"에서 \"일관성있게\" 동작하고,\n",
    "3) [K-means++](https://en.wikipedia.org/wiki/K-means%2B%2B)는 \"일반적으로 우수하게\" 동작한다고 한다.\n",
    "\n",
    "\n",
    "\n",
    "###### 2) Assignment & Update Step\n",
    "각 관측치를 \"Least squared Euclidean Distance\"를 갖도록 클러스터에 할당하고, 새로운 평균을 Centroid로 재할당하는 단계이다.\n",
    "\n",
    "일반적인 알고리즘은 Iterative Refinement Technique을 사용한다. 널리 쓰이기 때문에 \"**k-means \n",
    "algorithm** 또는 **[Lloyd's algorithm](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm)** 이라고도 불린다.\n",
    "\n",
    "*Assignment*\n",
    "$$ S_i^{(t)} = \\{x_p : \\Vert x_p-m_i^{(t)}\\Vert^2 \\leq \\Vert x_p-m_j^{(t)}\\Vert^2 \\forall j, 1 \\leq j \\leq k \\} $$ \n",
    "\n",
    "*Update*\n",
    "$$ m_i^{(t+1)} = \\frac{1}{\\vert S_i^{(t)} \\vert} \\sum_{x_j \\in S_i^{(t)}}x_j $$\n",
    "\n",
    "위 작업을 반복하다가, 더 이상 변화가 없을 경우 수렴하게 된다. 다시 강조하지만 이 방법을 사용하여 나온 해가 전역적으로 최적해라는 보장은 없다.\n",
    "\n",
    "### 3. Application\n",
    "k-means clustering은 구현하기 쉽고, 큰 데이터 셋에 적용하기가 쉽다. 그래서 '시장 세분화', '컴퓨터 비전', '지리통계학', '천문학', '농업' 등 많은 분야에 쓰였다.\n",
    "<br>또한 다른 알고리즘 전에 전처리 과정으로서 쓰이기도 한다.\n",
    "- Vector Quantization\n",
    "- Cluster Analysis\n",
    "- Feature Learning\n",
    "\n",
    "### 4. Discussion\n",
    "\n",
    "k-means를 효율적으로 만드는 장점들은 동시에 단점이 되기도 한다.\n",
    "- Euclidean Distance가 메트릭으로서 쓰이고, Variance가 클러스터 산집도의 메트릭으로 쓰인다.\n",
    "- `k`는 입력 파라미터이고, `k`를 잘못 선정할 경우 나쁜 결과를 얻을 수 있다. (대안 : [Determining the number of clusters](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set))\n",
    "- 지역 극소값으로의 수렴은 잘못된 (직관에 어긋난) 결과를 가져올 수 있다.\n",
    "\n",
    "예를 들어, 잘 알려진 Iris 데이터 셋에서 이 알고리즘은 잘 작동하지 않는다. k = 2일 경우, 3일 경우에 비해 더 분명하게 구분이 된다. (데이터 셋 자체는 3개의 클래스이지만!) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Quantization (VQ, 벡터 양자화)\n",
    "\n",
    "실제 상황에서 입력 신호는 연속적인 실수값을 갖기 때문에 데이터의 양이 무한하게 늘어나게 된다. 따라서 입력 신호의 데이터량을 이산적인(discrete) 값으로 표현해야 할 필요가 생기는데, 벡터 양자화는 여러개의 입력값을 가지고 있는 벡터를 간단한 형태의 벡터로 매핑하는 양자화 방법이다.\n",
    "\n",
    "(나중에 추가)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop vs. Multithreading\n",
    "\n",
    "하둡과 멀티스레딩의 원리는 비슷한 것일까? 아닌 것 같은데 정확히 알기 위해 찾아보았다.\n",
    "\n",
    "맵리듀스는 전통적인 멀티스레딩 프로그래밍 모델이 아니다. 멀티스레딩과 맵리듀스는 근본적으로 다른 원칙들을 따르고 있고, 서로 다른 문제를 해결하기 위해 등장했다.\n",
    "\n",
    "1. Multi-thread programming model\n",
    "멀티스레딩에서, 각 서브 셋들의 프로세스는 병렬화 되어있다. 각 스레드가 동작하는 데이터셋은 동일하고, \"**Locks, Semaphores**\" 등의 메커니즘을 통해 현재 프로세스를 공유한다. \n",
    "멀티스레딩의 당면 과제는 **데이터의 무결성**을 지키는 것이다. 즉, 각 프로세스 유닛들은 서로 대화하여야 한다. 하지만 \"Deadlocks, Racing Condition, Circular Wait \" 등의 문제는 빈번히 발생하며, 디버그 하기가 어렵다.\n",
    "\n",
    "\n",
    "2. Data-driven programming model\n",
    "이 방법은 데이터를 서로 다른 프로세스 유닛으로 보낸다. 프로세스 유닛은 주어진 데이터에만 접속할 수 있기 때문에, 직접적인 데이터 공유는 금지되어 있다. 따라서 여기서는 상호 소통이 큰 이슈는 아니다.\n",
    "\n",
    "하지만 데이터 접근에 아예 상호 작용이 없는 것은 아닌데, 프로세스 유닛 간의 연결을 explicit하게 해줌으로써 연결 할 수 있다.\n",
    "\n",
    "3. Map-Reduce Programming model\n",
    "하둡은 **Data-Parallel** 이지만, **Process-Sequential** 하다.\n",
    "Data-driven의 특별한 형태라고 볼 수 있는 맵리듀스는 보통 프로세스를 4 단계로 나눈다. (Mapping, Partitioning, Shuffling, Reducing) 그리고 데이터셋의 서브 셋에서 작동한다. 프로세스의 단계는 설계상 순차적으로 되어있다. \n",
    "\n",
    "주의할 점은, 맵리듀스에서 병렬 작업은 \"Job\" 내에서만 동작한다는 것이다. \"Job\" 사이는 정해진 순서에 따라 진행된다. \n",
    "\n",
    "맵리듀스 알고리즘이 큰 데이터셋에서 효과적이기 위해서는 데이터를 \"Distributed File System\"에 저장해야 한다. ( data chunks are stored logically in one place but physically in several nodes of a distributed system). 또한 데이터 처리가 각 노드에서 동시다발적으로 발생한다.\n",
    "\n",
    "\n",
    "4. 그 외 차이점 및 특성\n",
    "- 멀티스레딩은 *프로세스 단계*가 병렬화(parallelized) 되어있는 반면, 하둡은 *데이터*가 병렬화되어 있다. \n",
    "- 멀티스레딩에서, 데이터는 모든 동작하는 프로세스에 의해 공유되기 때문에, 주요 과제는 데이터에 접근을 잘 조정하는 것이다.\n",
    "- 맵리듀스에서, 데이터 접근은 하둡 프레임워크에 의해 처리된다. 하지만 하둡의 주요 과제는 알고리즘을 여러 순차적인 단계로 쪼개어 데이터 병렬을 이용하는 데 있다.\n",
    "- 모든 데이터 프로세스 알고리즘이 맵리듀스에 적용될 수 있는 것도 아니고, 하둡이 모든 데이터 프로세스 업무에 완벽한 해답인 것도 아니다.\n",
    "- 하둡에서 모든 데이터는 정지 상태여야 한다. 즉, 모든 job이 끝날 때까지 중간 업데이트가 발생할 수 없다.\n",
    "- 이는 하둡이 배치 방식으로 데이터를 다루기 때문에, 스트림 방식의 데이터에 적합하지 않다는 뜻이다.\n",
    "- 네트워크를 통한 입력/출력이 방대해지면, 대기시간이 발생한다. (데이터의 카피를 3개 만들어야 하기 때문에) 배치 방식에서는 별 문제가 되지 않지만, 온라인 접근에는 적합하지 않다는 뜻이다.\n",
    "- 하둡은 특히 아래 상황에서 좋지 않다.\n",
    "    - 온라인을 통해 데이터에 접근하는 것\n",
    "    - 큰 데이터 셋에서 작은 서브 셋을 처리하는 것 (하둡은 모든 데이터를 병렬적으로 스캔하도록 디자인 됨)\n",
    "    - 작은 데이터 셋을 처리하는 것 (수백 GB 이하의 데이터 셋에는 더 좋은 방법들이 존재한다)\n",
    "    - 실시간, 스트림 기반 데이터 처리를 하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analytics\n",
    "\n",
    "**예측 분석(Predictive Analytics)**은 예측 모델링(Predictive Modeling), 머신 러닝, 데이터 마이닝 등 넓은 통계적 기법을 널리 포함하는 개념이며 현재와 과거의 데이터를 이용하여 미래나 알려지지 않은 사건에 대한 예측을 하는 기법이다.\n",
    "\n",
    "예를 들어, 기업에서는 과거 데이터에서 발견된 패턴을 이용하여 위험과 기회를 발견한다. 이 때 사용되는 예측 모형은 많은 요소간의 관계를 분석하여 특정 상황에서의 위험도를 평가하여 의사 결정에 도움을 준다.\n",
    "\n",
    "예측 분석의 핵심은 설명 변수와 반응 변수 사이의 관계를 캐치하여 미지의 결과를 예측하는 데 사용하는 것이다.\n",
    "\n",
    "### 예측 분석 프로세스\n",
    "    - 프로젝트 정의\n",
    "    - 데이터 수집\n",
    "    - 데이터 분석 : 데이터 관측, 클리닝, 모델링을 통해 유용한 정보를 얻어내고 결론을 이끌어 낸다\n",
    "    - 통계적 분석 : 가정, 가설을 검정하고 표준 통계 모형을 통해 검증한다.\n",
    "    - 모델링\n",
    "    - 모델 적용\n",
    "    - 모델 검증\n",
    "\n",
    "\n",
    "### 예측 분석의 응용\n",
    "- Analytical CRM\n",
    "- Child Protection\n",
    "- Clinical Decision support system\n",
    "- Collection Analytics\n",
    "- Cross-sell\n",
    "- Customer Retention\n",
    "- Fraud Detection\n",
    "\n",
    "### 예측 분석의 종류(?)\n",
    "일반적으로, \"예측 분석\"이라는 용어는 예측 모형, 데이터에 값을 매기는 행위, 그리고 예측(Forecasting)을 의미한다. \n",
    "하지만 점점 사람들은 유사한 분석 학문인 **descriptive model**, **decision model** 혹은 **optimization**과 이를 혼동하고 있다. \n",
    "\n",
    "이 분야들도 데이터 분석이 포함돼 있기는 하지만, 서로 목적이 다르고 그 뒤에 숨겨진 통계적 기법이 조금씩 다르다.\n",
    "\n",
    "- Predictive Model (예측 모형)\n",
    "예측 모형은 표본에서 한 유닛의 특정 결과값과 그 유닛의 하나 이상 알려진 특성 사이의 관계를 나타내는 모형이다. \n",
    "모형의 목표는 다른 표본의 비슷한 유닛이 그러한 결과값을 보여줄 확률을 측정하는 것이다. \n",
    "연산 속도의 발전으로, Individual Agent Modeling System이 인간 행동을 복원해내는 게 가능해졌다.\n",
    "\n",
    "특성과 결과값이 알려져 있는 표본을 \"**Training Sample**\"이라고 부르고, 특성은 알려져 있지만 결과값이 알려져 있지 않은 샘플을 \"**out of training sample**\"이라고 부른다. 이 샘플들은 트레이닝 샘플과 시간적 관계에 있을 필요는 없다. \n",
    "\n",
    "- Descriptive Model (기술 모형)\n",
    "기술 모형은 고객을 분류하는 것처럼 데이터 안의 관계를 정량화한다.\n",
    "단일 고객의 행동 예측에 집중하는 예측 모형과 달리, 기술 모형은 고객 간, 혹은 상품 간 서로 다른 관계들을 찾아낸다.\n",
    "기술 모형은 예측 모형이 하는 것처럼 특정 행동의 가능성에 따라 고객을 순위매기는 대신, 상품 선호나 인생 주기에 따라 고객을 분류하는 일을 한다.\n",
    "\n",
    "기술 모형은 더 큰 경제 주체를 재현하고 예측을 하는 데 쓰일 수 있다.\n",
    "\n",
    "- Decision Model (의사결정 모형)\n",
    "의사결정 모형은 의사결정의 결과를 예측하기 위해 의사결정의 모든 요소들 사이의 관계를 묘사한다. \n",
    "이 모형들은 최적화, 최대화 기법들을 포함한다. \n",
    "의사결정 모형은 일반적으로 고객이나 상황에 따른 적합한 행동을 만들어내기 위해 의사결정 로직과 원칙을 세운다.\n",
    "\n",
    "\n",
    "### Predictive Modeling\n",
    "예측 모형은 결과를 예측하기 위해 통계를 사용한다. 대부분 예측할 사건은 미래에 있지만, 시간에 관계 없이 적용될 수 있다.\n",
    "\n",
    "정의마다 다르지만, 예측 모형은 머신 러닝의 분야와 많이 오버랩된다. \n",
    "\n",
    "- Models\n",
    "거의 모든 회귀 모형은 예측을 위해 쓰일 수 있다. \n",
    "\n",
    "크게 분류하면, \"**Parametric**\"한 방법과 \"**Non-parametric**\"한 방법이 있다. \"Semi-Parametric\"한 모형은 이 둘을 포함한다. \n",
    "\n",
    "모수적 방법은 모집단의 모수의 분포에 대한 가정을 하는 반면, 비모수적 방법은 그에 비해 가정이 덜하다.\n",
    "\n",
    "    - Group method of data handling\n",
    "    - Naive Bayes\n",
    "    - k-nearest neighbor algorithm\n",
    "    - Majority Classifier\n",
    "    - Support Vector Machine\n",
    "    - Random Forests\n",
    "    - Boosted Trees\n",
    "    - CART (Classification and Regression Trees)\n",
    "    - MARS (Multivariate adaptive regression splines)\n",
    "    - Neural Networks\n",
    "    - ACE and AVAS\n",
    "    - Ordinary Least Squares\n",
    "    - Generalized Linear Models (GLM)\n",
    "    - Logistic Regression\n",
    "    - Generalized Additive Models\n",
    "    - Robust Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./images/Predict.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop vs. Spark\n",
    "\n",
    "하둡과 스파크의 직접적인 비교는 어려운데, 서로 유사한 업무를 수행하기 때문이다.\n",
    "\n",
    "예를 들어, 스파크는 'File Management'가 없기 때문에 HDFS나 다른 솔루션에 의존해야 한다. 오히려 하둡의 맵리듀스와 스파크를 비교하는 것이 낫다. (데이터 처리 엔진으로서 더 비교 가능하다)\n",
    "\n",
    "특정 업무에서는 하둡이 스파크보다 더 낫지만, 스파크는 그 속도와 사용의 편리함 때문에 빅데이터 시장에서 크게 성장하였다. \n",
    "\n",
    "하둡과 스파크에 대해 기억할 점은 그 둘은 서로 배타적이지 않다는 점이고, 다른 하나가 어느 하나보다 열등하지도 않다는 점이다.\n",
    "\n",
    "##### 하둡\n",
    "하둡은 `Apache.org`의 프로젝트로서 큰 데이터셋의 분산처리를 가능하게 해주는 라이브러리이자 프레임워크이다. 하둡은 간단한 프로그래밍 모형을 사용하여 컴퓨터 클러스터들에 업무를 분산화하는데, 이 때 클러스터는 1개의 시스템부터 수천개의 시스템까지 확장할 수 있다. \n",
    "\n",
    "하둡은 프레임워크를 구성하는 여러 모듈로 구성되어 있는데, 주요 모듈은 다음과 같다.\n",
    "\n",
    "- Hadoop Common\n",
    "- Hadoop Distributed File System(HDFS)\n",
    "- Hadoop YARN\n",
    "- Hadoop MapReduce\n",
    "\n",
    "위 4개 모듈은 하둡의 코어를 구성하고 있으며, 이 외에도 다양한 모듈이 있다. (`Ambari, Avro, Cassandra, Hive, Pig, Oozie, Flume, Sqoop`)\n",
    "\n",
    "하둡은 본래 수십억개의 웹 페이지를 검색하고 정보를 데이터베이스에 모으기 위해 고안되었다. 그리고 이러한 소망은 HDFS와 맵리듀스로 나타났다.\n",
    "\n",
    "맵리듀스는 텍스트 처리에 좋은 툴이다.\n",
    "\n",
    "\n",
    "##### 스파크\n",
    "\n",
    "아파치 스파크의 개발자들은 스파크를 \"대용량 데이터 처리를 위한 빠른 엔진\"이라고 칭한다. 비유하자면, 하둡의 빅데이터 프레임워크는 800파운드짜리 고릴라이고, 스파크는 130파운드짜리 치타라고 할 수 있다.\n",
    "\n",
    "스파크의 내장 메모리 처리는 아주 빠르기는하지만 (하둡 맵리듀스의 최대 100배까지), 디스크에서는 약 10배까지 빨라진다.\n",
    "또한 스파크는 배치 단위로 처리할 수 있지만, 스트리밍, 인터랙티브 쿼리, 머신 러닝에 훨씬 더 효율적이다.\n",
    "\n",
    "스파크의 유명세의 원인으로는 맵리듀스의 디스크에 한정 배치 처리 엔진과 비교되는 실시간 데이터 처리 능력을 들 수 있다. 스파크는 하둡과 그 모듈과 호환 가능하고, 하둡의 소개 사이트에서는 스파크가 모듈의 한 종류로도 소개되어 있다.\n",
    "\n",
    "스파크는 따로 홈페이지가 존재한다. 왜냐하면 하둡에서 **YARN(Yet Another Resource Negotiator)**을 통해 실행될 수 있지만, 단독으로도 실행할 수 있기 때문이다. 따라서 이러한 점은 둘을 직접적으로 비교하기 어렵게 만든다. \n",
    "하지만 시간이 지남에 따라, 빅데이터 학자들은 스파크가 영역을 확장하여 하둡을 대체하기를 바란다. (특히 처리된 데이터에 빠른 접근이 필요한 상황에서)\n",
    "\n",
    "스파크는 클러스터-컴퓨팅 프레임워크인데, 하둡 에코시스템보다는 맵리듀스와 동일선상에 있다는 의미이다. 예를 들어, 스파크는 고유한 파일시스템이 없고 HDFS를 사용한다. \n",
    "\n",
    "스파크는 데이터 처리를 위해 메모리와 디스크를 둘 다 사용할 수 있는 반면, 맵리듀스는 오직 디스크에만 의존한다. 맵리듀스와 스파크의 큰 차이는 맵리듀스는 **Persistent Storage**를 쓰는 반면, 스파크는 **RDDs(Resilient Distributed Datasets)**를 사용한다는 점이다.\n",
    "\n",
    "\n",
    "##### 비교\n",
    "1. 성능비교\n",
    "스파크가 맵리듀스에 비해 빠르다는 점은 반박할 여지가 없다. 이 비교에서 문제점은 이 둘은 데이터 처리를 다른 방식으로 한다는 것이다. 스파크가 정말 빠른 이유는 모든 처리를 메모리에서 하기 때문이다. 그리고 메모리로 부족하다면 디스크까지 사용할 수 있다.\n",
    "\n",
    "2. 사용의 편리성\n",
    "스파크는 성능으로 잘 알려져 있지만, 사용의 편리함으로도 잘 알려져 있다. (Scale, Java, Python, Spark SQL을 위한 사용자 친화적인 API를 제공한다.) \n",
    "\n",
    "스파크는 또한 인터랙티브 모드가 있어 쿼리에 대해 즉각적인 피드백을 받을 수 있다. 반대로 맵리듀스는 인터랙티브하지 않기 때문에 받을 수 없다. (하지만 Hive나 Pig같은 모듈은 맵리듀스를 조금 더 쉽게 만들어주었다.)\n",
    "\n",
    "3. 비용\n",
    "맵리듀스와 스파크 모두 Apache 프로젝트의 일환으로, 오픈 소스로 공개되어 있다. 소프트웨어 자체에는 비용이 없는 반면, 플랫폼이나 하드웨어를 동작하는 비용이 발생한다. \n",
    "\n",
    "\n",
    "https://www.datamation.com/data-center/hadoop-vs.-spark-the-new-age-of-big-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Design Patterns (추가 필요)\n",
    "\n",
    "`MapReduce`는 수백개의 컴퓨터를 사용하여 데이터를 처리하는 연산 방식의 패러다임이다. Google, Hadoop, 그리고 다른 이들에 의해 알려졌다. \n",
    "\n",
    "이 놀라운 패러다임은 아주 효과적이지만 사람들이 소위 말하는 \"빅데이터\"에 일반적인 하나의 솔루션은 없다. 따라서 어떤 상황에는 정말 잘 들어맞는 반면에, 또 다른 상황에는 적용이 어려울 수 있다.\n",
    "\n",
    "또한 맵리듀스는 '도구'라기보다는 하나의 '프레임워크'이고, '기능'이라기보다는 '제약'이라고 말하는 편이 더 낫다.\n",
    "\n",
    "이러한 성질은 문제 해결을 더 쉽게도 만들고, 더 어렵게도 만든다. 사용자가 할 수 있는 것과 없는 것의 경계를 뚜렷하게 만들어주고, 선택할 수 있는 옵션을 제공해준다. \n",
    "\n",
    "맵리듀스 디자인 패턴이란 일반적인 데이터 처리 문제를 해결하는 일종의 템플릿이다. 이 패턴은 문자열 처리나 그래프 분석처럼 특정한 분야에 한정되어 있는 것이 아니고, 문제를 해결하는 일반적인 접근법이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanatory model vs Descriptive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prescriptive Analytics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storm\n",
    "\n",
    "실시간 분산형 데이터 처리 플랫폼\n",
    "\n",
    "(이건 나~중에 생각)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship btw Big Data & Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convexity에 대한 정리 필요\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Complexity\n",
    "\n",
    "https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Decision_theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Statistical_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://en.wikipedia.org/wiki/Curse_of_dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유사도를 측정하는 여러가지 방법들이 있다.\n",
    "\n",
    "여기에는 그 방법들을 정리해놓겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Euclidean Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Minkowsky Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Levenshtein Distance (Edit Distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Levenshtein Distance (Edit Distance)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance#Algorithm\n",
    "\n",
    "\n",
    "https://people.cs.pitt.edu/~kirk/cs1501/Pruhs/Spring2006/assignments/editdistance/Levenshtein%20Distance.htm\n",
    "\n",
    "\n",
    "String metric for measuring the edit distance between two sequences. Minimum number of operations of a single character, or transposition of two adjacent characters required to change one word into the other.\n",
    "\n",
    "\n",
    "** Damerau-Levenshtein distance differs from the classical Levenshtein distance by including transpositions to insertions, deletions and substitutions.\n",
    "\n",
    "\n",
    "Code attached to assignment #2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://www.cs.utah.edu/~jeffp/teaching/cs5140-S15/cs5140/L4-Jaccard+nGram.pdf\n",
    "\n",
    " Jaccard Index (Intersection over Union, Jaccard similarity coefficient)\n",
    "\n",
    "> used for comparing the similarity and diversity of sample sets.\n",
    "\n",
    "> is known as a method to test the similarity btw groups\n",
    "\n",
    "> Jaccard similarity J(A,B) is defined as dividing the size of intersection with the size of Union. (Intersection over Union)\n",
    "\n",
    "•Application to documents & Words\n",
    "\n",
    "\n",
    "\n",
    "Bag of words vs. k-Grams ( k-shingle )\n",
    "1.Bag of words model, each document is treated as an unordered set of works.\n",
    "\n",
    "\n",
    "2.k-Gram model, consecutive set of k words.\n",
    "\n",
    "\n",
    "a.I am Sam, Sam I am  -> [I am] [am Sam] ,[Sam Sam] , …\n",
    "\n",
    "\n",
    "b.professional -> [pr] [ro] [of] [fe], …\n",
    "\n",
    "\n",
    "\n",
    "Code is attached to assignment #2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (NOT HERE) Rank Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Kendall Rank Correlation Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (NOT HERE) Recommendation System (RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P vs NP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
