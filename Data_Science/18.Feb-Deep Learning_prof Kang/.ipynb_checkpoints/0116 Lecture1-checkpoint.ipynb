{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Machine learning\n",
    "\n",
    "\"Field of study that gives computers the abilitiy to learn without being explicitly programmed\" Arthur Samuel (1959)\n",
    "\n",
    "### 1) Conventional approaches\n",
    "- Extract meaningful features from data\n",
    "- Estimate and model the probabilistic behavior of the key features\n",
    "- Classify the patterns using rUle or probability based decision statistics\n",
    "    \n",
    "    \n",
    "### 2) ML process\n",
    "- Collect cots of example pairs\n",
    "- Using a learning/training algorithm that has a **generalized form of program.**\n",
    "- Training -> Evaluation (Tuning) -> Testing\n",
    "\n",
    "### 3) Typical Goal of ML\n",
    "- image / video -> Labels, suggest tags, image search, image description\n",
    "- speech/audio  -> Speech recognition, emotion classification, enhancement\n",
    "- text          -> Web search, Anti-spam, Machine translation, Summarization\n",
    "\n",
    "### 4) Database Examples\n",
    "1. MNIST : hand-written digits\n",
    "2. ImageNet : Objects\n",
    "3. TIMIT : Acoustic-Phonetic Continous Speech Corpus\n",
    "\n",
    "### 5) Types of ML\n",
    "1. Supervised\n",
    "    - Regression / Classification\n",
    "2. Unsupervised\n",
    "    - Do not need output labels\n",
    "    - automatically separate classes/features\n",
    "3. Reinforcement learning\n",
    "    - Track / control the system automatically to solve any problem\n",
    "    \n",
    "    \n",
    "## 1. Supervised Learning\n",
    "\n",
    "### 1) Linear Regression\n",
    "\n",
    "\n",
    "* Hypothesis Function\n",
    "$$\n",
    "\\begin{equation*}\n",
    "h(x) = \\sum_{i=0}^n \\theta_i x_i = \\theta^T x \n",
    "\\end{equation*}\n",
    "$$\n",
    "    \n",
    "* Cost function\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "$$\n",
    "\n",
    "\n",
    "* Gradient descent based algorithms\n",
    "\n",
    "    - LMS (Least Mean Square)\n",
    "$$\n",
    "\\theta_j = \\theta_j-\\alpha(h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "    - Batch gradient descent\n",
    "    \n",
    "    전체 파라미터 한 번에 보는 것\n",
    "$$\n",
    "\\theta_j = \\theta_j-\\alpha\\sum_{i-=1}^m(h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)}\n",
    "$$\n",
    "    - Stochastic gradient descent    \n",
    "    \n",
    "    한 샘플씩 이동하는 것 / Oscillation 일어날 수 있음\n",
    "$$\n",
    "\\theta_j = \\theta_j-\\alpha(h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)}\n",
    "$$\n",
    "    \n",
    "여기서 alpha는 Learning Rate이라고 할 수 있음. \n",
    "- alpha가 크면 빠르게 수렴, min 찾기 힘듦\n",
    "- alpha가 작으면 천천히 수렴. Optimal 값으로 서서히 감\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2) Classification and Logistic Regression\n",
    "* Logistic Regression (sigmoid function)\n",
    "$$\n",
    "h(x) = g(\\theta^Tx) = \\frac{1}{1+e^{-\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "$\n",
    "P(y=1|x;\\theta)=h_\\theta(x)\n",
    "$\n",
    "\n",
    "$\n",
    "p(y=0|x;\\theta)=1-h_\\theta(x)\n",
    "$\n",
    "\n",
    "**Put together,**\n",
    "\n",
    "\n",
    "$$\n",
    "p(y|x;\\theta) = (h_\\theta(x))^y(1-h_\\theta(x))^(1-y)\n",
    "$$\n",
    "\n",
    "* Likelihood function<br>\n",
    "-> Likelihood (확률) 값은 위에서 나온 Cost function의 반대 부호이다.\n",
    "$$\n",
    "L(\\theta) = p(\\vec y|X;\\theta) = \\prod_{j=1}^m(h_\\theta(x^{(i)}))^{y^{(i)}}(1-h_\\theta(x))^{1-y^{(i)}}\n",
    "$$\n",
    "\n",
    "* Log Likelihood function\n",
    "\n",
    "$$\n",
    "l(\\theta) = logL(\\theta) = \\sum_{j=1}^my^{(i)}logh_\\theta(x^{(i)})+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))\n",
    "$$\n",
    "\n",
    "* Partial = 0\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta_j}l(\\theta) = (y-h_\\theta(x))x_j\n",
    "$$\n",
    "\n",
    "* Stochastic gradient ascent<br>\n",
    "<br>\n",
    "-> 결국 gradient descent와 같더라.\n",
    "<br>\n",
    "$$\n",
    "\\theta_j = \\theta_j-\\alpha(h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)}\n",
    "$$\n",
    "<br>\n",
    "* Another algorithms for maximizing l($\\theta$)\n",
    "    <br>-Newton's method\n",
    "    $ : \\theta = \\theta - \\frac{f(\\theta)}{f'(\\theta)} $\n",
    "    <br>-Fisher scoring\n",
    "    <br>-Hessian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### 3) Multinomial Regression\n",
    "\n",
    "Multinomial : Need when the output label has multiple number of classes\n",
    "\n",
    "** Softmax : Normalize scores to obtain probability like values **\n",
    "$$ S(y_i) = \\frac{e^{y_i}}{\\sum_{j=1}e^{y_j}} $$\n",
    "\n",
    "** Cost function **\n",
    "- Cross-entropy\n",
    "$$ D(S,L) = -\\sum_{i=1}L_ilog(S_i) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
