{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ※ 영단어 database를 이용하여 마지막 글자를 예측하는 프로그램 만들기  # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 학습 시작\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사용할 library import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "            'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**글자 를 사용하므로 각 글자를 숫자에 대응하게 mapping.**\n",
    "      ex) a - 1\n",
    "          b - 2\n",
    "          c - 3\n",
    "          .\n",
    "          .\n",
    "          .\n",
    "      - enumerate : 리스트가 있는 경우 순서와 해당 리스트 값을 전달\n",
    "      - len : 입력값의 길이(요소 전체 갯수)를 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_data = ['helps', 'warm', 'deep', 'dive', 'cold', 'cool', 'load',\n",
    "           'love', 'kiss', 'kind', 'work', 'hide']\n",
    "\n",
    "test_data = ['helps', 'work', 'dump']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사용할 Data set**\n",
    "\n",
    "* training 단어 목록 = seq_data\n",
    "* test 단어 목록 = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch(word):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    input = [num_dic[n] for n in word[:-1]]\n",
    "    target = num_dic[word[-1]]  \n",
    "    input_batch = np.eye(dic_len)[input]\n",
    "    dim0, dim1 = np.shape(input_batch)\n",
    "    input_batch = np.reshape(input_batch, (-1, dim0, dim1))\n",
    "    target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**입력 받은 단어 알파벳 -> 숫자 나열 input과 target으로 바꾸기**\n",
    "\n",
    "        ex) input : deep - [3, 4, 4] (target이 될 p 제외)\n",
    "            target : deep - p의 10\n",
    "\n",
    "***\n",
    "**mapped data encoding**\n",
    "\n",
    "  input_batch : 한 단어당 한 2차원 vector가 대응되고 가로 세로 길이는 alphabet 수와 같음\n",
    "       ex) input = deep[3, 4, 4, 10]\n",
    "             -   [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "                  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "                  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]]\n",
    "                  \n",
    "  target_batch : target 숫자를 그대로 list에 붙이며 저장\n",
    "         - tf.append : input을 list 뒤에 붙임\n",
    "         - np.eye(dic_len)[input] : 각 줄의 input 자리가 1인 dic_len크기의 2차\n",
    "                                    원배열 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 30\n",
    "    # epoch은 트레이닝 반복 횟수\n",
    "n_input = n_class = dic_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**옵션 설정** \n",
    "\n",
    "* training 에 필요한 hyperparameter 결정\n",
    "       - node 수 : 128\n",
    "       - 최적화 위한 반복 수 : 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
    "    # W는 hidden layer 수만큼\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "    # W, b는 바뀌는 값이라서 Variable로 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**placeholder와 variable 생성** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "    # overfit 방지하기 위해 dropout\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    # 성능 향상을 위해 multi cell 사용\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN을 위한 Cell 생성**\n",
    "    - LSTM cell 2개를 만들어 조합하여 사용함. \n",
    "    - overfitting 방지를 위해 dropout을 병렬로 행함\n",
    "    \n",
    "***    \n",
    "    \n",
    "**cell을 이용하여 순환신경망 만들기**\n",
    "    - dynamic_rnn 함수 사용 : (cell, input, data type)\n",
    "                              output = output tensor와 마지막 state를 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "model = tf.matmul(outputs, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**최종 결과를 one-hot encoding 형식으로 만듬**\n",
    "        - transpose : 행렬을 전치. [[0, 1],   ->    [[0, 2, 4],\n",
    "                                   [2, 3],          [1, 3, 5] \n",
    "                                   [4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=Y))\n",
    "    # softmax가 아니라 다른 함수 사용 -> 성능 위해서\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cost 정의** \n",
    "    - reduce_mean 사용, sparse_softmax_cross_entropy_with_logits 사용\n",
    "\n",
    "**optimizer**\n",
    "    - AdamOptimizer 사용 back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 2.317828\n",
      "Epoch: 0002 cost = 1.136007\n",
      "Epoch: 0003 cost = 0.066638\n",
      "Epoch: 0004 cost = 1.405306\n",
      "Epoch: 0005 cost = 0.000416\n",
      "Epoch: 0006 cost = 0.006830\n",
      "Epoch: 0007 cost = 0.000101\n",
      "Epoch: 0008 cost = 0.025402\n",
      "Epoch: 0009 cost = 0.515868\n",
      "Epoch: 0010 cost = 0.004206\n",
      "Epoch: 0011 cost = 0.004031\n",
      "Epoch: 0012 cost = 0.005214\n",
      "Epoch: 0013 cost = 0.000019\n",
      "Epoch: 0014 cost = 0.000110\n",
      "Epoch: 0015 cost = 0.000054\n",
      "Epoch: 0016 cost = 0.002681\n",
      "Epoch: 0017 cost = 0.002921\n",
      "Epoch: 0018 cost = 0.000299\n",
      "Epoch: 0019 cost = 0.000378\n",
      "Epoch: 0020 cost = 0.008770\n",
      "Epoch: 0021 cost = 0.000083\n",
      "Epoch: 0022 cost = 0.000628\n",
      "Epoch: 0023 cost = 0.000596\n",
      "Epoch: 0024 cost = 0.000051\n",
      "Epoch: 0025 cost = 0.000098\n",
      "Epoch: 0026 cost = 0.000568\n",
      "Epoch: 0027 cost = 0.000636\n",
      "Epoch: 0028 cost = 0.001122\n",
      "Epoch: 0029 cost = 0.000040\n",
      "Epoch: 0030 cost = 0.000151\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for word in range(len(seq_data)):\n",
    "        input_batch, target_batch = make_batch(seq_data[word])\n",
    "        _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습을 위한 최적화 진행**\n",
    "\n",
    "* 신경망 모델 학습 (session run) -  variable initialize 필수\n",
    "* epoch=30\n",
    "       - epoch 가 지남에 따라 cost의 변화, 발전을 print하여 확인\n",
    "         최적화 완료 후 완료를 알리는 문구 print\n",
    "       \n",
    "* batch - 단어 하나가 한 batch 이므로 반복문 사용하여 매번 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "prediction_check = tf.equal(prediction, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**target label 값이 정수이므로 예측값도 정수로 변경**\n",
    "\n",
    "  정수끼리 그대로 비교 후 각 단어 별 비교값을 평균내어 accuracy 측정\n",
    "  측정을 위한 prediction, accuracy 묘사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = 0\n",
    "predict = []\n",
    "for i in range(len(test_data)):\n",
    "    input_batch, target_batch = make_batch(test_data[i])\n",
    "    pred, accuracy_val = sess.run([prediction, prediction_check],\n",
    "                               feed_dict={X: input_batch, Y: target_batch})\n",
    "    acc = acc + accuracy_val\n",
    "    predict.append(pred)\n",
    "acc = acc / len(test_data)\n",
    "acc = acc * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test data로 batch 생성 후 predict와 accuracy 값 얻어냄**\n",
    "\n",
    "* accuracy : 각 단어마다 prediction과 target을 비교한 값을 평균내어 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 예측 결과 ===\n",
      "입력값: ['help ', 'wor ', 'dum ']\n",
      "예측값: ['helps', 'work', 'dump']\n",
      "정확도: 100.00%\n"
     ]
    }
   ],
   "source": [
    "predict_words = []\n",
    "for idx, val in enumerate(test_data):\n",
    "    last_char = char_arr[predict[idx][0]]\n",
    "    predict_words.append(val[:-1] + last_char)\n",
    "    \n",
    "print('\\n=== 예측 결과 ===')\n",
    "print('입력값:', [w[:-1] + ' ' for w in test_data])\n",
    "print('예측값:', predict_words)\n",
    "print('정확도: %.2f' % acc + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predict word를 출력하여 입력과 비교, 결과를 시각적으로 확인하고 accuracy 출력**\n",
    "\n",
    "     seq_data 내에 prediction을 진행할 단어를 가져와 그 단어에 해당하는 prediction alpabet을 last_char로 저장\n",
    "     \n",
    "     단어의 앞에 세 글자와 last_char을 합쳐 predict_words로 출력\n",
    "     \n",
    "       - word[:3] : word 내에 index 3 앞에서 잘라 그 앞 부분만 의미."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
