{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## 2. Naive Bayes\n",
    "<br>\n",
    "\n",
    "GDA에서, 특징 벡터 $x$는 연속형 실수형 벡터였다. 이제 $x_i$들이 이산형 값들인 학습 알고리즘을 배워보자.\n",
    "\n",
    "한 예로 머신러닝을 이용해 이메일 스팸 필터를 만든다고 가정하자. 여기서 우리는 이메일이 스팸인지 넌스팸인지를 구분하고 싶다. 학습 뒤에는 자동으로 스팸 메시지를 분류하도록 만들 수 있다. \n",
    "\n",
    "사실 이메일 분류는 **텍스트 분류(Text Classification)**의 큰 범주에 속한다.\n",
    "\n",
    "트레이닝 셋이 있을 때, 특징 벡터 $x_i$를 먼저 정의해보자.\n",
    "\n",
    "하나의 이메일은 사전에 들어있는 단어의 수만큼의 특징 벡터를 가지고 있다. 따라서 사전의 i번째 단어가 이메일에 포함될 경우 ,$x_i=1$이 된다.\n",
    "\n",
    "여기서 특징 벡터에 들어있는 단어를 **단어(vocabulary)**라고 한다. 따라서 $x$의 차원은 단어의 크기와 같다.\n",
    "\n",
    "이제 특징 벡터가 정해졌으니 생성 모형인 $p(x|y)$를 만들어보자. 하지만 만약 우리에게 5만 단어가 있고, 이를 다변량 분포로 표현하려 한다면 $2^{50000}$의 가능한 결과값들이 있어서 너무나도 많은 파라미터가 존재하게 된다.\n",
    "\n",
    "따라서 우리는 *아주 강력한 가정을 내리게 된다.* 이 가정은 $x_i$들이 $y$가 주어졌을 때 조건부 독립이라는 것이다. 이러한 가정은 **나이브 베이즈(Naive Bayes)**라고 불리며, 이 가정으로 수행된 알고리즘은 **나이브 베이즈 분류기(Naive Bayes Classifier)**라고 부른다.\n",
    "\n",
    "예를 들어, $y=1$이 스팸 메일이고, \"buy\"가 $x_{2087}$, \"price\"가 $x_{39831}$일 경우, $x_{2087}$에 대한 정보는 $x_{39831}$에 대한 정보와 아무 관련이 없다는 것이다. \n",
    "\n",
    "이를 수식으로 표현하면 다음과 같다.\n",
    "\n",
    "<br>\n",
    "$$ p(x_{2087}|y) = p(x_{2087}|y, x_{39831})$$\n",
    "<br>\n",
    "\n",
    "*조심해야할 것은, 이 수식이 $x_{2087}$과 $x_{39831}$의 독립을 의미하는 것은 아니라는 점이다. 여기서는 $y$가 주어졌을 때의 조건부 독립만을 의미한다.*\n",
    "\n",
    "<br>\n",
    "$p(x_1, ..., x_{50000}|y)$\n",
    "\n",
    "$= p(x_1|y)p(x_2|y, x_1)p(x_3|y, x_1, x_2)...p(x_{50000}|y, x_1, ..., x_{49999}) $\n",
    "\n",
    "<br>\n",
    "$=p(x_1|y)p(x_2|y)...p(x_{50000}|y)$\n",
    "\n",
    "<br>\n",
    "$=\\prod_{i=1}^np(x_i|y)$\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 수식이 아주 강력한 가정을 전제로 하지만, 나이브 베이즈 알고리즘은 많은 문제에서 효율이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Parameters\n",
    "\n",
    "나이브 베이즈 모형은 다음 파라미터들로 정의된다.\n",
    "\n",
    "$ \\phi_{i|y=1} = p(x_i =1|y=1)$\n",
    "\n",
    "$ \\phi_{i|y=0} = p(x_i =1|y=0)$\n",
    "\n",
    "$ \\phi_y = p(y=1)$\n",
    "\n",
    "이때, 트레이닝셋 $\\{(x^{(i)}, y^{(i)}); i = 1, ... ,m\\}$이 주어졌을 때, 데이터의 결합 우도함수는 다음과 같다.\n",
    "\n",
    "<br>\n",
    "$$ L(\\phi_y, \\phi_{j|y=0}, \\phi_{j|y=1}) = \\prod_{i=1}^mp(x^{(i)}, y^{(i)}) $$\n",
    "<br>\n",
    "\n",
    "이 우도함수를 파라미터에 대해 최대화 시키면, 다음 추정치를 얻는다.\n",
    "\n",
    "<br>\n",
    "$ \\phi_{j|y=1} = \\frac{\\sum_{i=1}^m 1\\{x_j^{(i)} = 1 \\& y^{(i)} = 1 \\}}{\\sum_{i=1}^m1\\{y^{(i)}=1\\}} $\n",
    "\n",
    "$ \\phi_{j|y=0} = \\frac{\\sum_{i=1}^m 1\\{x_j^{(i)} = 1 \\& y^{(i)} = 0 \\}}{\\sum_{i=1}^m1\\{y^{(i)}=0\\}} $\n",
    "\n",
    "$ \\phi_y = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}}{m} $\n",
    "\n",
    "<br>\n",
    "\n",
    "위 식을 해석하면 매우 간단한다. 예를 들어, $\\phi_{j|y=1}$은 스팸 메일 중 단어 $j$가 나타난 비율이다.\n",
    "\n",
    "\n",
    "### 2-2. Predictions\n",
    "파라미터를 최적화시킨 후, 새로운 특징 벡터 $x$에 대해 예측을 하려면 아래 계산을 하면 된다.\n",
    "\n",
    "<br>\n",
    "\n",
    "$p(y=1|x) $\n",
    "\n",
    "\n",
    "$= \\frac{p(x|y=1)p(y=1)}{p(x)}$\n",
    "\n",
    "$ = \\frac { (\\prod_{i=1}^n p(x_i|y=1))p(y=1)} { (\\prod_{i=1}^n p(x_i|y=1))p(y=1) + (\\prod_{i=1}^n p(x_i|y=0))p(y=0) } $\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2-3. Multinomial Case\n",
    "\n",
    "위에서는 $x_i$이 이진형인 문제를 살펴보았다. $x_i$가 여러 값을 가지는 경우, $p(x_i|y)$를 이항분포(Bernoulli)가 아닌 다항분포(Multinomial)로 두고 계산할 수 있다.\n",
    "\n",
    "또한 변수들이 연속형으로 주어졌더라도, 여러 범주로 이산화 시켜 나이브 베이즈를 사용하는 것도 가능하다.\n",
    "\n",
    "다변량정규분포를 사용하여 모델링이 잘 안될 경우, 이러한 방법을 사용할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. Laplace Smoothing\n",
    "\n",
    "대부분의 문제에서 나이브 베이즈는 잘 작동한다. 하지만 몇가지를 바꿔주면 더 잘 작동할 수 있고, 특히 텍스트 분류에서의 효율을 증대시킬 수 있다.\n",
    "\n",
    "우선 앞서 배운 알고리즘이 가진 문제를 간단히 살펴보자.\n",
    "\n",
    "만약 알고리즘에 이전에 한번도 등장한 적 없던 단어가 들어간다면, 우리의 알고리즘은 이 메일이 스팸일 확률을 0으로 예측한다.\n",
    "\n",
    "왜냐하면 $\\prod_{i=1}^np(x_i|y)$ 식은 $p(x_{new}|y)$의 값을 곱해주는데, 이 값이 0이기 때문이다.\n",
    "\n",
    "하지만 트레이닝 셋에서 본 적이 없다고 해서 어떠한 사건의 확률을 0이라고 추정하는 것은 좋지 않은 방법이다. \n",
    "\n",
    "다항분포를 따르는 변수 $z$의 평균을 추정하는 문제를 생각해보자. \n",
    "\n",
    "$\\phi_i = p(z=i) $ 이므로, $m$개의 독립된 관측 $\\{z^{(1)}, ..., z^{(m)}\\}$에 대해서 최우추정법에 따라 파라미터는 다음이 된다.\n",
    "\n",
    "$$ \\phi_j = \\frac{\\prod_{i=1}^m 1 \\{z^{(i)} = j\\}} {m} $$\n",
    "\n",
    "하지만 앞서 본 것처럼 몇몇 $\\phi_j$는 0의 값이 될 것이다. 이러한 문제를 해결하기 위해 **라플라스 스무딩(Laplace Smoothing) **을 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
