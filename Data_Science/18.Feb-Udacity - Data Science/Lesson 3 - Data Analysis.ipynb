{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Rigor\n",
    "\n",
    "### Significance Tests\n",
    "Using our data, can we disprove an assumption with a pre-defined level of confidence?\n",
    "\n",
    "* Null Hypothesis : It is a statement that we're trying to disprove by running our test.\n",
    "\n",
    "* 귀무가설 : 강력한 증거가 없이는 가급적 지키려고 하는 가설\n",
    "\n",
    "* p-value : Probability of obtaining a best statistic at least as extreme as ours if null hypothesis was true.\n",
    "\n",
    "* p-value : 주어진 관찰통계량 값으로서 귀무가설을 기각할 수 있는 최소한의 유의수\n",
    "\n",
    "### Why is statistics useful in data science?\n",
    "-> They provide a formalized framework for comparing and evaluating data\n",
    "\n",
    "-> They enable us to evaluate whether perceived effects in our dataset reflect differences across the whole population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : Is there any difference btw the batting average of lefties and righties?\n",
    "\n",
    "1. Many tests make assumptions about data's distribution.\n",
    "\n",
    "2. Very common distribution - Normal Distribution (Gaussian Distribution)\n",
    "\n",
    "H_null : Two samples come from same population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. T-test\n",
    "\n",
    "When doing the t-test, we assume the data is normally distributed. \n",
    "\n",
    "If variance differs, we do the Welch's t-test, otherwise we just do normal t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# T-test between two groups\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "import scipy.stats\n",
    "\n",
    "def compare_averages() :\n",
    "    df = pandas.read_csv(filename)\n",
    "    ttest = scipy.stats.ttest_ind(df['avg'][df['handedness']=='L'], \n",
    "    df['avg'][df['handedness']=='R'], equal_var=False)\n",
    "\n",
    "    if ttest[1] <= .05 :\n",
    "        return (False, result)\n",
    "    else :\n",
    "        return (True, result)\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    result = compare_averages()\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shapiro-Wilk Test\n",
    "\n",
    "A test to check the normality of a data. It's in scipy lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, p = scipy.stats.shapiro(data)\n",
    "\n",
    "# w is shapiro statistics, p is p-value\n",
    "# H_null : This data is from normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Non-parametric Test\n",
    "\n",
    "A statistical test that does not assume our data is drawn from any particular underlying probability distribution.\n",
    "\n",
    "-> Mann-Whitney-WilCoxon u test : Tests null hypothesis that two populations are the same.\n",
    "\n",
    "u, p = scipy.stats.mannwhitneyu(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics vs. Machine Learning?\n",
    "\n",
    "-> Statistics is focused on analyzing existing data, and drawing valid conclusions.\n",
    "\n",
    "-> Machine learning is focused on making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with Regression with gradient descent\n",
    "\n",
    "Can we write an equation that takes a bunch of info and predicts Home Runs?\n",
    "\n",
    "First, we define Cost Function. In this case, Cost function is the Least Square Error term.\n",
    "\n",
    "Second, how to minimize Cost Function? -> Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gradient Descent - Cost Function **\n",
    "\n",
    "Cost function = 0.5 * ( sum ( Y_predicted - Y_observed )^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "\n",
    "def compute_cost(features, values, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost of a list of parameters, theta, given a list of features \n",
    "    (input data points) and values (output data points).\n",
    "    \"\"\"\n",
    "    m = len(values)\n",
    "    sum_of_square_errors = numpy.square(numpy.dot(features, theta) - values).sum()\n",
    "    cost = sum_of_square_errors / (2*m)\n",
    "\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(features, values, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent given a data set with an arbitrary number of features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Write code here that performs num_iterations updates to the elements of theta.\n",
    "    # times. Every time you compute the cost for a given list of thetas, append it \n",
    "    # to cost_history.\n",
    "    # See the Instructor notes for hints. \n",
    "    \n",
    "    m = len(values)\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(num_iterations) :\n",
    "        predicted_values = numpy.dot(features,theta)\n",
    "        theta = theta - alpha/m * numpy.dot((predicted_values-values), features)\n",
    "        cost_history.append(compute_cost(features,values,theta))\n",
    "\n",
    "    return theta, pandas.Series(cost_history) # leave this line for the grader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R^2 = 1 - SSR / SSTO\n",
    "\n",
    "SSR = sum (data-prediction)^2\n",
    "\n",
    "SSTO = sum ( data - avg ) ^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_r_squared(data, predictions) :\n",
    "    SST = ((data-np.mean(data))**2).sum()\n",
    "    SSReg = ((predictions-data)**2).sum()\n",
    "    r_squared = 1 - SSReg / SST\n",
    "    \n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Other types of linear regression -> ordinaly least squares regression\n",
    "\n",
    "- Parameter Estimation\n",
    "\n",
    "- Over / Underfitting\n",
    "\n",
    "- Multiple local minima"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
