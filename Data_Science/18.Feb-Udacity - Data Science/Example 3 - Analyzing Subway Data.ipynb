{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Subway Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q1. Exploratory Data Analysis **\n",
    "\n",
    "Let's examine the hourly entries in our NYC subway data and determine what distribution the data follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def entries_histogram(turnstile_weather) :\n",
    "    plt.figure()\n",
    "    turnstile_weather['ENTRIESn_hourly'][turnstile_weather['rain'] ==1].hist() \n",
    "    turnstile_weather['ENTRIESn_hourly'][turnstile_weather['rain'] ==0].hist() \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. Does entries data from the previous exercise seem normally distributed? **\n",
    "\n",
    "** And can we run Welch's T test on entries data? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> No. It's not normally distributed.\n",
    "\n",
    "-> We cannot run Welch's T test. T test is based on data from normally distributed samples.\n",
    "\n",
    "-> So we do the Mann-Whitney U-Test (or bootstrapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. Mann-Whitney U-Test **\n",
    "\n",
    "Your function should return\n",
    "\n",
    "    1) the mean of entries with rain\n",
    "    2) the mean of entries without rain\n",
    "    3) the Mann-Whitney U-statistic and p-value comparing the number of entries with rain and the number of entries without rain\n",
    "    \n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import pandas\n",
    "\n",
    "def mann_whitney_plus_means(turnstile_weather):\n",
    "\n",
    "    with_rain_mean = turnstile_weather['ENTRIESn_hourly'][turnstile_weather['rain']==1].mean()\n",
    "    without_rain_mean = turnstile_weather['ENTRIESn_hourly'][turnstile_weather['rain']==0].mean()\n",
    "    Mann_Whitney = scipy.stats.mannwhitneyu(turnstile_weather['ENTRIESn_hourly'][turnstile_weather['rain']==1], \n",
    "    turnstile_weather['ENTRIESn_hourly'][turnstile_weather['rain']==0], \n",
    "    use_continuity = True, alternative = 'greater')\n",
    "    # alternative가 greater인 것에 주의! 여기서는 비오는날에 지하철 타는 사람이 더 많은지를 확인하고 싶은 것.\n",
    "    \n",
    "    U = Mann_Whitney[0]\n",
    "    p = Mann_Whitney[1]\n",
    "    \n",
    "    return with_rain_mean, without_rain_mean, U, p # leave this line for the grader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. Was the test significant?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can say there is a difference between two data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. Linear Regression**\n",
    "\n",
    "You need to\n",
    "\n",
    "    1) implement the compute_cost() and gradient_descent() procedures\n",
    "    2) Select features (in the predictions procedure) and make predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from ggplot import *\n",
    "\n",
    "\"\"\"\n",
    "In this question, you need to:\n",
    "1) implement the compute_cost() and gradient_descent() procedures\n",
    "2) Select features (in the predictions procedure) and make predictions.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def normalize_features(df):\n",
    "    \"\"\"\n",
    "    Normalize the features in the data set.\n",
    "    \"\"\"\n",
    "    mu = df.mean()\n",
    "    sigma = df.std()\n",
    "    \n",
    "    if (sigma == 0).any():\n",
    "        raise Exception(\"One or more features had the same value for all samples, and thus could \" + \\\n",
    "                         \"not be normalized. Please do not include features with only a single value \" + \\\n",
    "                         \"in your model.\")\n",
    "    df_normalized = (df - df.mean()) / df.std()\n",
    "\n",
    "    return df_normalized, mu, sigma\n",
    "\n",
    "def compute_cost(features, values, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost function given a set of features / values, \n",
    "    and the values for our thetas.\n",
    "    \n",
    "    This can be the same code as the compute_cost function in the lesson #3 exercises,\n",
    "    but feel free to implement your own.\n",
    "    \"\"\"\n",
    "    m = len(values)\n",
    "    sum_of_square_error = np.square(np.dot(features, theta) - values).sum()\n",
    "    cost = sum_of_square_error / (2*m)\n",
    "    # your code here\n",
    "\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(features, values, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent given a data set with an arbitrary number of features.\n",
    "    \n",
    "    This can be the same gradient descent code as in the lesson #3 exercises,\n",
    "    but feel free to implement your own.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(values)\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        predicted_values = np.dot(features, theta)\n",
    "        theta = theta-alpha/m *np.dot( (predicted_values-values), features)\n",
    "        cost_history.append(compute_cost(features, values, theta))\n",
    "    return theta, pandas.Series(cost_history)\n",
    "\n",
    "def predictions(dataframe):\n",
    "\n",
    "    # Select Features (try different features!)\n",
    "    features = dataframe[['rain', 'precipi', 'Hour', 'meantempi']]\n",
    "    \n",
    "    # Add UNIT to features using dummy variables\n",
    "    dummy_units = pandas.get_dummies(dataframe['UNIT'], prefix='unit')\n",
    "    features = features.join(dummy_units)\n",
    "    \n",
    "    # Values\n",
    "    values = dataframe['ENTRIESn_hourly']\n",
    "    m = len(values)\n",
    "\n",
    "    features, mu, sigma = normalize_features(features)\n",
    "    features['ones'] = np.ones(m) # Add a column of 1s (y intercept)\n",
    "    \n",
    "    # Convert features and values to numpy arrays\n",
    "    features_array = np.array(features)\n",
    "    values_array = np.array(values)\n",
    "\n",
    "    # Set values for alpha, number of iterations.\n",
    "    alpha = 0.1 # please feel free to change this value\n",
    "    num_iterations = 75 # please feel free to change this value\n",
    "\n",
    "    # Initialize theta, perform gradient descent\n",
    "    theta_gradient_descent = np.zeros(len(features.columns))\n",
    "    theta_gradient_descent, cost_history = gradient_descent(features_array, \n",
    "                                                            values_array, \n",
    "                                                            theta_gradient_descent, \n",
    "                                                            alpha, \n",
    "                                                            num_iterations)\n",
    "    \n",
    "    plot = None\n",
    "    # -------------------------------------------------\n",
    "    # Uncomment the next line to see your cost history\n",
    "    # -------------------------------------------------\n",
    "    # plot = plot_cost_history(alpha, cost_history)\n",
    "    # \n",
    "    # Please note, there is a possibility that plotting\n",
    "    # this in addition to your calculation will exceed \n",
    "    # the 30 second limit on the compute servers.\n",
    "    \n",
    "    predictions = np.dot(features_array, theta_gradient_descent)\n",
    "    return predictions, plot\n",
    "\n",
    "\n",
    "def plot_cost_history(alpha, cost_history):\n",
    "   \"\"\"This function is for viewing the plot of your cost history.\n",
    "   You can run it by uncommenting this\n",
    "\n",
    "       plot_cost_history(alpha, cost_history) \n",
    "\n",
    "   call in predictions.\n",
    "   \n",
    "   If you want to run this locally, you should print the return value\n",
    "   from this function.\n",
    "   \"\"\"\n",
    "   cost_df = pandas.DataFrame({\n",
    "      'Cost_History': cost_history,\n",
    "      'Iteration': range(len(cost_history))\n",
    "   })\n",
    "   return ggplot(cost_df, aes('Iteration', 'Cost_History')) + \\\n",
    "      geom_point() + ggtitle('Cost History for alpha = %.3f' % alpha )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6. Plotting Residuals **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_residuals(turnstile_weather, predictions):\n",
    "    '''\n",
    "    Using the same methods that we used to plot a histogram of entries\n",
    "    per hour for our data, why don't you make a histogram of the residuals\n",
    "    (that is, the difference between the original hourly entry data and the predicted values).\n",
    "    Try different binwidths for your histogram.\n",
    "\n",
    "    Based on this residual histogram, do you have any insight into how our model\n",
    "    performed?  Reading a bit on this webpage might be useful:\n",
    "\n",
    "    http://www.itl.nist.gov/div898/handbook/pri/section2/pri24.htm\n",
    "    '''\n",
    "    \n",
    "    plt.figure()\n",
    "    (turnstile_weather['ENTRIESn_hourly'] - predictions).hist()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7. Compute R^2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "def compute_r_squared(data, predictions):\n",
    "    '''\n",
    "    In exercise 5, we calculated the R^2 value for you. But why don't you try and\n",
    "    and calculate the R^2 value yourself.\n",
    "    \n",
    "    Given a list of original data points, and also a list of predicted data points,\n",
    "    write a function that will compute and return the coefficient of determination (R^2)\n",
    "    for this data.  numpy.mean() and numpy.sum() might both be useful here, but\n",
    "    not necessary.\n",
    "\n",
    "    Documentation about numpy.mean() and numpy.sum() below:\n",
    "    http://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html\n",
    "    http://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html\n",
    "    '''\n",
    "    \n",
    "    # your code here\n",
    "    SST = ((data-np.mean(data))**2).sum()\n",
    "    SSReg = ((predictions-data)**2).sum()\n",
    "    r_squared = 1 - SSReg / SST\n",
    "    return r_squared\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q8. More Linear Regressions **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
