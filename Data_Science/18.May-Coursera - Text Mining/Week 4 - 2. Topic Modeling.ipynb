{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! Intuition : Documents as a mixture of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "## 1. What is Topic Modeling?\n",
    "-> A corpus-level analysis of what's in a text collection.\n",
    "<br>-> What kind of document is this?\n",
    "\n",
    "- Topic : the subject (theme) of a discourse.\n",
    "    - ex) human, genome, genetic, disease, computer science, news, ...\n",
    "- Topics are represented as a word distribution.\n",
    "- A document is assumed to be a mixture of topics.\n",
    "\n",
    "-> What's known?\n",
    "    - The text colletion or corpus\n",
    "    - Number of topics\n",
    "\n",
    "-> What's not known?\n",
    "    - The actual topics\n",
    "    - Topic distribution for each document\n",
    "\n",
    "-> Essentially, it is a **text clustering** problem\n",
    "\n",
    "-> Different topic modeling approaches are available\n",
    "    - Probabilistic Latent Semantic Analysis (PLSA)\n",
    "    - Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Generative Models for Text\n",
    "\n",
    "**1) Coming from 1 chest**\n",
    "<br>['Harry', 'Potter', 'movie', 'the', 'is'] -> Generation -> Document\n",
    "\n",
    "(Now the Inverse)\n",
    "\n",
    "Document -> Inference, Estimation -> Distribution of words from 1 topic\n",
    "\n",
    "**2) Mixture Chest (model) **\n",
    "<br>Document -> Inference, Estimation -> Coming from different topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Latent Dirichlet Allocation (LDA, generative model)\n",
    "\n",
    "1. Generative model for a document d\n",
    "    - Choose length of document d\n",
    "    - Choose a mixture of topics for document d\n",
    "    - Use a topic's multinomial distribution to output words to fill that topic's quota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Modeling Issues\n",
    "\n",
    "1. How many topics?\n",
    "    - Finding one or even guessing the number is very hard.\n",
    "2. Interpreting topics\n",
    "    - Topics are just word distributions\n",
    "    - Making sense of words / generating labels is subjective\n",
    "\n",
    "<br><br>\n",
    "To summarize,\n",
    "\n",
    "1. Great tool for exploratory text analysis\n",
    "    - What are the documents (tweets, reviews, news articles) about?\n",
    "    - Many tools available in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with LDA in Python\n",
    "\n",
    "1. Simple steps\n",
    "    - Many packages available, such as gensim, lda\n",
    "    - Pre-processing text\n",
    "        - Tokenize, normalize (lowercase)\n",
    "        - Stop word removal\n",
    "        - Stemming\n",
    "    - Convert tokenized documents to a document-term matrix\n",
    "    - Build LDA models on the doc-term matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doc_set : set of pre-processed text documents\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(doc_set)\n",
    "corpus = [dictonary.doc2bow(doc) for doc in doc_set]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, \n",
    "                                           id2words = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=4, num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary reference for the Latent Dirichlet Allocation (LDA) is the following. The first five pages (Pg nos. 993-997) describes the model and the plate notation.David M. Blei, Andrew Y. Ng, Michael I. Jordan; Latent Dirichlet Allocation. Journal of Machine Learning Research (JMLR); 3(Jan):993-1022, 2003.http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see the following Wikipedia pages on:\n",
    "\n",
    "    Latent Dirichlet Allocation: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "    Description of the plate notation: https://en.wikipedia.org/wiki/Plate_notation\n",
    "\n",
    "WordNet based similarity measures in NLTK: http://www.nltk.org/howto/wordnet.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
