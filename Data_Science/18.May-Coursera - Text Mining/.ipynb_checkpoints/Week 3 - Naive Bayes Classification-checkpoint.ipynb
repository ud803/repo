{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is Classification?\n",
    "\n",
    "Given a set of classes, assigning the correct class label to the given input\n",
    "\n",
    "##### **Examples of Text Classification*\n",
    "- Topic identification (Politics, Sports, Technology, ...)\n",
    "- Spam Detection\n",
    "- Sentiment Analysis (Is this review positive or negative?)\n",
    "- Spelling correction (weather or whether? color or colour?)\n",
    "\n",
    "### 1) Supervised Learning\n",
    "\n",
    "Humans learn from past experiences, machines learn from past instances!\n",
    "\n",
    "##### **Supervised Classification*\n",
    "Learning a **classification model** on properties (\"features\") and their importance (\"weights\") from labeled instances\n",
    "- Training Phase (given Labeled input with Classification algorithm) \n",
    "- Labeled Data set is split into Training Data / Test Data\n",
    "- -> Inference Phase (Classification Model given unlabeled input)\n",
    "<br><br>\n",
    "$ X $: set of attributes or features : $   {   \\{x_1, x_2, ..., x_n\\}} $\n",
    "<br><br>\n",
    "$ y $ : A \"class\" label from the label set $ Y = \\{y_1, y_2, ..., y_n\\} $\n",
    "<br><br>\n",
    "\n",
    "Apply the model on new instances to predict label.\n",
    "\n",
    "\n",
    "### 2) Classification Paradigms\n",
    "1. When there are only two possible classes -> ** Binary Classification **\n",
    "2. More than two -> ** Multi-class Classification **\n",
    "3. When data instances can have two or more labels -> ** Multi-label Classification **\n",
    "\n",
    "##### **Questions to ask in supervised learning*\n",
    "In Training phase,\n",
    "- What are the features? How do you represent them?\n",
    "- What is the classification model / algorithm?\n",
    "- What are the model parameters?\n",
    "\n",
    "In inference phase,\n",
    "- What is the expected performance? What is a good measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Identifying Features from Text\n",
    "\n",
    "##### **Why is textual data unique? *\n",
    "- Textual data presents a unique set of challenges\n",
    "- All the information you need is in the text\n",
    "- But features can be pulled out from text at different granularities!\n",
    "\n",
    "### 1) Types of textual features\n",
    "\n",
    "1. Words\n",
    "    - By far the most common class of features\n",
    "    - Handling commonly-occurring words : ** Stop words**\n",
    "    - Normalization : Make lower case vs. leave as-is\n",
    "    - Stemming / Lemmatization\n",
    "    <br><br>\n",
    "    \n",
    "2. Characteristic of words\n",
    "    - Capitalization ( white house vs White House )\n",
    "    - Parts of speech in a sentence\n",
    "    - Grammatical structure, sentence parsing\n",
    "    - Grouping words of similar meaning, semantics \n",
    "        - {buy, purchase}\n",
    "        - {Mr., Ms.,.. }\n",
    "        - Numbers, Digits, Dates ..\n",
    "3. Other\n",
    "    - Depending on classification tasks, features may come from inside words and word sequences \n",
    "        - ex. bigrams, trigrams, n-grams : \"White House\"\n",
    "    - character sub-sequences in words : \"ing\", \"ion\", ...\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naive Bayes Classifier\n",
    "\n",
    "##### *Case study : Classifying text search queries\n",
    "\n",
    "Suppose you are interested in classifying search queries in three classes\n",
    "         : <br>   *Entertainment, Computer Science, Zoology*\n",
    "\n",
    "The most common class among three is \"Entertainment\".\n",
    "\n",
    "<br>**1. Suppose the query is \"Python\"**\n",
    "- Python, the snake (Zoology)\n",
    "- Python, the programming language (Computer Science)\n",
    "- Python, as in Monty Python (Entertainment)\n",
    "\n",
    "Most common class, given \"Python\", is Zoology.\n",
    "\n",
    "<br>**2. Now suppose the query is \"Python download\"**\n",
    "\n",
    "Most probable class, is computer science.\n",
    "<br><br><br>\n",
    "\n",
    "**So what is happening? **\n",
    "\n",
    "### 1) Probabilistic Model\n",
    "Update the likelihood of the class given new information.\n",
    "\n",
    "***Prior Probability*** : <br>Pr(y = Entertain), Pr(y = CS), Pr(y = Zoology)\n",
    "\n",
    "***Posterior Probability*** :<br>Pr(y = Entertain | x = \"Python\")\n",
    "<br><br>\n",
    "##### ***Bayes' Rule*** : <br>\n",
    "$$ Pr( y | X ) = \\frac{ Pr(y)*Pr(X|y)}{Pr(X)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Naive Bayes Classification\n",
    "\n",
    "$$ Pr(y= CS | \"Python\" ) = \\frac{Pr(y=CS)*Pr(\"Python\"|y=CS)}{Pr(\"Pyton\")}$$\n",
    "\n",
    "<br>\n",
    "$$ if Pr(y=CS | \"Python\") > Pr(y=Zoology | \"Python\") , y = CS $$\n",
    "\n",
    "<br>\n",
    "$$ y^* = argmax_yPr(y|X) = argmax_yPr(y)*Pr(X|y)$$\n",
    "<br>\n",
    "##### ***Naive assumption *** \n",
    "-> Given the class label, features are assumed to be independent of each other.\n",
    "\n",
    "$$ y^* = argmax_yPr(y|X) = argmax_yPr(y)*\\prod_{i=1}^nPr(X_i|y)$$\n",
    "\n",
    "\n",
    "##### ***For example, ***\n",
    "-> Query : \"Python Download\"\n",
    "\n",
    "$$ y^* = argmax_yPr(y)*Pr(\"Python\"|y)*Pr(\"Download\"|y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) What are the parameters?\n",
    "\n",
    "1. Prior probabilities : Pr(y) for all y in Y\n",
    "2. Likelihood : $Pr(x_i|y)$ for all features $x_i$ and labels y in Y\n",
    "<br>\n",
    "If there are 3 classes and the dimension of the data element (features) is 100, how many parameters does the naive Bayes model have?\n",
    "\n",
    "A naive Bayes Classifier has two kinds of parameters ;\n",
    "1. Pr(y) for every y in Y: so if |Y| = 3, there are three such parameters.\n",
    "2. Pr(x_i | y) for every binary feature x_i in X and y in Y. <br> Specifically, for a particular feature x_1, the parameters are Pr(x_1=1 |y) and Pr(x_1=0 | y). So if |X| = 100 binary features and |Y| = 3, there are (2*100) * 3 = 600 such features.\n",
    "3. Hence in all, there are 603 features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Training Parameters\n",
    "\n",
    "1. Prior Probabilities : Pr(y) for all y in Y\n",
    "    - Count the number of instances in each class\n",
    "    - IF there are N instances in all, and n out of those are labeled as class y, Pr(y) = n / N<br><br>\n",
    "2. Likelihood : Pr(x_i | y) for all features x_i and labels y in Y\n",
    "    - Count how many times feature x_i appears in instances labeled as class y\n",
    "    - IF there are p instances of class y, and x_i appears in k of those, Pr(x_i | y) = k / p<br><br>\n",
    "3. **Smoothing**\n",
    "    - What happens if Pr(x_i|y) = 0? -> x_i never occurs in label y\n",
    "    - then, posterior prob Pr(y|x_i) will be 0 !!\n",
    "    - Instead, we smooth the parameters. (add a dummy count)\n",
    "    - ***Laplace smoothing*** or ***Additive smoothing*** : Add a dummy count\n",
    "    - Pr(x_i|y ) = (k+1) / (p+n); where n is number of features<br>\n",
    "    ( p + n , because I haved added n words as dummies)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Naive Bayes Variations\n",
    "\n",
    "### 1) Two classic Naive Bayes Variants for Text\n",
    "\n",
    "Two common options for Naive Bayes Classification you'll face.\n",
    "\n",
    "\n",
    "1. Multinomial Naive Bayes\n",
    "    - Assumes data follows a multinomial dist\n",
    "    - Each feature value is a count (word occurrence count, TF-IDF weighting, ...)\n",
    "    - often used in text documents\n",
    "    <br><br>\n",
    "2. Bernoulli Naive Bayes\n",
    "    - Data follows multivariate bernoulli dist\n",
    "    - Each feature is binary (word is present / absent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
