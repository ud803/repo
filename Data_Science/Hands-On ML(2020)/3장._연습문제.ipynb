{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.8 연습문제\n",
    "\n",
    "1번 MNIST 데이터셋으로 분류기를 만들어 테스트 세트에서 97% 정확도를 달성해보세요. **KNeighborsClassifier**가 이 작업에 아주 잘 맞습니다. 좋은 하이퍼파라미터 값만 찾으면 됩니다. (weights와 n_neighbors 하이퍼파라미터로 그리드 탐색을 시도해보세요.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uidam_package import fetchData\n",
    "\n",
    "X_train, y_train, X_test, y_test = fetchData.fetch_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((60000, 28*28))\n",
    "X_test = X_test.reshape((10000, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from uidam_package import model\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors' : [3, 5, 7],\n",
    "    'weights' : ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "model_name = \"grid_search_knn_clf.pkl\"\n",
    "\n",
    "if model.model_exists(model_name):\n",
    "    grid_search_knn_clf = model.get_model(model_name)\n",
    "else:\n",
    "    knn_clf = KNeighborsClassifier()\n",
    "    grid_search_knn_clf = GridSearchCV(knn_clf, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "    grid_search_knn_clf.fit(X_train, y_train)\n",
    "    model.save_model(grid_search_knn_clf, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_knn_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_knn_clf.best_estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2번. MNIST 이미지를 어느 방향으로든 한 픽셀 이동시킬 수 있는 함수를 만들어보세요. 그런 다음 훈련세트에 있는 각 이미지에 대해 네 개의 이동된 복사본 (방향마다 한 개씩) 을 만들어 훈련 세트에 추가하세요. 마지막으로 이 확장된 데이터셋에서 앞에서 찾은 최선의 모델을 훈련시키고 테스트 세트에서 정확도를 측정해보세요. 모델 성능이 더 높아졌는지 확인하세요! 인위적으로 훈련 세트를 늘리는 이 기법을 **데이터 증식** 또는 **훈련 세트 확장 Training Set Expansion** 이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3번. 타이타닉 데이터셋에 도전해보세요. (생략)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4번. 스팸 분류기를 만들어보세요. (아주 도전적인 과제입니다.)\n",
    "- 아파치 스팸어쌔신 공공 데이터셋에서 스팸과 햄 샘플을 내려받습니다.\n",
    "- 훈련 세트와 테스트 세트로 나눕니다.\n",
    "- 각 이메일을 특성 벡터로 변환하는 데이터 준비 파이프라인을 만듭니다. 이 준비 파이프라인은 하나의 이메일을 가능한 단어의 존재 여부를 나타내는 (희소) 벡터로 바꿔야 합니다. 예를 들어, 모든 이메일이 네 개의 단어 'Hello', 'how', 'are', 'you'만 포함한다면 'Hello you Hello Hello you'라는 이메일은 벡터 [1, 0, 0, 1]로 변환되거나, 단어의 출현 횟수에 관심이 있다면 [3, 0, 0, 2]로 변환되어야 합니다.\n",
    "- 준비 파이프라인에 이메일 헤더 제거, 소문자 변환, 구두점 제거, 모든 URLs 주소를 'URL'로 대체, 모든 숫자를 'NUMBER'로 대체, **어간** 추출 등을 수행할지 여부를 제어하기 위해 하이퍼파라미터를 추가합니다.\n",
    "- 여러 분류기를 시도해보고 재현율과 정밀도가 모두 높은 스팸 분류기를 만들 수 있는지 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\\spam\\20021010_easy_ham.tar\n",
      "datasets\\spam\\20021010_hard_ham.tar\n",
      "datasets\\spam\\20021010_spam.tar\n",
      "datasets\\spam\\20030228_easy_ham.tar\n",
      "datasets\\spam\\20030228_easy_ham_2.tar\n",
      "datasets\\spam\\20030228_hard_ham.tar\n",
      "datasets\\spam\\20030228_spam.tar\n",
      "datasets\\spam\\20030228_spam_2.tar\n",
      "datasets\\spam\\20050311_spam_2.tar\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "target_path = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "file_list = os.listdir(target_path)\n",
    "\n",
    "for file_name in file_list:\n",
    "    if('bz2' not in file_name): continue\n",
    "    file_path = os.path.join(target_path, file_name)\n",
    "    extracted_file_path = file_path[:-4]\n",
    "    print(extracted_file_path)\n",
    "    \n",
    "    if os.path.exists(extracted_file_path): continue\n",
    "\n",
    "    with open(file_path, 'rb') as f_in:\n",
    "        with open(extracted_file_path, 'wb') as f_out:\n",
    "            for data in iter(lambda : f_in.read(100 * 1024), b''):\n",
    "                f_out.write(data)\n",
    "\n",
    "file_list = os.listdir(target_path)\n",
    "\n",
    "for file_name in file_list:\n",
    "    if('bz2' in file_name): continue\n",
    "    file_path = os.path.join(target_path, file_name)\n",
    "    tar = tarfile.open(file_path)\n",
    "    extracted_file_path = file_path[:-4]\n",
    "    tar.extractall(extracted_file_path)\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 6.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([])\n",
    "\n",
    "np.append(X, [1, 2, 3])\n",
    "np.append(X, [4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-125-355eca1163cb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-125-355eca1163cb>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    list(map([1,2,3], str)\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "list(map([1,2,3], str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([])\n",
    "y = np.array([])\n",
    "\n",
    "target_path = os.path.join(\"datasets\", \"spam\")\n",
    "file_list = os.listdir(target_path)\n",
    "\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(target_path, file_name)\n",
    "    if(os.path.isdir(file_path)):\n",
    "        inner_folder_name = os.listdir(file_path)[0]\n",
    "        files_path = os.path.join(file_path, inner_folder_name)\n",
    "        add_file = os.listdir(files_path)\n",
    "        add_file = list(map(lambda x : os.path.join(files_path, x), add_file))\n",
    "        if('spam' in inner_folder_name):\n",
    "            X = np.append(X, add_file)\n",
    "            y = np.append(y, [1] * len(add_file))\n",
    "        elif('ham' in inner_folder_name):\n",
    "            X = np.append(X, add_file)\n",
    "            y = np.append(y, [0] * len(add_file))\n",
    "        else:\n",
    "            print(\"NOPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64686047, 0.35313953])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(y_train, bins=2)[0]/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64667596, 0.35332404])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(y_test, bins=2)[0]/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stri = \"<HEAD> hello </HEAD>dd\"\n",
    "\n",
    "import re\n",
    "\n",
    "a = re.compile('<HEAD>(.*)</HEAD>')\n",
    "a.sub('', stri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'from': 6,\n",
       "          'rssfeeds@jmason.org': 1,\n",
       "          'mon': 1,\n",
       "          'oct': 5,\n",
       "          '7': 3,\n",
       "          '12:05:14': 1,\n",
       "          '2002': 5,\n",
       "          'return-path:': 1,\n",
       "          '<rssfeeds@spamassassin.taint.org>': 2,\n",
       "          'delivered-to:': 1,\n",
       "          'yyyy@localhost.spamassassin.taint.org': 1,\n",
       "          'received:': 3,\n",
       "          'localhost': 2,\n",
       "          '(jalapeno': 1,\n",
       "          '[127.0.0.1])': 2,\n",
       "          'by': 5,\n",
       "          'jmason.org': 1,\n",
       "          '(postfix)': 1,\n",
       "          'with': 5,\n",
       "          'esmtp': 2,\n",
       "          'id': 2,\n",
       "          '9693816f76': 1,\n",
       "          'for': 5,\n",
       "          '<jm@localhost>;': 1,\n",
       "          'mon,': 4,\n",
       "          '12:03:52': 2,\n",
       "          '+0100': 3,\n",
       "          '(ist)': 2,\n",
       "          'jalapeno': 1,\n",
       "          '[127.0.0.1]': 1,\n",
       "          'imap': 1,\n",
       "          '(fetchmail-5.9.0)': 1,\n",
       "          'jm@localhost': 1,\n",
       "          '(single-drop);': 1,\n",
       "          '07': 2,\n",
       "          'dogma.slashnull.org': 2,\n",
       "          '(localhost': 1,\n",
       "          '(8.11.6/8.11.6)': 1,\n",
       "          'g9780ok23233': 1,\n",
       "          '<jm@jmason.org>;': 1,\n",
       "          '09:00:24': 1,\n",
       "          'message-id:': 1,\n",
       "          '<200210070800.g9780ok23233@dogma.slashnull.org>': 1,\n",
       "          'to:': 1,\n",
       "          'yyyy@spamassassin.taint.org': 1,\n",
       "          'from:': 1,\n",
       "          'boingboing': 1,\n",
       "          'subject:': 1,\n",
       "          'football': 5,\n",
       "          'players': 3,\n",
       "          'addicted': 2,\n",
       "          'to': 5,\n",
       "          'video': 4,\n",
       "          'date:': 2,\n",
       "          '08:00:23': 1,\n",
       "          '-0000': 1,\n",
       "          'content-type:': 1,\n",
       "          'text/plain;': 1,\n",
       "          'encoding=utf-8': 1,\n",
       "          'url:': 1,\n",
       "          'http://boingboing.net/#85531549': 1,\n",
       "          'not': 1,\n",
       "          'supplied': 1,\n",
       "          'pro': 1,\n",
       "          'are': 1,\n",
       "          'games,': 2,\n",
       "          'as': 3,\n",
       "          'a': 13,\n",
       "          'means': 1,\n",
       "          'of': 11,\n",
       "          'wish-fulfillment': 1,\n",
       "          '--': 1,\n",
       "          '\"managing\"': 1,\n",
       "          'the': 14,\n",
       "          'team,': 1,\n",
       "          'they': 1,\n",
       "          'can': 3,\n",
       "          'be': 4,\n",
       "          'free': 2,\n",
       "          'rule': 1,\n",
       "          'their': 1,\n",
       "          'coaches': 1,\n",
       "          'and': 5,\n",
       "          'bosses.': 1,\n",
       "          'maybe': 1,\n",
       "          'this': 1,\n",
       "          'explains': 1,\n",
       "          'amazing': 1,\n",
       "          'success': 1,\n",
       "          'sims,': 1,\n",
       "          'which,': 1,\n",
       "          'on': 3,\n",
       "          'face': 1,\n",
       "          'it,': 1,\n",
       "          'should': 1,\n",
       "          'dull': 1,\n",
       "          'hell:': 1,\n",
       "          'while': 1,\n",
       "          'away': 2,\n",
       "          'your': 2,\n",
       "          'time': 2,\n",
       "          'office': 1,\n",
       "          'simulating': 1,\n",
       "          'an': 1,\n",
       "          'existence': 2,\n",
       "          'shlub': 1,\n",
       "          'day-job': 1,\n",
       "          'drive': 1,\n",
       "          'acquire': 1,\n",
       "          'consumer': 1,\n",
       "          'goods': 1,\n",
       "          'credit.': 1,\n",
       "          \"you'd\": 1,\n",
       "          'think': 1,\n",
       "          \"it'd\": 1,\n",
       "          'last': 1,\n",
       "          'thing': 1,\n",
       "          'you': 4,\n",
       "          'want': 1,\n",
       "          'do.': 1,\n",
       "          'but': 1,\n",
       "          \"it's\": 1,\n",
       "          'not.': 1,\n",
       "          'when': 3,\n",
       "          \"you're\": 1,\n",
       "          'sim,': 1,\n",
       "          'tweak': 1,\n",
       "          'smidge,': 1,\n",
       "          'discover': 1,\n",
       "          'what': 1,\n",
       "          'life': 1,\n",
       "          'would': 1,\n",
       "          'like': 2,\n",
       "          'if': 1,\n",
       "          'took': 1,\n",
       "          'path': 2,\n",
       "          'instead': 1,\n",
       "          'b,': 1,\n",
       "          'try': 1,\n",
       "          'alternate': 1,\n",
       "          'universe': 1,\n",
       "          'size.': 1,\n",
       "          'idea': 1,\n",
       "          'playing': 3,\n",
       "          'themselves': 1,\n",
       "          'in': 4,\n",
       "          'licensed': 1,\n",
       "          'games': 1,\n",
       "          'is': 1,\n",
       "          'neat': 1,\n",
       "          'recursive,': 1,\n",
       "          'episode': 1,\n",
       "          'simpsons': 1,\n",
       "          'mr.': 1,\n",
       "          'burns': 1,\n",
       "          'runs': 1,\n",
       "          'into': 2,\n",
       "          'krusty': 1,\n",
       "          'buying': 1,\n",
       "          \"krusty-o's\": 1,\n",
       "          'at': 2,\n",
       "          'supermarket': 1,\n",
       "          'asks': 1,\n",
       "          'where': 1,\n",
       "          'he': 1,\n",
       "          'find': 1,\n",
       "          '\"burns-o\\'s.\"': 1,\n",
       "          '\"it\\'s': 1,\n",
       "          'always': 1,\n",
       "          'trip,\"': 1,\n",
       "          'carr': 1,\n",
       "          'says.': 1,\n",
       "          '\"the': 1,\n",
       "          'first': 1,\n",
       "          'i': 2,\n",
       "          'saw': 1,\n",
       "          'myself': 1,\n",
       "          'game': 1,\n",
       "          'was': 2,\n",
       "          'college': 1,\n",
       "          '(at': 1,\n",
       "          'fresno': 1,\n",
       "          'state)': 1,\n",
       "          'walked': 1,\n",
       "          'best': 1,\n",
       "          'buy': 1,\n",
       "          'store': 1,\n",
       "          'some': 1,\n",
       "          'kid': 2,\n",
       "          'me.': 1,\n",
       "          'that': 1,\n",
       "          'kind': 1,\n",
       "          'trips': 1,\n",
       "          'out': 1,\n",
       "          'little': 1,\n",
       "          'bit.\"': 1,\n",
       "          'every': 1,\n",
       "          '12-year-old': 1,\n",
       "          'who': 1,\n",
       "          'spends': 1,\n",
       "          'countless': 1,\n",
       "          'hours': 1,\n",
       "          'front': 1,\n",
       "          'television': 1,\n",
       "          \"there's\": 1,\n",
       "          'group': 1,\n",
       "          '300-pound': 1,\n",
       "          'offensive': 1,\n",
       "          'linemen': 1,\n",
       "          'challenging': 1,\n",
       "          'each': 1,\n",
       "          'other': 1,\n",
       "          'everything': 1,\n",
       "          'madden': 1,\n",
       "          'nfl': 1,\n",
       "          '2003': 1,\n",
       "          'action-packed': 1,\n",
       "          '\"halo:': 1,\n",
       "          'combat': 1,\n",
       "          'evolved.\"': 1,\n",
       "          'link[1]': 1,\n",
       "          'discuss[2]': 1,\n",
       "          '(_thanks,': 1,\n",
       "          'lawrence[3]!_)': 1,\n",
       "          '[1]': 1,\n",
       "          'http://www.chron.com/cs/cda/story.hts/sports/1604474': 1,\n",
       "          '[2]': 1,\n",
       "          'http://www.quicktopic.com/boing/h/xwcdy9acbah': 1,\n",
       "          '[3]': 1,\n",
       "          'http://www.io.com/~lawrence': 1})]"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class wordSparseVector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, remove_header = True, is_lowercase = True):\n",
    "        self.remove_header = remove_header\n",
    "        self.is_lowercase = is_lowercase\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        email = np.random.choice(X)\n",
    "        X_transformed = []\n",
    "        with open(email, 'rb') as f_in:\n",
    "            contents = f_in.read().decode('cp1252')\n",
    "\n",
    "            if self.remove_header:\n",
    "                remover = re.compile('<HEAD>.*</HEAD>')\n",
    "                contents = remover.sub('', contents)\n",
    "\n",
    "            if self.is_lowercase:\n",
    "                contents = contents.lower()\n",
    "\n",
    "            word_vec = contents.split()\n",
    "            X_transformed.append(Counter(word_vec))\n",
    "        return X_transformed\n",
    "            \n",
    "wordSparseVector().fit_transform(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
