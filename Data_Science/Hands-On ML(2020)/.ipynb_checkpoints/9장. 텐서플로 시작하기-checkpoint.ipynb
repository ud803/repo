{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9장. 텐서플로 시작하기\n",
    "\n",
    "**텐서플로 TensorFlow**는 수치 계산을 위한 강력한 오픈소스 라이브러리로 특히 대규모 머신러닝에 세밀하게 튜닝되어 있다.\n",
    "\n",
    "원리는 간단하다. 먼저 파이썬으로 수행할 계산 그래프 Computation Graph 를 정의한 다음, 텐서플로가 최적화된 C++ 코드를 사용해 이 그래프를 효율적으로 실행시킨다.\n",
    "\n",
    "이 계산 그래프는 노드 Node와 엣지 Edge로 구성되어 있다.\n",
    "\n",
    "무엇보다도 계산 그래프를 여러 부분으로 나누어 CPU나 GPU에서 병렬로 실행할 수 있다. 또한 텐서플로는 분산 컴퓨팅도 지원하므로 수백 대의 서버에 계산을 나누어 납득할만한 시간 안에 대규모 데이터셋으로 거대한 신경망을 훈련시킬 수 있다.\n",
    "\n",
    "텐서플로는 구글 브레인 팀에서 개발했고 구글 클라우드 스피치, 구글 포토, 구글 검색 같은 구글의 대규모 서비스를 지원하고 있기 때문에 이러한 분산 처리들이 매우 우수하다.\n",
    "\n",
    "텐서플로가 2015년 11월에 오픈소스로 공개되었을 때 이미 인기 있는 오픈소스 라이브러리드링 있었다. 그럼에도 텐서플로우의 깔끔한 설계, 확장성, 유연성과 잘 정돈된 문서 덕분에 단숨에 가장 높은 인기를 얻었다.\n",
    "\n",
    "**텐서플로의 장점은 아래와 같다.**\n",
    "\n",
    "- 윈도우, 리눅스, macOS 뿐만 아니라 iOS와 안드로이드 같은 모바일에서도 실행된다.\n",
    "- 사이킷런과 호환되는 매우 간결한 파이썬 API인 `TF.learn`을 제공한다. 여러 종류의 신경망을 몇 줄의 코드로 훈련시킬 수 있다.\n",
    "- 신경망의 구축, 훈련, 평가를 단순화한 `TF-slim`이라는 또 다른 간편한 API도 제공한다.\n",
    "- 이제는 `tensorflow.keras`에 포함된 케라스나 프리티 텐서와 같은 여러 가지 고수준 API가 텐서플로를 기반으로 독립적으로 구축되었다.\n",
    "- API가 훨씬 유연하여 생각하는 어떤 신경망 구조도 구성할 수 있다.\n",
    "- 신경망을 만드는 데 필요한 연산을 C++로 매우 효율적으로 구현했다. 또한 C++ API도 제공하고 있어 자신만의 고성능 연산도 만들 수 있다.\n",
    "- 비용 함수를 최소화하는 모델 파라미터를 찾기 위한 여러 가지 고수준의 최적화 노드를 제공한다. 텐서플로는 우리가 정의한 함수의 그래디언트를 자동으로 계산해주기 때문에 사용하기 매우 쉽다. 이를 **자동 미분 Automatic DIfferentiation, autodiff**라고 한다.\n",
    "- **텐서보드 Tensorboard**라는 훌륭한 시각화 도구를 사용해 계산 그래프와 학습 곡선 등을 웹 브라우저로 확인할 수 있다.\n",
    "- 이 오픈소스에 기여하는 커뮤니티는 매우 빠르게 성장중이며, 깃허브에서 가장 인기 있느 오픈소스 프로젝트 중 하나이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 설치\n",
    "\n",
    "그럼 시작해보자. `pip` 명령으로 쉽게 텐서플로를 설치할 수 있다.\n",
    "\n",
    "나는 GPU 설정을 하기 위해 CUDA를 설치해야 한다. 그 과정은 뒤 12장에서, 혹은 실전에 사용할 때 하자.\n",
    "\n",
    "지금은 텐서플로 2.1이 나와있지만, 튜토리얼을 따라하기 위해 1.7.0 버전으로 낮춘다.\n",
    "\n",
    "많은 것들이 변한 것 같다. (특히 Session의 동작이 없음.)\n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/#download-cuda-software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 hands_on_ml 콘다 환경에 설치가 되었다.\n",
    "\n",
    "이제 버전 확인을 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 첫 번째 계산 그래프\n",
    "\n",
    "계산 그래프를 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이게 전부이다! 꼭 이해해야 할 중요한 점은 이 코드가 뭔가 계산하는 것 같아 보이지만 실제로는 어떠한 계산도 수행하지 않는다는 점이다.\n",
    "\n",
    "단지 **계산 그래프**만 만들 뿐이다. 변수조차도 초기화되지 않는다.\n",
    "\n",
    "이 계산 그래프를 평가(run, execute)하려면 텐서플로 세션을 시작하고 변수를 초기화한 다음 f를 평가해야 한다.\n",
    "\n",
    "텐서플로 세션은 연산을 CPU나 GPU같은 장치에 올리고 실행하는 것을 도와주며 모든 변숫값을 가지고 있다.\n",
    "\n",
    "다음 코드는 세션을 만들고 변수를 초기화한 다음 f를 평가하고 세션을 닫는다. (즉, 자원을 해제한다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매번 `sess.run()`을 반복하면 번거롭기 때문에, 더 나은 방법이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 블록 안에서는 with 문에서 선언한 세션이 기본 세션으로 지정된다. \n",
    "\n",
    "또한 각 변수의 초기화를 일일이 실행하는 대신 `global_variables_initializer()` 함수를 사용할 수도 있다.\n",
    "\n",
    "이 함수는 초기화를 바로 수행하지 않고 계산 그래프가 실행될 때 모든 변수를 초기화할 노드를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 텐서플로 프로그램은 두 부분으로 나뉜다.\n",
    "\n",
    "첫 부분은 계산 그래프를 만들고(**구성 단계**),  두 번째 부분은 이 그래프를 실행한다. (**실행 단계**)\n",
    "\n",
    "구성 단계에서는 훈련에 필요한 계산과 머신러닝 모델을 표현한 계산 그래프를 만든다.\n",
    "\n",
    "실행 단계에서는 훈련 스텝을 반복해서 평가하고, 모델 파라미터를 점진적으로 개선하기 위해 반복 루프를 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 계산 그래프 관리\n",
    "\n",
    "노드를 만들면 자도으로 기본 계산 그래프에 추가된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 경우 이것으로 충분하지만, 가끔은 독립적인 계산 그래프를 여러 개 만들어야 할 때가 있다.\n",
    "\n",
    "이렇게 하려면 다음과 같이 새로운 Grpah 객체를 만들어 with 블록 안에서 임시로 이를 기본 계산 그래프에 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Tip.** 주피토에서 실험적인 작업을 하는 동안에는 같은 명령을 여러 번 실행하는 경우가 많다.\n",
    "\n",
    "이렇게 하면 기본 그래프에 중복된 노드가 많이 포함된다. 주피터 커널을 다시 시작하는 것이 한 가지 방법이지만, 더 편리한 방법은 `tf.reset_default_graph()`로 기본 그래프를 초기화해주는 것이다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 노드 값의 생애주기\n",
    "\n",
    "한 노드를 평가할 때 텐서플로는 이 노드가 의존하고 있는 다른 노드들을 자동으로 찾아 먼저 평가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드는 매우 간단한 그래프를 정의하고 있는데, 세션을 시작하고 y를 평가하기 위해서 계산 그래프를 실행한다.\n",
    "\n",
    "텐서플로는 자동으로 y가 x에 의존한다는 것, x가 w에 의존한다는 것을 감지한다.\n",
    "\n",
    "그래서 먼저 w를 평가하고, 그 다음 x, y를 평가하여 반환한다. \n",
    "\n",
    "마지막으로 z를 평가하기 위해 그래프를 다시 실행하고, **이전에 평가된 w와 x를 재사용하지 않고** 다시 각각을 평가한다.\n",
    "\n",
    "모든 노드의 값은 계산 그래프 실행 간에 유지되지 않는다! \n",
    "\n",
    "위 코드와 달리 w와 x를 두 번 평가하지 않고 y와 z를 효율적으로 평가하려면 아래와 같이 바꾸어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Caution.** 단일 프로세스 텐서플로에서는 같은 그래프를 재사용하더라도 여러 세션에서 어떤 상태도 공유하지 않는다. (각 세션은 모든 변수에 대한 고유한 복사본이다.)\n",
    "\n",
    "반대로 분산 텐서플로에서는 변수 상태가 세션이 아니라 서버에 저장되므로 여러 세션이 같은 변수를 공유할 수 있다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 텐서플로를 이용한 선형 회귀\n",
    "\n",
    "텐서플로 연산(Operation, Ops)은 여러 개의 입력을 받아 출력을 만들 수 있다.\n",
    "\n",
    "예를 들어 덧셈과 곱셈 연산은 두 개의 입력을 받아 하나의 출력을 만든다. 상수와 변수 연산은 입력이 없다. (**소스 연산 Source Op**이라고 한다.)\n",
    "\n",
    "입력과 출력은 **텐서 Tensor**라는 다차원 배열이다. (그래서 텐서플로!) 넘파이 배열과 비슷하게 텐서는 데이터 타입과 크기를 가진다.\n",
    "\n",
    "사실 파이썬 API에서는 텐서는 넘파이 ndarray로 나타난다. 보통은 실수로 채워지지만 문자열을 저장할 수도 있다.\n",
    "\n",
    "아래서 볼 캘리포니아 주택 가격 데이터셋은 두 개의 텐서플로 상수 노드 X와 y를 만들고 데이터와 타깃을 담는다.\n",
    "\n",
    "여기서 사용되는 함수들은 앞서 언급한 것처럼 계산을 즉각 수행하지는 않는다. 대신 그래프가 실행될 때 계산을 수행할 노드를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.6845909e+01]\n",
      " [ 4.3667579e-01]\n",
      " [ 9.4581824e-03]\n",
      " [-1.0708507e-01]\n",
      " [ 6.4334780e-01]\n",
      " [-3.9223723e-06]\n",
      " [-3.7883162e-03]\n",
      " [-4.2040566e-01]\n",
      " [-4.3347809e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "넘파이를 사용해 정규방정식을 직접 계산하는 대신 이 코드를 사용하는 장점은 GPU가 있는 경우 텐서플로가 자동으로 GPU에서 이 코드를 실행한다는 점이다.\n",
    "\n",
    "# 6. 경사 하강법 구현\n",
    "\n",
    "이번에는 정규방정식이 아닌 경사 하강법을 사용해보자. 먼저 그래디언트를 수동으로 계산해보고 그다음에 텐서플로의 자동 미분 기능을 사용해 그래디언트를 자동으로 계산해보자.\n",
    "\n",
    "마지막으로 텐서플로에 내장된 **옵티마이저 Optimizer**를 사용한다.\n",
    "\n",
    "---\n",
    "\n",
    "**Caution.** 경사 하강법을 사용할 때는 입력 특성 벡터를 정규화하는 것이 중요하다. 그렇지 않으면 훈련 속도가 매우 느려진다.\n",
    "\n",
    "정규화는 텐서플로나 넘파이, 사이킷런의 `StandardScaler` 또는 선호하는 다른 도구를 사용하기도 한다.\n",
    "\n",
    "이어지는 코드는 정규화가 이미 되어 있다고 가정한다.\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 직접 그래디언트 계산\n",
    "\n",
    "아래의 코드는 몇 개의 요소만 제외하고는 이해하기 쉽다.\n",
    "- `random_uniform()` 함수는 난수를 담은 텐서를 생성하는 노드를 그래프에 생성한다. 넘파이의 `rand()` 함수처럼 크기와 난수의 범위를 입력받는다.\n",
    "- `assign()` 함수는 변수에 새로운 값을 할당하는 노드를 생성한다. \n",
    "- 반복 루프는 훈련 단계를 계속 반복해서 실행하고 100번 반복마다 현재의 평균 제곱 에러를 출력한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "scaled_housing_data_plus_bias = std_scaler.fit_transform(housing_data_plus_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 11.211982\n",
      "Epoch 100 MSE = 4.945639\n",
      "Epoch 200 MSE = 4.8605485\n",
      "Epoch 300 MSE = 4.8478093\n",
      "Epoch 400 MSE = 4.838843\n",
      "Epoch 500 MSE = 4.831821\n",
      "Epoch 600 MSE = 4.826275\n",
      "Epoch 700 MSE = 4.8218713\n",
      "Epoch 800 MSE = 4.8183594\n",
      "Epoch 900 MSE = 4.815546\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 자동 미분 사용\n",
    "\n",
    "앞선 코드가 잘 작동하지만 비용 함수(MSE)의 그래디언트를 수학적으로 유도해야 한다.\n",
    "\n",
    "선형 회귀의 경우에는 아주 쉽게 할 수 있지만 심층 신경망에서 하려면 매우 번거롭고 실수하기가 쉽다.\n",
    "\n",
    "**기호 미분 Symbolic Differentiation**을 사용해 자동으로 편미분 방정식을 구할 수 있다. 하지만 결과 코드의 효율이 몹시 나쁠 수도 있다.\n",
    "\n",
    "위의 경사 하강법 코드에서 아래의 코드로만 변경해주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gradients()` 함수는 하나의 연산(여기서는 mse)과 변수 리스트(여기서는 theta 하나)를 받아 각 변수에 대한 연산의 그래디언트를 계산하는 새로운 연산을 만든다.\n",
    "\n",
    "자동으로 그래디언트를 계산하는 방법은 네 가지가 있는데, 텐서플로는 **후진 모드 자동 미분 Reverse-mode autodiff**를 사용한다. \n",
    "\n",
    "신경망에서처럼 입력이 많고 출력이 적을 때 완벽한 방법이다.\n",
    "\n",
    "## 6.3 옵티마이저 사용\n",
    "\n",
    "앞 절에서 설명한 것처럼 텐서플로는 자동으로 그래디언트를 계산해준다. 하지만 더 쉬운 방법이 있는데, 텐서플로의 내장 옵티마이저를 사용하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 또는 모멘텀 옵티마이저도 있다.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 훈련 알고리즘에 데이터 주입\n",
    "\n",
    "미니배치 경사 하강법을 구현하기 위해 이전 코드를 변경해보자.\n",
    "\n",
    "이렇게 하려면 매 반복에서 X와 y를 다음번 미니배치로 바꿔야 한다. 가장 간단한 방법은 **플레이스 홀더 Placeholder** 노드를 사용하는 것이다.\n",
    "\n",
    "이 노드는 실제로 아무 계산도 하지 않는 노드이며, 실행 시에 주입한 데이터를 출력하기만 한다. 일반적으로 훈련을 하는 동안 텐서플로에 훈련 데이터를 전달하기 위해 사용된다.\n",
    "\n",
    "실행 시 플레이스홀더에 값을 지정하지 않으면 예외가 발생한다.\n",
    "\n",
    "플레이스홀더 노드를 만들려면 `placeholder()` 함수를 호출하고 출력 텐서의 데이터 타입을 지정해야 한다. 부가적으로 크기를 지정하여 강제할 수 있는데, 차원을 None으로 설정하면 어떤 크기도 가능하다는 의미가 된다.\n",
    "\n",
    "다음 코드는 플레이스홀더 노드 A와 B를 만들며, B를 평가할 때 `eval()` 메서드에 `feed_dict` 매개변수로 A의 값을 전달한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "                     \n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note.** 실제로는 플레이스홀더뿐만 아니라 어떤 연산의 출력값도 `feed_dict`를 통해 주입할 수 있다.\n",
    "\n",
    "이런 경우 텐서플로는 이 연산을 평가하지 않고 주입된 값을 사용한다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치 경사 하강법을 구현하기 위해서는 기존 코드를 조금만 변경하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18059373],\n",
       "       [ 0.7876107 ],\n",
       "       [ 0.15842117],\n",
       "       [-0.09852041],\n",
       "       [ 0.13126495],\n",
       "       [ 0.00959064],\n",
       "       [-0.04246187],\n",
       "       [-0.6675766 ],\n",
       "       [-0.62663466]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 모델 저장과 복원\n",
    "\n",
    "모델을 훈련시키고 나면 필요할 때 다시 쓸 수 있도록 모델 파라미터를 디스크에 저장해야 한다. \n",
    "\n",
    "또한 훈련하는 동안 일정한 간격으로 체크포인트를 저장해두면 컴퓨터가 훈련 중간에 문제를 일으켜도 마지막 체크포인트부터 이어나갈 수 있다.\n",
    "\n",
    "텐서플로에서 모델을 저장하는 일은 매우 쉽다. 구성 단계의 끝에서 (모든 변수 노드를 생성한 후) `Saver` 노드를 추가하고, 실행 단계에서 `save()` 메서드에 세션과 체크포인트 파일의 경로를 전달하여 호출하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"models/tf_save/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반대로 복원하려면 `init` 노드를 사용하여 변수를 초기화하는 대신 `Saver` 객체의 `restore()` 메서드를 호출하면 된다.\n",
    "\n",
    "이렇게 하면 원래 만들었던 코드를 찾아보지 않고도 그래프 구조와 변수값을 포함해 저장된 모델을 완벽하게 복원할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 텐서보드로 그래프와 학습 곡선 시각화하기\n",
    "\n",
    "지금까지 미니배치 경사 하강법을 사용하는 선형 회귀 모델을 훈련시킬 계산 그래프를 만들었고, 일정한 간격으로 체크포인트를 저장하고 있다.\n",
    "\n",
    "하지만 여전히 훈련 과정을 `print()` 함수에 의존하고 있다. 이보다 나은 방법으로는 텐서보드가 있다.\n",
    "\n",
    "텐서보드에 훈련 통곗값을 전달하면 브라우저에서 멋진 반응형 그래프를 보여준다. 또한 계산 그래프의 정의를 사용하여 그래프 구조를 살펴볼 수 있는 인터페이스를 제공한다.\n",
    "\n",
    "이는 그래프에서 에러나 병목점 등을 확인하는 데 아주 유용하다. \n",
    "\n",
    "먼저 그래프 정의와 훈련 통계를 텐서보드가 읽을 수 있는 로그 디렉터리에 쓰도록 프로그램을 조금 수정해야 한다.\n",
    "\n",
    "프로그램을 실행할 때마다 다른 로그 디렉토리를 사용하여야 한다. 그렇지 않으면 프로그램을 실행할 때마다 만들어진 통계가 합쳐져 텐서보드가 엉망이 될 것이다.\n",
    "\n",
    "로그 디렉터리 이름에 타임스탬프를 포함하면 간단하게 해결된다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 줄은 MSE값을 평가하고 그것을 **서머리 Summary**라고 부르는 텐서보드가 인식하는 이진 로그 문자열에 쓰기 위한 노드를 그래프에 추가한다.\n",
    "\n",
    "두 번째 줄은 `FileWriter`라는 객체를 만들어 로그 디렉토링에 있는 로그 파일에 서머리를 기록한다.\n",
    "\n",
    "첫 번째 매개변수는 로그 디렉토리의 경로를 나타내며, 두 번째 매개변수는 시각화하고자 하는 계산 그래프이다.\n",
    "\n",
    "그런 다음 실행 단계에서 훈련하는 동안 mse_summary 노드를 정기적으로 (예를 들어 미니배치 10회마다) 평가하도록 수정해야 한다.\n",
    "\n",
    "이렇게 하여 만들어진 서머리는 `file_writer`를 사용해 이벤트 파일에 기록할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y:y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"models/tf_save/my_model_final.ckpt\")\n",
    "    \n",
    "    \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 텐서보드 서버를 실행해보자. 가상환경을 만들었다면 이를 활성화하고 최상위 로그 디렉토리를 옵션으로 주어 `tensorboard` 명령을 실행해 서버를 시작시킨다.\n",
    "\n",
    "그러면 텐서보드 서버가 포트 6006 (goog를 뒤집은 숫자)에서 시작된다.\n",
    "\n",
    "이 안에서 시간에 따른 데이터의 추이와 그래프 대시보드를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 이름 범위\n",
    "\n",
    "신경망처럼 매우 복잡한 모델을 다룰 때는 계산 그래프가 수천 개의 노드로 인해 어질러지기 쉽다.\n",
    "\n",
    "이를 피하려면 **이름 범위 Name Scope**를 만들어 관련 있는 노드들을 그룹으로 묶어야 한다.\n",
    "\n",
    "예를 들어 이전 코드를 수정해 \"loss\" 이름 범위 안에 있는 error와 mse를 정의해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 하면 텐서보드에서 같은 이름 범위 내에 노드들이 위치하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 모듈화\n",
    "\n",
    "두 개의 **ReLu Rectified Linear Unit** 출력을 더하는 그래프를 만든다고 가정해보자. ReLU는 입력에 대한 선형 함수로서 양수는 그대로 출력하고 음수일 때는 0을 출력한다.\n",
    "\n",
    "다음은 이 작업을 수행하는 코드인데, 반복되는 부분이 많다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z2, 0., name=\"relu2\")\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 반복적인 코드는 유지 보수하기 어렵고 에러가 발생하기 쉽다. \n",
    "\n",
    "ReLU 함수를 더 추가해야 한다면 상황은 더 심각해진다. 다행히 텐서플로는 DRY(Don't Repeat Yourself) 원칙을 유지하게 도와준다.\n",
    "\n",
    "정말 간단한 방법으로, ReLU를 구현하는 함수를 만들면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노드가 생성될 때 텐서플로는 그 이름이 이미 존재하는지 확인한다. 만약 같은 이름이 존재하면, 밑줄 다음에 숫자를 붙여 고유 이름으로 만든다.\n",
    "\n",
    "텐서보드는 이런 노드 시리즈를 인식하고 하나로 합쳐 번잡함을 줄인다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. 변수 공유\n",
    "\n",
    "그래프의 여러 구성 요소 간에 변수를 공유하고 싶다면, 간단한 해결 방법은 변수를 먼저 만들고 필요한 함수에 매개변수로 전달하는 것이다.\n",
    "\n",
    "예를 들어 ReLU의 임곗값을 조정하기 위해 threshold 변수를 모든 ReLU에 공유하려 한다고 하자. 먼저 변수를 만들고 relu() 함수에 전달한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, 0., name=\"relu\")\n",
    "    \n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 해도 문제없이 작동한다. 하지만 공유 변수가 많으면 항상 매개변수로 전달해야 하므로 번거로워진다.\n",
    "\n",
    "많은 사람이 파이썬 딕셔너리를 만들고 함수마다 이를 전달하는 방식을 사용하고, 어떤 사람은 파이썬 클래스를 활용하여 클래스 변수로 공유 매개변수를 쓴다.\n",
    "\n",
    "또 하나의 선택은 `relu()`를 맨 처음 호출할 때 함수의 속성으로 다음과 같이 공유 변수를 지정하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(3.0, name=\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로에서는 이것보다 더 깔끔하고 모듈화하기 좋은 방법을 제공한다. 이 방식은 처음엔 이해하기 조금 까다롭지만, 텐서플로에서 많이 사용된다.\n",
    "\n",
    "기본 아이디어는 `get_variable()` 함수를 사용해 공유 변수가 아직 존재하지 않을 때는 새로 만들고 이미 있을 때는 재사용하는 것이다.\n",
    "\n",
    "상황에 맞는 동작은 현재 `variable_scope()`의 속성값으로 결정된다. 예를 들어 다음 코드는 relu/threshold 변수를 생성할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                               initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 이 변수가 이전의 `get_variable()` 호출에서 이미 생성되었다면 이 코드는 예외를 발생할 것이다.\n",
    "\n",
    "이런 동작 방식은 실수로 변수가 사용되는 것을 막아준다. 변수를 재사용하고 싶다면 명시적으로 변수 범위의 `reuse` 속성을 True로 지정하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 매개변수로 전달하지 않고 threshold 변수를 공유하도록 relu() 함수를 만들기 위한 준비가 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "    \n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                               initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 먼저 `relu()` 함수를 정의하고, `relu/threshold` 변수를 생성한다.\n",
    "\n",
    "그러고 나서 `relu()` 함수를 호출해 다섯 개의 ReLU를 만든다. 여기서 `relu()` 함수는 `relu/threshold` 변수를 재사용하여 ReLU 노드를 만든다.\n",
    "\n",
    "모든 ReLU 코드가 `relu()` 함수 안에 있지만, `threshold` 변수는 함수 밖에서 정의되어야 한다. 이를 개선하기 위해 다음 코드는 처음 호출될 때 함수 안에서 `threshold`를 생성하고 재사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "    \n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                               initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 경우, 공유 변수가 첫 번째 ReLU 안에 들어가 있기 때문에 앞과는 그래프 형식이 조금 다르다.\n",
    "\n",
    "이것으로 텐서플로 소개를 마치며, 이어지는 장에서는 더 수준 높은 주제를 다룬다.\n",
    "\n",
    "특히 **심층 신경망 Deep Neural Network**, 합성곱 신경망 **Convolution Neural Network**, 순환 신경망 **Recurrent Neural Network**과 관련된 다양한 연산은 물론, \n",
    "\n",
    "멀티스레딩, 큐, 다중 GPU, 다중 서버를 사용한 텐서플로의 스케일 확장 방법을 논의한다.\n",
    "\n",
    "*여기는 연습문제 꼭 해보기*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
