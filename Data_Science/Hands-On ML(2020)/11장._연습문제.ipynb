{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. He 초기화를 사용하여 무작위로 선택한 값이라면 모든 가중치를 같은 값으로 초기화해도 괜찮을까?\n",
    "\n",
    "(답변) 안 된다. 애초에 초기화를 하는 이유는 가중치마다 서로 다른 임의의 값을 갖게 하기 위함이기 때문이다. \n",
    "\n",
    "### Q2. 편향을 0으로 초기화해도 괜찮을까?\n",
    "\n",
    "(답변) 편향은 초기화하지 않아도 무관하기 때문에 상관 없다. 합연산이기 때문에 ?\n",
    "\n",
    "### Q3. ReLU보다 ELU 활성화 함수가 나은 세 가지는 무엇일까요?\n",
    "\n",
    "(답변) 음의 값이 나와도 미분이 가능하다. 학습 속도가 빠르다. 유연하다.\n",
    "\n",
    "### Q4. 어떤 경우에 ELU, LeakyReLU, ReLU, tanh, logistic, softmax와 같은 활성화 함수를 사용해야 할까?\n",
    "\n",
    "(답변) 출력이 분류인 문제일 경우에 사용한다.\n",
    "\n",
    "### Q5. MomentumOptimizer를 사용할 때 momentum 하이퍼파라미터를 너무 1에 가깝게 하면 어떤 일이 일어날까?\n",
    "\n",
    "(답변) 발산한다.\n",
    "\n",
    "### Q6. 희소 모델을 만들 수 있는 세 가지 방법은?\n",
    "\n",
    "(답변) 모른다.\n",
    "\n",
    "### Q7. 드롭아웃이 훈련 속도를 느리게 만들까? 추론(새로운 샘플에 대한 예측을 만드는 것)도 느리게 만들까?\n",
    "\n",
    "(답변) 뉴런의 수가 줄어들기 때문에 더 빨라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. 딥러닝 실습\n",
    "\n",
    "- a. He 초기화와 ELU 활성화 함수를 사용하여 각각 뉴런이 100개인 은닉층 다섯 개를 가진 DNN을 만드세요.\n",
    "- b. Adam 최적화와 조기 종료를 사용하여 MNIST 데이터셋에 훈련시키되 0에서 4까지의 숫자만 사용하세요. 다음 연습문제에서 5에서 9까지의 숫자에 대해 전이 학습을 사용할 것입니다. 출력층은 다섯 개의 뉴런에 소프트맥스 함수를 사용하고, 나중에 재사용할 수 있도록 항상 일정한 간격으로 체크포인트와 최종 모델을 저장하세요.\n",
    "- c. 교차 검증을 사용하여 하이퍼파라미터를 튜닝하고 얼마의 성능을 달성할 수 있는지 확인해보세요.\n",
    "- d. 배치 정규화를 추가한 다음 학습 곡선을 비교해보세요. 이전보다 수렴이 빨라졌나요? 모델의 성능이 더 나아졌나요?\n",
    "- e. 모델이 훈련 세트에 과대적합되었나요? 모든 층에 드롭아웃을 적용하고 다시 시도해보세요. 도움이 되었나요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 기본 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Datasets\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_train = X_train[y_train <= 4]\n",
    "X_test = X_test[y_test <= 4]\n",
    "y_train = y_train[y_train <= 4]\n",
    "y_test = y_test[y_test <= 4]\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "# Prepare Functions\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28 #MNIST\n",
    "n_hidden1, n_hidden2, n_hidden3, n_hidden4, n_hidden5 = [100] * 5\n",
    "n_outputs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Tensorflow Preparation\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "# Tensorflow Graph Declaration\n",
    "## X, Y Var\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "## DNN\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, name=\"hidden4\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, name=\"hidden5\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"logits\", kernel_initializer=he_init)\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "## LOSS\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "## Training\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "## Eval\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 손실: 0.1143729 최선의 손실: 0.1143729 검증 세트 정확도: 0.9684\n",
      "1 검증 세트 손실: 0.7052579 최선의 손실: 0.1143729 검증 세트 정확도: 0.9286\n",
      "2 검증 세트 손실: 0.08815428 최선의 손실: 0.1143729 검증 세트 정확도: 0.9738\n",
      "3 검증 세트 손실: 0.14425915 최선의 손실: 0.1143729 검증 세트 정확도: 0.9634\n",
      "4 검증 세트 손실: 0.12995398 최선의 손실: 0.1143729 검증 세트 정확도: 0.9656\n",
      "5 검증 세트 손실: 0.05894281 최선의 손실: 0.1143729 검증 세트 정확도: 0.9868\n",
      "6 검증 세트 손실: 0.07724715 최선의 손실: 0.1143729 검증 세트 정확도: 0.981\n",
      "7 검증 세트 손실: 0.08882479 최선의 손실: 0.1143729 검증 세트 정확도: 0.9746\n",
      "8 검증 세트 손실: 0.08154777 최선의 손실: 0.1143729 검증 세트 정확도: 0.9844\n",
      "9 검증 세트 손실: 0.1851009 최선의 손실: 0.1143729 검증 세트 정확도: 0.9802\n",
      "10 검증 세트 손실: 0.09481877 최선의 손실: 0.1143729 검증 세트 정확도: 0.9746\n",
      "11 검증 세트 손실: 0.11775906 최선의 손실: 0.1143729 검증 세트 정확도: 0.976\n",
      "12 검증 세트 손실: 0.09208188 최선의 손실: 0.1143729 검증 세트 정확도: 0.9812\n",
      "13 검증 세트 손실: 0.09289459 최선의 손실: 0.1143729 검증 세트 정확도: 0.9822\n",
      "14 검증 세트 손실: 0.082883 최선의 손실: 0.1143729 검증 세트 정확도: 0.9844\n",
      "15 검증 세트 손실: 0.07850165 최선의 손실: 0.1143729 검증 세트 정확도: 0.9858\n",
      "16 검증 세트 손실: 0.27883676 최선의 손실: 0.1143729 검증 세트 정확도: 0.9768\n",
      "17 검증 세트 손실: 0.28988734 최선의 손실: 0.1143729 검증 세트 정확도: 0.9582\n",
      "18 검증 세트 손실: 0.21325134 최선의 손실: 0.1143729 검증 세트 정확도: 0.9826\n",
      "19 검증 세트 손실: 0.17942443 최선의 손실: 0.1143729 검증 세트 정확도: 0.9818\n",
      "조기 종료!\n"
     ]
    }
   ],
   "source": [
    "## Execution\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "best_loss = np.inf\n",
    "max_try = 20\n",
    "current_try = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        save_path = saver.save(sess, \"models/tmp/chap11_exercise.ckpt\")\n",
    "\n",
    "        \n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_valid = loss.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        \n",
    "        if acc_valid < best_loss:\n",
    "            save_path = saver.save(sess, \"models/chap11_exercise.ckpt\")\n",
    "            best_loss = loss_valid\n",
    "            current_try = 0\n",
    "        else:\n",
    "            current_try += 1\n",
    "            if current_try >= max_try:\n",
    "                print(\"조기 종료!\")\n",
    "                break\n",
    "        print(epoch, \"검증 세트 손실:\", loss_valid, \"최선의 손실:\", best_loss, \"검증 세트 정확도:\", acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from models/chap11_exercise.ckpt\n",
      "최종 테스트 정확도: 97.94%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/chap11_exercise.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "    print(\"최종 테스트 정확도: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. 교차 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.22'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers = 5, n_neurons = 100, optimizer_class = tf.train.AdamOptimizer, learning_rate = 0.01, \n",
    "                 batch_size = 20, activation = tf.nn.elu, initializer = he_init, batch_norm_momentum = None, dropout_rate = None, random_staet = None ):\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons, kernel_initializer=self.initializer, name=\"hidden%d\" % (layer + 1))\n",
    "\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum, training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "        \n",
    "    def _close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "            \n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        # Tensorflow Graph Declaration\n",
    "        ## X, Y Var\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "            \n",
    "        ## DNN\n",
    "        dnn_outputs = self._dnn(X)\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, name=\"logits\", kernel_initializer=he_init)\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "        \n",
    "        ## LOSS\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        ## Training\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        ## Eval\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))    \n",
    "        \n",
    "        ## Init & Save\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"모든 변수 값을 가져옵니다 (조기 종료를 위해 사용하며 디스크에 저장하는 것보다 빠릅니다)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "    \n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"모든 변수를 주어진 값으로 설정합니다 (조기 종료를 위해 사용하며 디스크에 저장하는 것보다 빠릅니다)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "        \n",
    "    def fit(self, X, y, n_epochs=1000, X_valid=None, y_valid=None):\n",
    "        \"\"\"훈련 세트에 모델을 훈련시킵니다. X_valid와 y_valid가 주어지면 조기 종료를 적용합니다.\"\"\"\n",
    "        self._close_session()\n",
    "\n",
    "        # 훈련 세트로부터 n_inputs와 n_outputs를 구합니다.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # 레이블 벡터를 정렬된 클래스 인덱스 벡터로 변환합니다.\n",
    "        # 0부터 n_outputs - 1까지의 정수를 담고 있게 됩니다.\n",
    "        # 예를 들어, y가 [8, 8, 9, 5, 7, 6, 6, 6]이면 \n",
    "        # 정렬된 클래스 레이블(self.classes_)은 [5, 6, 7, 8, 9]가 되고\n",
    "        # 레이블 벡터는 [3, 3, 4, 0, 2, 1, 1, 1]로 변환됩니다.\n",
    "        self.class_to_index_ = {label: index\n",
    "                                for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # 배치 정규화를 위한 추가 연산\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        # 조기 종료를 위해\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        # 이제 모델을 훈련합니다!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\t검증 세트 손실: {:.6f}\\t최선의 손실: {:.6f}\\t정확도: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"조기 종료!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch,\n",
    "                                                                self._y: y_batch})\n",
    "                    print(\"{}\\t마지막 훈련 배치 손실: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # 조기 종료를 사용하면 이전의 최상의 모델로 되돌립니다.\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"%s 객체가 아직 훈련되지 않았습니다\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t검증 세트 손실: 0.109910\t최선의 손실: 0.109910\t정확도: 97.62%\n",
      "1\t검증 세트 손실: 0.192316\t최선의 손실: 0.109910\t정확도: 95.23%\n",
      "2\t검증 세트 손실: 0.113300\t최선의 손실: 0.109910\t정확도: 97.54%\n",
      "3\t검증 세트 손실: 0.206293\t최선의 손실: 0.109910\t정확도: 95.35%\n",
      "4\t검증 세트 손실: 0.131957\t최선의 손실: 0.109910\t정확도: 97.65%\n",
      "5\t검증 세트 손실: 0.234273\t최선의 손실: 0.109910\t정확도: 95.78%\n",
      "6\t검증 세트 손실: 0.132155\t최선의 손실: 0.109910\t정확도: 97.46%\n",
      "7\t검증 세트 손실: 0.157653\t최선의 손실: 0.109910\t정확도: 96.64%\n",
      "8\t검증 세트 손실: 0.131527\t최선의 손실: 0.109910\t정확도: 97.42%\n",
      "9\t검증 세트 손실: 0.103936\t최선의 손실: 0.103936\t정확도: 97.97%\n",
      "10\t검증 세트 손실: 1.700996\t최선의 손실: 0.103936\t정확도: 18.73%\n",
      "11\t검증 세트 손실: 1.687720\t최선의 손실: 0.103936\t정확도: 22.01%\n",
      "12\t검증 세트 손실: 1.658944\t최선의 손실: 0.103936\t정확도: 22.01%\n",
      "13\t검증 세트 손실: 1.705560\t최선의 손실: 0.103936\t정확도: 18.73%\n",
      "14\t검증 세트 손실: 1.660976\t최선의 손실: 0.103936\t정확도: 19.27%\n",
      "15\t검증 세트 손실: 1.780466\t최선의 손실: 0.103936\t정확도: 19.27%\n",
      "16\t검증 세트 손실: 1.628658\t최선의 손실: 0.103936\t정확도: 22.01%\n",
      "17\t검증 세트 손실: 1.615816\t최선의 손실: 0.103936\t정확도: 22.01%\n",
      "18\t검증 세트 손실: 1.692735\t최선의 손실: 0.103936\t정확도: 19.08%\n",
      "19\t검증 세트 손실: 1.669257\t최선의 손실: 0.103936\t정확도: 19.08%\n",
      "20\t검증 세트 손실: 1.704453\t최선의 손실: 0.103936\t정확도: 22.01%\n",
      "21\t검증 세트 손실: 1.701767\t최선의 손실: 0.103936\t정확도: 19.08%\n",
      "22\t검증 세트 손실: 1.684231\t최선의 손실: 0.103936\t정확도: 19.08%\n",
      "23\t검증 세트 손실: 1.667076\t최선의 손실: 0.103936\t정확도: 22.01%\n",
      "24\t검증 세트 손실: 1.698178\t최선의 손실: 0.103936\t정확도: 22.01%\n",
      "25\t검증 세트 손실: 1.661047\t최선의 손실: 0.103936\t정확도: 18.73%\n",
      "26\t검증 세트 손실: 1.657812\t최선의 손실: 0.103936\t정확도: 22.01%\n",
      "27\t검증 세트 손실: 1.724535\t최선의 손실: 0.103936\t정확도: 19.27%\n",
      "28\t검증 세트 손실: 1.640954\t최선의 손실: 0.103936\t정확도: 22.01%\n",
      "29\t검증 세트 손실: 1.709712\t최선의 손실: 0.103936\t정확도: 19.27%\n",
      "30\t검증 세트 손실: 1.645659\t최선의 손실: 0.103936\t정확도: 19.08%\n",
      "조기 종료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\sklearn\\base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function elu at 0x000002AD974CD048>,\n",
       "              batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x000002AD9C5E88C8>,\n",
       "              learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_staet=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = DNNClassifier()\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9846273594084453"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # 은닉층의 수나 옵티마이저 등을 달리하여 탐색해 볼 수 있습니다.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "}\n",
    "\n",
    "# 생성자에 넘겨주는 fit_params 매개변수는 사이킷런 0.19 버전부터 경고가 발생하고 0.21 버전에서 사라지므로\n",
    "# 대신 fit() 메서드에 매개변수로 전달해야 합니다.:\n",
    "rnd_search = RandomizedSearchCV(DNNClassifier(), param_distribs, n_iter=50,\n",
    "                                random_state=42, verbose=2, cv=3)\n",
    "\n",
    "rnd_search.fit(X = X_train1, y = y_train1, X_valid = X_valid1, y_valid = y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. 전이 학습\n",
    "- a. 이전 모델에서 미리 학습한 은닉층을 모두 재사용하는 새로운 DNN을 만드세요. 이 은닉층을 동결시키고 소프트맥스 출력층은 새로운 것으로 바꾸세요.\n",
    "- b. 이 새로운 DNN을 숫자 5~9에 대해 숫자마다 100개의 이미지만 사용해 훈련시켜 보고 얼마나 시간이 걸리는지 재보세요. 작은 양의 샘플만으로도 높은 성능을 얻을 수 있나요?\n",
    "- c. 동결된 층을 캐싱하고 모델을 다시 훈련시켜보세요. 얼마나 빨라졌나요?\n",
    "- d. 다섯 개 대신 네 개의 은닉층만 재사용하여 다시 시도해보세요. 높은 성능을 얻을 수 있나요?\n",
    "- e. 이제 최상단 두 개 층의 동결을 해제하고 훈련을 계속 해보세요. 더 나은 모델을 얻을 수 있나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/truncated_normal/shape\n",
      "hidden1/kernel/Initializer/truncated_normal/mean\n",
      "hidden1/kernel/Initializer/truncated_normal/stddev\n",
      "hidden1/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden1/kernel/Initializer/truncated_normal/mul\n",
      "hidden1/kernel/Initializer/truncated_normal\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Elu\n",
      "hidden2/kernel/Initializer/truncated_normal/shape\n",
      "hidden2/kernel/Initializer/truncated_normal/mean\n",
      "hidden2/kernel/Initializer/truncated_normal/stddev\n",
      "hidden2/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden2/kernel/Initializer/truncated_normal/mul\n",
      "hidden2/kernel/Initializer/truncated_normal\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Elu\n",
      "hidden3/kernel/Initializer/truncated_normal/shape\n",
      "hidden3/kernel/Initializer/truncated_normal/mean\n",
      "hidden3/kernel/Initializer/truncated_normal/stddev\n",
      "hidden3/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden3/kernel/Initializer/truncated_normal/mul\n",
      "hidden3/kernel/Initializer/truncated_normal\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Elu\n",
      "hidden4/kernel/Initializer/truncated_normal/shape\n",
      "hidden4/kernel/Initializer/truncated_normal/mean\n",
      "hidden4/kernel/Initializer/truncated_normal/stddev\n",
      "hidden4/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden4/kernel/Initializer/truncated_normal/mul\n",
      "hidden4/kernel/Initializer/truncated_normal\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Elu\n",
      "hidden5/kernel/Initializer/truncated_normal/shape\n",
      "hidden5/kernel/Initializer/truncated_normal/mean\n",
      "hidden5/kernel/Initializer/truncated_normal/stddev\n",
      "hidden5/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden5/kernel/Initializer/truncated_normal/mul\n",
      "hidden5/kernel/Initializer/truncated_normal\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Elu\n",
      "outputs/kernel/Initializer/truncated_normal/shape\n",
      "outputs/kernel/Initializer/truncated_normal/mean\n",
      "outputs/kernel/Initializer/truncated_normal/stddev\n",
      "outputs/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "outputs/kernel/Initializer/truncated_normal/mul\n",
      "outputs/kernel/Initializer/truncated_normal\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "train/gradients/Shape\n",
      "train/gradients/grad_ys_0\n",
      "train/gradients/Fill\n",
      "train/gradients/loss/loss_grad/Reshape/shape\n",
      "train/gradients/loss/loss_grad/Reshape\n",
      "train/gradients/loss/loss_grad/Shape\n",
      "train/gradients/loss/loss_grad/Tile\n",
      "train/gradients/loss/loss_grad/Shape_1\n",
      "train/gradients/loss/loss_grad/Shape_2\n",
      "train/gradients/loss/loss_grad/Const\n",
      "train/gradients/loss/loss_grad/Prod\n",
      "train/gradients/loss/loss_grad/Const_1\n",
      "train/gradients/loss/loss_grad/Prod_1\n",
      "train/gradients/loss/loss_grad/Maximum/y\n",
      "train/gradients/loss/loss_grad/Maximum\n",
      "train/gradients/loss/loss_grad/floordiv\n",
      "train/gradients/loss/loss_grad/Cast\n",
      "train/gradients/loss/loss_grad/truediv\n",
      "train/gradients/zeros_like\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "train/gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden5/Elu_grad/EluGrad\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden4/Elu_grad/EluGrad\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden3/Elu_grad/EluGrad\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden2/Elu_grad/EluGrad\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden1/Elu_grad/EluGrad\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "train/beta1_power/initial_value\n",
      "train/beta1_power\n",
      "train/beta1_power/Assign\n",
      "train/beta1_power/read\n",
      "train/beta2_power/initial_value\n",
      "train/beta2_power\n",
      "train/beta2_power/Assign\n",
      "train/beta2_power/read\n",
      "hidden1/kernel/Adam/Initializer/zeros/shape_as_tensor\n",
      "hidden1/kernel/Adam/Initializer/zeros/Const\n",
      "hidden1/kernel/Adam/Initializer/zeros\n",
      "hidden1/kernel/Adam\n",
      "hidden1/kernel/Adam/Assign\n",
      "hidden1/kernel/Adam/read\n",
      "hidden1/kernel/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "hidden1/kernel/Adam_1/Initializer/zeros/Const\n",
      "hidden1/kernel/Adam_1/Initializer/zeros\n",
      "hidden1/kernel/Adam_1\n",
      "hidden1/kernel/Adam_1/Assign\n",
      "hidden1/kernel/Adam_1/read\n",
      "hidden1/bias/Adam/Initializer/zeros\n",
      "hidden1/bias/Adam\n",
      "hidden1/bias/Adam/Assign\n",
      "hidden1/bias/Adam/read\n",
      "hidden1/bias/Adam_1/Initializer/zeros\n",
      "hidden1/bias/Adam_1\n",
      "hidden1/bias/Adam_1/Assign\n",
      "hidden1/bias/Adam_1/read\n",
      "hidden2/kernel/Adam/Initializer/zeros/shape_as_tensor\n",
      "hidden2/kernel/Adam/Initializer/zeros/Const\n",
      "hidden2/kernel/Adam/Initializer/zeros\n",
      "hidden2/kernel/Adam\n",
      "hidden2/kernel/Adam/Assign\n",
      "hidden2/kernel/Adam/read\n",
      "hidden2/kernel/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "hidden2/kernel/Adam_1/Initializer/zeros/Const\n",
      "hidden2/kernel/Adam_1/Initializer/zeros\n",
      "hidden2/kernel/Adam_1\n",
      "hidden2/kernel/Adam_1/Assign\n",
      "hidden2/kernel/Adam_1/read\n",
      "hidden2/bias/Adam/Initializer/zeros\n",
      "hidden2/bias/Adam\n",
      "hidden2/bias/Adam/Assign\n",
      "hidden2/bias/Adam/read\n",
      "hidden2/bias/Adam_1/Initializer/zeros\n",
      "hidden2/bias/Adam_1\n",
      "hidden2/bias/Adam_1/Assign\n",
      "hidden2/bias/Adam_1/read\n",
      "hidden3/kernel/Adam/Initializer/zeros/shape_as_tensor\n",
      "hidden3/kernel/Adam/Initializer/zeros/Const\n",
      "hidden3/kernel/Adam/Initializer/zeros\n",
      "hidden3/kernel/Adam\n",
      "hidden3/kernel/Adam/Assign\n",
      "hidden3/kernel/Adam/read\n",
      "hidden3/kernel/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "hidden3/kernel/Adam_1/Initializer/zeros/Const\n",
      "hidden3/kernel/Adam_1/Initializer/zeros\n",
      "hidden3/kernel/Adam_1\n",
      "hidden3/kernel/Adam_1/Assign\n",
      "hidden3/kernel/Adam_1/read\n",
      "hidden3/bias/Adam/Initializer/zeros\n",
      "hidden3/bias/Adam\n",
      "hidden3/bias/Adam/Assign\n",
      "hidden3/bias/Adam/read\n",
      "hidden3/bias/Adam_1/Initializer/zeros\n",
      "hidden3/bias/Adam_1\n",
      "hidden3/bias/Adam_1/Assign\n",
      "hidden3/bias/Adam_1/read\n",
      "hidden4/kernel/Adam/Initializer/zeros/shape_as_tensor\n",
      "hidden4/kernel/Adam/Initializer/zeros/Const\n",
      "hidden4/kernel/Adam/Initializer/zeros\n",
      "hidden4/kernel/Adam\n",
      "hidden4/kernel/Adam/Assign\n",
      "hidden4/kernel/Adam/read\n",
      "hidden4/kernel/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "hidden4/kernel/Adam_1/Initializer/zeros/Const\n",
      "hidden4/kernel/Adam_1/Initializer/zeros\n",
      "hidden4/kernel/Adam_1\n",
      "hidden4/kernel/Adam_1/Assign\n",
      "hidden4/kernel/Adam_1/read\n",
      "hidden4/bias/Adam/Initializer/zeros\n",
      "hidden4/bias/Adam\n",
      "hidden4/bias/Adam/Assign\n",
      "hidden4/bias/Adam/read\n",
      "hidden4/bias/Adam_1/Initializer/zeros\n",
      "hidden4/bias/Adam_1\n",
      "hidden4/bias/Adam_1/Assign\n",
      "hidden4/bias/Adam_1/read\n",
      "hidden5/kernel/Adam/Initializer/zeros/shape_as_tensor\n",
      "hidden5/kernel/Adam/Initializer/zeros/Const\n",
      "hidden5/kernel/Adam/Initializer/zeros\n",
      "hidden5/kernel/Adam\n",
      "hidden5/kernel/Adam/Assign\n",
      "hidden5/kernel/Adam/read\n",
      "hidden5/kernel/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "hidden5/kernel/Adam_1/Initializer/zeros/Const\n",
      "hidden5/kernel/Adam_1/Initializer/zeros\n",
      "hidden5/kernel/Adam_1\n",
      "hidden5/kernel/Adam_1/Assign\n",
      "hidden5/kernel/Adam_1/read\n",
      "hidden5/bias/Adam/Initializer/zeros\n",
      "hidden5/bias/Adam\n",
      "hidden5/bias/Adam/Assign\n",
      "hidden5/bias/Adam/read\n",
      "hidden5/bias/Adam_1/Initializer/zeros\n",
      "hidden5/bias/Adam_1\n",
      "hidden5/bias/Adam_1/Assign\n",
      "hidden5/bias/Adam_1/read\n",
      "outputs/kernel/Adam/Initializer/zeros\n",
      "outputs/kernel/Adam\n",
      "outputs/kernel/Adam/Assign\n",
      "outputs/kernel/Adam/read\n",
      "outputs/kernel/Adam_1/Initializer/zeros\n",
      "outputs/kernel/Adam_1\n",
      "outputs/kernel/Adam_1/Assign\n",
      "outputs/kernel/Adam_1/read\n",
      "outputs/bias/Adam/Initializer/zeros\n",
      "outputs/bias/Adam\n",
      "outputs/bias/Adam/Assign\n",
      "outputs/bias/Adam/read\n",
      "outputs/bias/Adam_1/Initializer/zeros\n",
      "outputs/bias/Adam_1\n",
      "outputs/bias/Adam_1/Assign\n",
      "outputs/bias/Adam_1/read\n",
      "train/Adam/learning_rate\n",
      "train/Adam/beta1\n",
      "train/Adam/beta2\n",
      "train/Adam/epsilon\n",
      "train/Adam/update_hidden1/kernel/ApplyAdam\n",
      "train/Adam/update_hidden1/bias/ApplyAdam\n",
      "train/Adam/update_hidden2/kernel/ApplyAdam\n",
      "train/Adam/update_hidden2/bias/ApplyAdam\n",
      "train/Adam/update_hidden3/kernel/ApplyAdam\n",
      "train/Adam/update_hidden3/bias/ApplyAdam\n",
      "train/Adam/update_hidden4/kernel/ApplyAdam\n",
      "train/Adam/update_hidden4/bias/ApplyAdam\n",
      "train/Adam/update_hidden5/kernel/ApplyAdam\n",
      "train/Adam/update_hidden5/bias/ApplyAdam\n",
      "train/Adam/update_outputs/kernel/ApplyAdam\n",
      "train/Adam/update_outputs/bias/ApplyAdam\n",
      "train/Adam/mul\n",
      "train/Adam/Assign\n",
      "train/Adam/mul_1\n",
      "train/Adam/Assign_1\n",
      "train/Adam\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/Mean\n",
      "init\n",
      "save/filename/input\n",
      "save/filename\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/Assign_12\n",
      "save/Assign_13\n",
      "save/Assign_14\n",
      "save/Assign_15\n",
      "save/Assign_16\n",
      "save/Assign_17\n",
      "save/Assign_18\n",
      "save/Assign_19\n",
      "save/Assign_20\n",
      "save/Assign_21\n",
      "save/Assign_22\n",
      "save/Assign_23\n",
      "save/Assign_24\n",
      "save/Assign_25\n",
      "save/Assign_26\n",
      "save/Assign_27\n",
      "save/Assign_28\n",
      "save/Assign_29\n",
      "save/Assign_30\n",
      "save/Assign_31\n",
      "save/Assign_32\n",
      "save/Assign_33\n",
      "save/Assign_34\n",
      "save/Assign_35\n",
      "save/Assign_36\n",
      "save/Assign_37\n",
      "save/restore_all\n",
      "X_1\n",
      "y_1\n",
      "hidden1/kernel_1/Initializer/truncated_normal/shape\n",
      "hidden1/kernel_1/Initializer/truncated_normal/mean\n",
      "hidden1/kernel_1/Initializer/truncated_normal/stddev\n",
      "hidden1/kernel_1/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden1/kernel_1/Initializer/truncated_normal/mul\n",
      "hidden1/kernel_1/Initializer/truncated_normal\n",
      "hidden1/kernel_1\n",
      "hidden1/kernel_1/Assign\n",
      "hidden1/kernel_1/read\n",
      "hidden1/bias_1/Initializer/zeros\n",
      "hidden1/bias_1\n",
      "hidden1/bias_1/Assign\n",
      "hidden1/bias_1/read\n",
      "dnn/hidden1/MatMul_1\n",
      "dnn/hidden1/BiasAdd_1\n",
      "dnn/hidden1/Elu_1\n",
      "hidden2/kernel_1/Initializer/truncated_normal/shape\n",
      "hidden2/kernel_1/Initializer/truncated_normal/mean\n",
      "hidden2/kernel_1/Initializer/truncated_normal/stddev\n",
      "hidden2/kernel_1/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden2/kernel_1/Initializer/truncated_normal/mul\n",
      "hidden2/kernel_1/Initializer/truncated_normal\n",
      "hidden2/kernel_1\n",
      "hidden2/kernel_1/Assign\n",
      "hidden2/kernel_1/read\n",
      "hidden2/bias_1/Initializer/zeros\n",
      "hidden2/bias_1\n",
      "hidden2/bias_1/Assign\n",
      "hidden2/bias_1/read\n",
      "dnn/hidden2/MatMul_1\n",
      "dnn/hidden2/BiasAdd_1\n",
      "dnn/hidden2/Elu_1\n",
      "hidden3/kernel_1/Initializer/truncated_normal/shape\n",
      "hidden3/kernel_1/Initializer/truncated_normal/mean\n",
      "hidden3/kernel_1/Initializer/truncated_normal/stddev\n",
      "hidden3/kernel_1/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden3/kernel_1/Initializer/truncated_normal/mul\n",
      "hidden3/kernel_1/Initializer/truncated_normal\n",
      "hidden3/kernel_1\n",
      "hidden3/kernel_1/Assign\n",
      "hidden3/kernel_1/read\n",
      "hidden3/bias_1/Initializer/zeros\n",
      "hidden3/bias_1\n",
      "hidden3/bias_1/Assign\n",
      "hidden3/bias_1/read\n",
      "dnn/hidden3/MatMul_1\n",
      "dnn/hidden3/BiasAdd_1\n",
      "dnn/hidden3/Elu_1\n",
      "hidden4/kernel_1/Initializer/truncated_normal/shape\n",
      "hidden4/kernel_1/Initializer/truncated_normal/mean\n",
      "hidden4/kernel_1/Initializer/truncated_normal/stddev\n",
      "hidden4/kernel_1/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden4/kernel_1/Initializer/truncated_normal/mul\n",
      "hidden4/kernel_1/Initializer/truncated_normal\n",
      "hidden4/kernel_1\n",
      "hidden4/kernel_1/Assign\n",
      "hidden4/kernel_1/read\n",
      "hidden4/bias_1/Initializer/zeros\n",
      "hidden4/bias_1\n",
      "hidden4/bias_1/Assign\n",
      "hidden4/bias_1/read\n",
      "dnn/hidden4/MatMul_1\n",
      "dnn/hidden4/BiasAdd_1\n",
      "dnn/hidden4/Elu_1\n",
      "hidden5/kernel_1/Initializer/truncated_normal/shape\n",
      "hidden5/kernel_1/Initializer/truncated_normal/mean\n",
      "hidden5/kernel_1/Initializer/truncated_normal/stddev\n",
      "hidden5/kernel_1/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden5/kernel_1/Initializer/truncated_normal/mul\n",
      "hidden5/kernel_1/Initializer/truncated_normal\n",
      "hidden5/kernel_1\n",
      "hidden5/kernel_1/Assign\n",
      "hidden5/kernel_1/read\n",
      "hidden5/bias_1/Initializer/zeros\n",
      "hidden5/bias_1\n",
      "hidden5/bias_1/Assign\n",
      "hidden5/bias_1/read\n",
      "dnn/hidden5/MatMul_1\n",
      "dnn/hidden5/BiasAdd_1\n",
      "dnn/hidden5/Elu_1\n",
      "outputs/kernel_1/Initializer/truncated_normal/shape\n",
      "outputs/kernel_1/Initializer/truncated_normal/mean\n",
      "outputs/kernel_1/Initializer/truncated_normal/stddev\n",
      "outputs/kernel_1/Initializer/truncated_normal/TruncatedNormal\n",
      "outputs/kernel_1/Initializer/truncated_normal/mul\n",
      "outputs/kernel_1/Initializer/truncated_normal\n",
      "outputs/kernel_1\n",
      "outputs/kernel_1/Assign\n",
      "outputs/kernel_1/read\n",
      "outputs/bias_1/Initializer/zeros\n",
      "outputs/bias_1\n",
      "outputs/bias_1/Assign\n",
      "outputs/bias_1/read\n",
      "dnn/outputs/MatMul_1\n",
      "dnn/outputs/BiasAdd_1\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape_1\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_1\n",
      "loss/Const_1\n",
      "loss/loss_1\n",
      "train/gradients/Shape_1\n",
      "train/gradients/grad_ys_0_1\n",
      "train/gradients/Fill_1\n",
      "train/gradients/loss/loss_1_grad/Reshape/shape\n",
      "train/gradients/loss/loss_1_grad/Reshape\n",
      "train/gradients/loss/loss_1_grad/Shape\n",
      "train/gradients/loss/loss_1_grad/Tile\n",
      "train/gradients/loss/loss_1_grad/Shape_1\n",
      "train/gradients/loss/loss_1_grad/Shape_2\n",
      "train/gradients/loss/loss_1_grad/Const\n",
      "train/gradients/loss/loss_1_grad/Prod\n",
      "train/gradients/loss/loss_1_grad/Const_1\n",
      "train/gradients/loss/loss_1_grad/Prod_1\n",
      "train/gradients/loss/loss_1_grad/Maximum/y\n",
      "train/gradients/loss/loss_1_grad/Maximum\n",
      "train/gradients/loss/loss_1_grad/floordiv\n",
      "train/gradients/loss/loss_1_grad/Cast\n",
      "train/gradients/loss/loss_1_grad/truediv\n",
      "train/gradients/zeros_like_1\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_1_grad/PreventGradient\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_1_grad/ExpandDims/dim\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_1_grad/ExpandDims\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_1_grad/mul\n",
      "train/gradients/dnn/outputs/BiasAdd_1_grad/BiasAddGrad\n",
      "train/gradients/dnn/outputs/BiasAdd_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/outputs/BiasAdd_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/outputs/BiasAdd_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/outputs/MatMul_1_grad/MatMul\n",
      "train/gradients/dnn/outputs/MatMul_1_grad/MatMul_1\n",
      "train/gradients/dnn/outputs/MatMul_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/outputs/MatMul_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/outputs/MatMul_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden5/Elu_1_grad/EluGrad\n",
      "train/gradients/dnn/hidden5/BiasAdd_1_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden5/BiasAdd_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden5/BiasAdd_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden5/BiasAdd_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden5/MatMul_1_grad/MatMul\n",
      "train/gradients/dnn/hidden5/MatMul_1_grad/MatMul_1\n",
      "train/gradients/dnn/hidden5/MatMul_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden5/MatMul_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden5/MatMul_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden4/Elu_1_grad/EluGrad\n",
      "train/gradients/dnn/hidden4/BiasAdd_1_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden4/BiasAdd_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden4/BiasAdd_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden4/BiasAdd_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden4/MatMul_1_grad/MatMul\n",
      "train/gradients/dnn/hidden4/MatMul_1_grad/MatMul_1\n",
      "train/gradients/dnn/hidden4/MatMul_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden4/MatMul_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden4/MatMul_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden3/Elu_1_grad/EluGrad\n",
      "train/gradients/dnn/hidden3/BiasAdd_1_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden3/BiasAdd_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden3/BiasAdd_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden3/BiasAdd_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden3/MatMul_1_grad/MatMul\n",
      "train/gradients/dnn/hidden3/MatMul_1_grad/MatMul_1\n",
      "train/gradients/dnn/hidden3/MatMul_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden3/MatMul_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden3/MatMul_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden2/Elu_1_grad/EluGrad\n",
      "train/gradients/dnn/hidden2/BiasAdd_1_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden2/BiasAdd_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden2/BiasAdd_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden2/BiasAdd_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden2/MatMul_1_grad/MatMul\n",
      "train/gradients/dnn/hidden2/MatMul_1_grad/MatMul_1\n",
      "train/gradients/dnn/hidden2/MatMul_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden2/MatMul_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden2/MatMul_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden1/Elu_1_grad/EluGrad\n",
      "train/gradients/dnn/hidden1/BiasAdd_1_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden1/BiasAdd_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden1/BiasAdd_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden1/BiasAdd_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden1/MatMul_1_grad/MatMul\n",
      "train/gradients/dnn/hidden1/MatMul_1_grad/MatMul_1\n",
      "train/gradients/dnn/hidden1/MatMul_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden1/MatMul_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden1/MatMul_1_grad/tuple/control_dependency_1\n",
      "train/beta1_power_1/initial_value\n",
      "train/beta1_power_1\n",
      "train/beta1_power_1/Assign\n",
      "train/beta1_power_1/read\n",
      "train/beta2_power_1/initial_value\n",
      "train/beta2_power_1\n",
      "train/beta2_power_1/Assign\n",
      "train/beta2_power_1/read\n",
      "hidden1/kernel_1/Adam/Initializer/zeros/shape_as_tensor\n",
      "hidden1/kernel_1/Adam/Initializer/zeros/Const\n",
      "hidden1/kernel_1/Adam/Initializer/zeros\n",
      "hidden1/kernel_1/Adam\n",
      "hidden1/kernel_1/Adam/Assign\n",
      "hidden1/kernel_1/Adam/read\n",
      "hidden1/kernel_1/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "hidden1/kernel_1/Adam_1/Initializer/zeros/Const\n",
      "hidden1/kernel_1/Adam_1/Initializer/zeros\n",
      "hidden1/kernel_1/Adam_1\n",
      "hidden1/kernel_1/Adam_1/Assign\n",
      "hidden1/kernel_1/Adam_1/read\n",
      "hidden1/bias_1/Adam/Initializer/zeros\n",
      "hidden1/bias_1/Adam\n",
      "hidden1/bias_1/Adam/Assign\n",
      "hidden1/bias_1/Adam/read\n",
      "hidden1/bias_1/Adam_1/Initializer/zeros\n",
      "hidden1/bias_1/Adam_1\n",
      "hidden1/bias_1/Adam_1/Assign\n",
      "hidden1/bias_1/Adam_1/read\n",
      "hidden2/kernel_1/Adam/Initializer/zeros/shape_as_tensor\n",
      "hidden2/kernel_1/Adam/Initializer/zeros/Const\n",
      "hidden2/kernel_1/Adam/Initializer/zeros\n",
      "hidden2/kernel_1/Adam\n",
      "hidden2/kernel_1/Adam/Assign\n",
      "hidden2/kernel_1/Adam/read\n",
      "hidden2/kernel_1/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "hidden2/kernel_1/Adam_1/Initializer/zeros/Const\n",
      "hidden2/kernel_1/Adam_1/Initializer/zeros\n",
      "hidden2/kernel_1/Adam_1\n",
      "hidden2/kernel_1/Adam_1/Assign\n",
      "hidden2/kernel_1/Adam_1/read\n",
      "hidden2/bias_1/Adam/Initializer/zeros\n",
      "hidden2/bias_1/Adam\n",
      "hidden2/bias_1/Adam/Assign\n",
      "hidden2/bias_1/Adam/read\n",
      "hidden2/bias_1/Adam_1/Initializer/zeros\n",
      "hidden2/bias_1/Adam_1\n",
      "hidden2/bias_1/Adam_1/Assign\n",
      "hidden2/bias_1/Adam_1/read\n",
      "hidden3/kernel_1/Adam/Initializer/zeros/shape_as_tensor\n",
      "hidden3/kernel_1/Adam/Initializer/zeros/Const\n",
      "hidden3/kernel_1/Adam/Initializer/zeros\n",
      "hidden3/kernel_1/Adam\n",
      "hidden3/kernel_1/Adam/Assign\n",
      "hidden3/kernel_1/Adam/read\n",
      "hidden3/kernel_1/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "hidden3/kernel_1/Adam_1/Initializer/zeros/Const\n",
      "hidden3/kernel_1/Adam_1/Initializer/zeros\n",
      "hidden3/kernel_1/Adam_1\n",
      "hidden3/kernel_1/Adam_1/Assign\n",
      "hidden3/kernel_1/Adam_1/read\n",
      "hidden3/bias_1/Adam/Initializer/zeros\n",
      "hidden3/bias_1/Adam\n",
      "hidden3/bias_1/Adam/Assign\n",
      "hidden3/bias_1/Adam/read\n",
      "hidden3/bias_1/Adam_1/Initializer/zeros\n",
      "hidden3/bias_1/Adam_1\n",
      "hidden3/bias_1/Adam_1/Assign\n",
      "hidden3/bias_1/Adam_1/read\n",
      "hidden4/kernel_1/Adam/Initializer/zeros/shape_as_tensor\n",
      "hidden4/kernel_1/Adam/Initializer/zeros/Const\n",
      "hidden4/kernel_1/Adam/Initializer/zeros\n",
      "hidden4/kernel_1/Adam\n",
      "hidden4/kernel_1/Adam/Assign\n",
      "hidden4/kernel_1/Adam/read\n",
      "hidden4/kernel_1/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "hidden4/kernel_1/Adam_1/Initializer/zeros/Const\n",
      "hidden4/kernel_1/Adam_1/Initializer/zeros\n",
      "hidden4/kernel_1/Adam_1\n",
      "hidden4/kernel_1/Adam_1/Assign\n",
      "hidden4/kernel_1/Adam_1/read\n",
      "hidden4/bias_1/Adam/Initializer/zeros\n",
      "hidden4/bias_1/Adam\n",
      "hidden4/bias_1/Adam/Assign\n",
      "hidden4/bias_1/Adam/read\n",
      "hidden4/bias_1/Adam_1/Initializer/zeros\n",
      "hidden4/bias_1/Adam_1\n",
      "hidden4/bias_1/Adam_1/Assign\n",
      "hidden4/bias_1/Adam_1/read\n",
      "hidden5/kernel_1/Adam/Initializer/zeros/shape_as_tensor\n",
      "hidden5/kernel_1/Adam/Initializer/zeros/Const\n",
      "hidden5/kernel_1/Adam/Initializer/zeros\n",
      "hidden5/kernel_1/Adam\n",
      "hidden5/kernel_1/Adam/Assign\n",
      "hidden5/kernel_1/Adam/read\n",
      "hidden5/kernel_1/Adam_1/Initializer/zeros/shape_as_tensor\n",
      "hidden5/kernel_1/Adam_1/Initializer/zeros/Const\n",
      "hidden5/kernel_1/Adam_1/Initializer/zeros\n",
      "hidden5/kernel_1/Adam_1\n",
      "hidden5/kernel_1/Adam_1/Assign\n",
      "hidden5/kernel_1/Adam_1/read\n",
      "hidden5/bias_1/Adam/Initializer/zeros\n",
      "hidden5/bias_1/Adam\n",
      "hidden5/bias_1/Adam/Assign\n",
      "hidden5/bias_1/Adam/read\n",
      "hidden5/bias_1/Adam_1/Initializer/zeros\n",
      "hidden5/bias_1/Adam_1\n",
      "hidden5/bias_1/Adam_1/Assign\n",
      "hidden5/bias_1/Adam_1/read\n",
      "outputs/kernel_1/Adam/Initializer/zeros\n",
      "outputs/kernel_1/Adam\n",
      "outputs/kernel_1/Adam/Assign\n",
      "outputs/kernel_1/Adam/read\n",
      "outputs/kernel_1/Adam_1/Initializer/zeros\n",
      "outputs/kernel_1/Adam_1\n",
      "outputs/kernel_1/Adam_1/Assign\n",
      "outputs/kernel_1/Adam_1/read\n",
      "outputs/bias_1/Adam/Initializer/zeros\n",
      "outputs/bias_1/Adam\n",
      "outputs/bias_1/Adam/Assign\n",
      "outputs/bias_1/Adam/read\n",
      "outputs/bias_1/Adam_1/Initializer/zeros\n",
      "outputs/bias_1/Adam_1\n",
      "outputs/bias_1/Adam_1/Assign\n",
      "outputs/bias_1/Adam_1/read\n",
      "train/Adam_1/learning_rate\n",
      "train/Adam_1/beta1\n",
      "train/Adam_1/beta2\n",
      "train/Adam_1/epsilon\n",
      "train/Adam_1/update_hidden1/kernel_1/ApplyAdam\n",
      "train/Adam_1/update_hidden1/bias_1/ApplyAdam\n",
      "train/Adam_1/update_hidden2/kernel_1/ApplyAdam\n",
      "train/Adam_1/update_hidden2/bias_1/ApplyAdam\n",
      "train/Adam_1/update_hidden3/kernel_1/ApplyAdam\n",
      "train/Adam_1/update_hidden3/bias_1/ApplyAdam\n",
      "train/Adam_1/update_hidden4/kernel_1/ApplyAdam\n",
      "train/Adam_1/update_hidden4/bias_1/ApplyAdam\n",
      "train/Adam_1/update_hidden5/kernel_1/ApplyAdam\n",
      "train/Adam_1/update_hidden5/bias_1/ApplyAdam\n",
      "train/Adam_1/update_outputs/kernel_1/ApplyAdam\n",
      "train/Adam_1/update_outputs/bias_1/ApplyAdam\n",
      "train/Adam_1/mul\n",
      "train/Adam_1/Assign\n",
      "train/Adam_1/mul_1\n",
      "train/Adam_1/Assign_1\n",
      "train/Adam_1\n",
      "eval/in_top_k/InTopKV2_1/k\n",
      "eval/in_top_k/InTopKV2_1\n",
      "eval/Cast_1\n",
      "eval/Const_1\n",
      "eval/Mean_1\n",
      "init_1\n",
      "save/filename_1/input\n",
      "save/filename_1\n",
      "save/Const_1\n",
      "save/SaveV2_1/tensor_names\n",
      "save/SaveV2_1/shape_and_slices\n",
      "save/SaveV2_1\n",
      "save/control_dependency_1\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_38\n",
      "save/Assign_39\n",
      "save/Assign_40\n",
      "save/Assign_41\n",
      "save/Assign_42\n",
      "save/Assign_43\n",
      "save/Assign_44\n",
      "save/Assign_45\n",
      "save/Assign_46\n",
      "save/Assign_47\n",
      "save/Assign_48\n",
      "save/Assign_49\n",
      "save/Assign_50\n",
      "save/Assign_51\n",
      "save/Assign_52\n",
      "save/Assign_53\n",
      "save/Assign_54\n",
      "save/Assign_55\n",
      "save/Assign_56\n",
      "save/Assign_57\n",
      "save/Assign_58\n",
      "save/Assign_59\n",
      "save/Assign_60\n",
      "save/Assign_61\n",
      "save/Assign_62\n",
      "save/Assign_63\n",
      "save/Assign_64\n",
      "save/Assign_65\n",
      "save/Assign_66\n",
      "save/Assign_67\n",
      "save/Assign_68\n",
      "save/Assign_69\n",
      "save/Assign_70\n",
      "save/Assign_71\n",
      "save/Assign_72\n",
      "save/Assign_73\n",
      "save/Assign_74\n",
      "save/Assign_75\n",
      "save/Assign_76\n",
      "save/Assign_77\n",
      "save/Assign_78\n",
      "save/Assign_79\n",
      "save/Assign_80\n",
      "save/Assign_81\n",
      "save/Assign_82\n",
      "save/Assign_83\n",
      "save/Assign_84\n",
      "save/Assign_85\n",
      "save/Assign_86\n",
      "save/Assign_87\n",
      "save/Assign_88\n",
      "save/Assign_89\n",
      "save/Assign_90\n",
      "save/Assign_91\n",
      "save/Assign_92\n",
      "save/Assign_93\n",
      "save/Assign_94\n",
      "save/Assign_95\n",
      "save/Assign_96\n",
      "save/Assign_97\n",
      "save/Assign_98\n",
      "save/Assign_99\n",
      "save/Assign_100\n",
      "save/Assign_101\n",
      "save/Assign_102\n",
      "save/Assign_103\n",
      "save/Assign_104\n",
      "save/Assign_105\n",
      "save/Assign_106\n",
      "save/Assign_107\n",
      "save/Assign_108\n",
      "save/Assign_109\n",
      "save/Assign_110\n",
      "save/Assign_111\n",
      "save/Assign_112\n",
      "save/Assign_113\n",
      "save/restore_all_1\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"models/chap11_exercise.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss/loss:0\")\n",
    "Y_proba = tf.get_default_graph().get_tensor_by_name(\"dnn/Y_proba:0\")\n",
    "logits = Y_proba.op.inputs[0]\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "X_train2_full = X_train[y_train >= 5]\n",
    "y_train2_full = y_train[y_train >= 5] - 5\n",
    "X_valid2_full = X_valid[y_valid >= 5]\n",
    "y_valid2_full = y_valid[y_valid >= 5] - 5\n",
    "X_test2 = X_test[y_test >= 5]\n",
    "y_test2 = y_test[y_test >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/chap11_exercise.ckpt\n",
      "0\t검증 세트 손실: 1.357602\t최선의 손실: 1.357602\t정확도: 40.00%\n",
      "1\t검증 세트 손실: 1.183432\t최선의 손실: 1.183432\t정확도: 58.67%\n",
      "2\t검증 세트 손실: 1.217803\t최선의 손실: 1.183432\t정확도: 53.33%\n",
      "3\t검증 세트 손실: 1.204619\t최선의 손실: 1.183432\t정확도: 56.00%\n",
      "4\t검증 세트 손실: 1.140937\t최선의 손실: 1.140937\t정확도: 56.67%\n",
      "5\t검증 세트 손실: 1.185161\t최선의 손실: 1.140937\t정확도: 55.33%\n",
      "6\t검증 세트 손실: 1.089114\t최선의 손실: 1.089114\t정확도: 60.00%\n",
      "7\t검증 세트 손실: 1.171013\t최선의 손실: 1.089114\t정확도: 58.00%\n",
      "8\t검증 세트 손실: 1.288682\t최선의 손실: 1.089114\t정확도: 50.00%\n",
      "9\t검증 세트 손실: 1.109311\t최선의 손실: 1.089114\t정확도: 57.33%\n",
      "10\t검증 세트 손실: 1.134755\t최선의 손실: 1.089114\t정확도: 56.00%\n",
      "11\t검증 세트 손실: 1.142793\t최선의 손실: 1.089114\t정확도: 54.00%\n",
      "12\t검증 세트 손실: 1.194862\t최선의 손실: 1.089114\t정확도: 52.00%\n",
      "13\t검증 세트 손실: 1.225149\t최선의 손실: 1.089114\t정확도: 52.00%\n",
      "14\t검증 세트 손실: 1.162608\t최선의 손실: 1.089114\t정확도: 56.00%\n",
      "15\t검증 세트 손실: 1.249606\t최선의 손실: 1.089114\t정확도: 55.33%\n",
      "16\t검증 세트 손실: 1.197919\t최선의 손실: 1.089114\t정확도: 56.67%\n",
      "17\t검증 세트 손실: 1.072532\t최선의 손실: 1.072532\t정확도: 64.00%\n",
      "18\t검증 세트 손실: 1.152226\t최선의 손실: 1.072532\t정확도: 55.33%\n",
      "19\t검증 세트 손실: 1.215641\t최선의 손실: 1.072532\t정확도: 55.33%\n",
      "20\t검증 세트 손실: 1.204286\t최선의 손실: 1.072532\t정확도: 55.33%\n",
      "21\t검증 세트 손실: 1.101986\t최선의 손실: 1.072532\t정확도: 60.67%\n",
      "22\t검증 세트 손실: 1.114266\t최선의 손실: 1.072532\t정확도: 57.33%\n",
      "23\t검증 세트 손실: 1.136834\t최선의 손실: 1.072532\t정확도: 63.33%\n",
      "24\t검증 세트 손실: 1.195184\t최선의 손실: 1.072532\t정확도: 54.00%\n",
      "25\t검증 세트 손실: 1.097964\t최선의 손실: 1.072532\t정확도: 62.00%\n",
      "26\t검증 세트 손실: 1.158243\t최선의 손실: 1.072532\t정확도: 57.33%\n",
      "27\t검증 세트 손실: 1.163322\t최선의 손실: 1.072532\t정확도: 54.67%\n",
      "28\t검증 세트 손실: 1.122081\t최선의 손실: 1.072532\t정확도: 61.33%\n",
      "29\t검증 세트 손실: 1.118385\t최선의 손실: 1.072532\t정확도: 58.67%\n",
      "30\t검증 세트 손실: 1.086066\t최선의 손실: 1.072532\t정확도: 62.00%\n",
      "31\t검증 세트 손실: 1.096934\t최선의 손실: 1.072532\t정확도: 66.00%\n",
      "32\t검증 세트 손실: 1.132986\t최선의 손실: 1.072532\t정확도: 58.67%\n",
      "33\t검증 세트 손실: 1.095975\t최선의 손실: 1.072532\t정확도: 60.00%\n",
      "34\t검증 세트 손실: 1.090075\t최선의 손실: 1.072532\t정확도: 66.00%\n",
      "35\t검증 세트 손실: 1.056452\t최선의 손실: 1.056452\t정확도: 65.33%\n",
      "36\t검증 세트 손실: 1.095339\t최선의 손실: 1.056452\t정확도: 60.00%\n",
      "37\t검증 세트 손실: 1.128881\t최선의 손실: 1.056452\t정확도: 62.00%\n",
      "38\t검증 세트 손실: 1.154084\t최선의 손실: 1.056452\t정확도: 61.33%\n",
      "39\t검증 세트 손실: 1.100734\t최선의 손실: 1.056452\t정확도: 60.67%\n",
      "40\t검증 세트 손실: 1.079168\t최선의 손실: 1.056452\t정확도: 63.33%\n",
      "41\t검증 세트 손실: 1.113833\t최선의 손실: 1.056452\t정확도: 59.33%\n",
      "42\t검증 세트 손실: 1.080299\t최선의 손실: 1.056452\t정확도: 64.00%\n",
      "43\t검증 세트 손실: 1.059557\t최선의 손실: 1.056452\t정확도: 62.67%\n",
      "44\t검증 세트 손실: 1.123067\t최선의 손실: 1.056452\t정확도: 62.00%\n",
      "45\t검증 세트 손실: 1.185175\t최선의 손실: 1.056452\t정확도: 58.00%\n",
      "46\t검증 세트 손실: 1.043178\t최선의 손실: 1.043178\t정확도: 67.33%\n",
      "47\t검증 세트 손실: 1.247590\t최선의 손실: 1.043178\t정확도: 60.00%\n",
      "48\t검증 세트 손실: 1.085106\t최선의 손실: 1.043178\t정확도: 60.67%\n",
      "49\t검증 세트 손실: 1.250785\t최선의 손실: 1.043178\t정확도: 55.33%\n",
      "50\t검증 세트 손실: 1.188239\t최선의 손실: 1.043178\t정확도: 63.33%\n",
      "51\t검증 세트 손실: 1.096536\t최선의 손실: 1.043178\t정확도: 64.67%\n",
      "52\t검증 세트 손실: 1.080787\t최선의 손실: 1.043178\t정확도: 65.33%\n",
      "53\t검증 세트 손실: 1.089835\t최선의 손실: 1.043178\t정확도: 60.00%\n",
      "54\t검증 세트 손실: 1.085391\t최선의 손실: 1.043178\t정확도: 66.67%\n",
      "55\t검증 세트 손실: 1.222933\t최선의 손실: 1.043178\t정확도: 59.33%\n",
      "56\t검증 세트 손실: 1.102650\t최선의 손실: 1.043178\t정확도: 64.67%\n",
      "57\t검증 세트 손실: 1.134348\t최선의 손실: 1.043178\t정확도: 58.67%\n",
      "58\t검증 세트 손실: 1.068475\t최선의 손실: 1.043178\t정확도: 64.00%\n",
      "59\t검증 세트 손실: 1.030383\t최선의 손실: 1.030383\t정확도: 71.33%\n",
      "60\t검증 세트 손실: 1.111663\t최선의 손실: 1.030383\t정확도: 63.33%\n",
      "61\t검증 세트 손실: 1.189098\t최선의 손실: 1.030383\t정확도: 60.00%\n",
      "62\t검증 세트 손실: 1.046359\t최선의 손실: 1.030383\t정확도: 66.67%\n",
      "63\t검증 세트 손실: 1.147486\t최선의 손실: 1.030383\t정확도: 64.00%\n",
      "64\t검증 세트 손실: 1.064369\t최선의 손실: 1.030383\t정확도: 64.67%\n",
      "65\t검증 세트 손실: 1.066502\t최선의 손실: 1.030383\t정확도: 63.33%\n",
      "66\t검증 세트 손실: 1.052030\t최선의 손실: 1.030383\t정확도: 64.00%\n",
      "67\t검증 세트 손실: 1.020023\t최선의 손실: 1.020023\t정확도: 66.67%\n",
      "68\t검증 세트 손실: 1.078156\t최선의 손실: 1.020023\t정확도: 66.67%\n",
      "69\t검증 세트 손실: 1.032902\t최선의 손실: 1.020023\t정확도: 66.00%\n",
      "70\t검증 세트 손실: 1.017936\t최선의 손실: 1.017936\t정확도: 69.33%\n",
      "71\t검증 세트 손실: 1.164401\t최선의 손실: 1.017936\t정확도: 60.67%\n",
      "72\t검증 세트 손실: 1.059563\t최선의 손실: 1.017936\t정확도: 67.33%\n",
      "73\t검증 세트 손실: 1.057376\t최선의 손실: 1.017936\t정확도: 70.00%\n",
      "74\t검증 세트 손실: 1.121358\t최선의 손실: 1.017936\t정확도: 62.67%\n",
      "75\t검증 세트 손실: 1.113039\t최선의 손실: 1.017936\t정확도: 64.00%\n",
      "76\t검증 세트 손실: 1.033384\t최선의 손실: 1.017936\t정확도: 68.00%\n",
      "77\t검증 세트 손실: 1.067340\t최선의 손실: 1.017936\t정확도: 69.33%\n",
      "78\t검증 세트 손실: 1.082473\t최선의 손실: 1.017936\t정확도: 62.00%\n",
      "79\t검증 세트 손실: 1.045935\t최선의 손실: 1.017936\t정확도: 64.67%\n",
      "80\t검증 세트 손실: 1.066287\t최선의 손실: 1.017936\t정확도: 64.67%\n",
      "81\t검증 세트 손실: 1.021209\t최선의 손실: 1.017936\t정확도: 68.00%\n",
      "82\t검증 세트 손실: 1.104792\t최선의 손실: 1.017936\t정확도: 62.67%\n",
      "83\t검증 세트 손실: 1.152581\t최선의 손실: 1.017936\t정확도: 62.00%\n",
      "84\t검증 세트 손실: 1.065110\t최선의 손실: 1.017936\t정확도: 71.33%\n",
      "85\t검증 세트 손실: 1.004111\t최선의 손실: 1.004111\t정확도: 71.33%\n",
      "86\t검증 세트 손실: 1.103127\t최선의 손실: 1.004111\t정확도: 64.67%\n",
      "87\t검증 세트 손실: 1.149125\t최선의 손실: 1.004111\t정확도: 60.67%\n",
      "88\t검증 세트 손실: 1.070162\t최선의 손실: 1.004111\t정확도: 62.67%\n",
      "89\t검증 세트 손실: 1.126472\t최선의 손실: 1.004111\t정확도: 64.00%\n",
      "90\t검증 세트 손실: 1.065203\t최선의 손실: 1.004111\t정확도: 68.67%\n",
      "91\t검증 세트 손실: 1.011178\t최선의 손실: 1.004111\t정확도: 70.00%\n",
      "92\t검증 세트 손실: 1.055096\t최선의 손실: 1.004111\t정확도: 68.67%\n",
      "93\t검증 세트 손실: 1.168636\t최선의 손실: 1.004111\t정확도: 62.00%\n",
      "94\t검증 세트 손실: 1.107876\t최선의 손실: 1.004111\t정확도: 69.33%\n",
      "95\t검증 세트 손실: 1.092528\t최선의 손실: 1.004111\t정확도: 69.33%\n",
      "96\t검증 세트 손실: 1.055137\t최선의 손실: 1.004111\t정확도: 67.33%\n",
      "97\t검증 세트 손실: 1.104263\t최선의 손실: 1.004111\t정확도: 65.33%\n",
      "98\t검증 세트 손실: 1.103803\t최선의 손실: 1.004111\t정확도: 65.33%\n",
      "99\t검증 세트 손실: 1.009833\t최선의 손실: 1.004111\t정확도: 71.33%\n",
      "100\t검증 세트 손실: 1.033832\t최선의 손실: 1.004111\t정확도: 71.33%\n",
      "101\t검증 세트 손실: 1.085038\t최선의 손실: 1.004111\t정확도: 67.33%\n",
      "102\t검증 세트 손실: 1.158826\t최선의 손실: 1.004111\t정확도: 66.67%\n",
      "103\t검증 세트 손실: 1.203226\t최선의 손실: 1.004111\t정확도: 64.00%\n",
      "104\t검증 세트 손실: 1.124595\t최선의 손실: 1.004111\t정확도: 61.33%\n",
      "105\t검증 세트 손실: 1.135211\t최선의 손실: 1.004111\t정확도: 62.00%\n",
      "조기 종료!\n",
      "전체 훈련 시간: 1.7s\n",
      "INFO:tensorflow:Restoring parameters from models/chap11_exercise_five.ckpt\n",
      "최종 테스트 정확도: 68.22%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"models/chap11_exercise.ckpt\")\n",
    "\n",
    "    t0 = time.time()\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        \n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"models/chap11_exercise_five.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"조기 종료!\")\n",
    "                break\n",
    "        print(\"{}\\t검증 세트 손실: {:.6f}\\t최선의 손실: {:.6f}\\t정확도: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"전체 훈련 시간: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"models/chap11_exercise_five.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"최종 테스트 정확도: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나의 층만 사용했기 때문에 성능이 좋지는 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden5/Elu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/chap11_exercise.ckpt\n",
      "0\t검증 세트 손실: 1.219045\t최선의 손실: 1.219045\t정확도: 55.33%\n",
      "1\t검증 세트 손실: 1.168979\t최선의 손실: 1.168979\t정확도: 57.33%\n",
      "2\t검증 세트 손실: 1.220361\t최선의 손실: 1.168979\t정확도: 52.67%\n",
      "3\t검증 세트 손실: 1.134673\t최선의 손실: 1.134673\t정확도: 54.00%\n",
      "4\t검증 세트 손실: 1.127449\t최선의 손실: 1.127449\t정확도: 58.00%\n",
      "5\t검증 세트 손실: 1.141214\t최선의 손실: 1.127449\t정확도: 60.67%\n",
      "6\t검증 세트 손실: 1.176387\t최선의 손실: 1.127449\t정확도: 56.00%\n",
      "7\t검증 세트 손실: 1.181723\t최선의 손실: 1.127449\t정확도: 54.67%\n",
      "8\t검증 세트 손실: 1.189654\t최선의 손실: 1.127449\t정확도: 48.67%\n",
      "9\t검증 세트 손실: 1.082460\t최선의 손실: 1.082460\t정확도: 62.67%\n",
      "10\t검증 세트 손실: 1.144644\t최선의 손실: 1.082460\t정확도: 57.33%\n",
      "11\t검증 세트 손실: 1.217727\t최선의 손실: 1.082460\t정확도: 61.33%\n",
      "12\t검증 세트 손실: 1.135005\t최선의 손실: 1.082460\t정확도: 58.00%\n",
      "13\t검증 세트 손실: 1.098785\t최선의 손실: 1.082460\t정확도: 62.67%\n",
      "14\t검증 세트 손실: 1.072092\t최선의 손실: 1.072092\t정확도: 58.00%\n",
      "15\t검증 세트 손실: 1.160124\t최선의 손실: 1.072092\t정확도: 57.33%\n",
      "16\t검증 세트 손실: 1.191662\t최선의 손실: 1.072092\t정확도: 53.33%\n",
      "17\t검증 세트 손실: 1.094844\t최선의 손실: 1.072092\t정확도: 64.00%\n",
      "18\t검증 세트 손실: 1.103728\t최선의 손실: 1.072092\t정확도: 60.00%\n",
      "19\t검증 세트 손실: 1.099616\t최선의 손실: 1.072092\t정확도: 62.67%\n",
      "20\t검증 세트 손실: 1.094663\t최선의 손실: 1.072092\t정확도: 61.33%\n",
      "21\t검증 세트 손실: 1.142141\t최선의 손실: 1.072092\t정확도: 62.00%\n",
      "22\t검증 세트 손실: 1.148983\t최선의 손실: 1.072092\t정확도: 58.00%\n",
      "23\t검증 세트 손실: 1.169322\t최선의 손실: 1.072092\t정확도: 58.00%\n",
      "24\t검증 세트 손실: 1.161844\t최선의 손실: 1.072092\t정확도: 56.00%\n",
      "25\t검증 세트 손실: 1.207336\t최선의 손실: 1.072092\t정확도: 57.33%\n",
      "26\t검증 세트 손실: 1.081395\t최선의 손실: 1.072092\t정확도: 64.67%\n",
      "27\t검증 세트 손실: 1.092888\t최선의 손실: 1.072092\t정확도: 60.00%\n",
      "28\t검증 세트 손실: 1.192610\t최선의 손실: 1.072092\t정확도: 58.00%\n",
      "29\t검증 세트 손실: 1.187412\t최선의 손실: 1.072092\t정확도: 62.00%\n",
      "30\t검증 세트 손실: 1.066028\t최선의 손실: 1.066028\t정확도: 63.33%\n",
      "31\t검증 세트 손실: 1.134876\t최선의 손실: 1.066028\t정확도: 60.67%\n",
      "32\t검증 세트 손실: 1.105573\t최선의 손실: 1.066028\t정확도: 64.00%\n",
      "33\t검증 세트 손실: 1.171033\t최선의 손실: 1.066028\t정확도: 56.67%\n",
      "34\t검증 세트 손실: 1.222905\t최선의 손실: 1.066028\t정확도: 57.33%\n",
      "35\t검증 세트 손실: 1.223859\t최선의 손실: 1.066028\t정확도: 59.33%\n",
      "36\t검증 세트 손실: 1.172427\t최선의 손실: 1.066028\t정확도: 58.00%\n",
      "37\t검증 세트 손실: 1.072712\t최선의 손실: 1.066028\t정확도: 63.33%\n",
      "38\t검증 세트 손실: 1.122275\t최선의 손실: 1.066028\t정확도: 58.67%\n",
      "39\t검증 세트 손실: 1.096092\t최선의 손실: 1.066028\t정확도: 62.00%\n",
      "40\t검증 세트 손실: 1.190355\t최선의 손실: 1.066028\t정확도: 56.67%\n",
      "41\t검증 세트 손실: 1.148140\t최선의 손실: 1.066028\t정확도: 58.00%\n",
      "42\t검증 세트 손실: 1.105303\t최선의 손실: 1.066028\t정확도: 64.67%\n",
      "43\t검증 세트 손실: 1.099523\t최선의 손실: 1.066028\t정확도: 66.00%\n",
      "44\t검증 세트 손실: 1.047113\t최선의 손실: 1.047113\t정확도: 62.00%\n",
      "45\t검증 세트 손실: 1.069543\t최선의 손실: 1.047113\t정확도: 66.67%\n",
      "46\t검증 세트 손실: 1.045273\t최선의 손실: 1.045273\t정확도: 68.67%\n",
      "47\t검증 세트 손실: 1.088314\t최선의 손실: 1.045273\t정확도: 64.00%\n",
      "48\t검증 세트 손실: 1.184038\t최선의 손실: 1.045273\t정확도: 58.00%\n",
      "49\t검증 세트 손실: 1.087727\t최선의 손실: 1.045273\t정확도: 60.00%\n",
      "50\t검증 세트 손실: 1.277811\t최선의 손실: 1.045273\t정확도: 56.67%\n",
      "51\t검증 세트 손실: 1.078561\t최선의 손실: 1.045273\t정확도: 60.67%\n",
      "52\t검증 세트 손실: 1.083849\t최선의 손실: 1.045273\t정확도: 64.00%\n",
      "53\t검증 세트 손실: 1.152880\t최선의 손실: 1.045273\t정확도: 60.67%\n",
      "54\t검증 세트 손실: 1.036051\t최선의 손실: 1.036051\t정확도: 66.67%\n",
      "55\t검증 세트 손실: 1.079879\t최선의 손실: 1.036051\t정확도: 66.67%\n",
      "56\t검증 세트 손실: 1.069881\t최선의 손실: 1.036051\t정확도: 63.33%\n",
      "57\t검증 세트 손실: 1.046677\t최선의 손실: 1.036051\t정확도: 64.67%\n",
      "58\t검증 세트 손실: 1.061208\t최선의 손실: 1.036051\t정확도: 66.00%\n",
      "59\t검증 세트 손실: 1.120719\t최선의 손실: 1.036051\t정확도: 66.00%\n",
      "60\t검증 세트 손실: 1.057527\t최선의 손실: 1.036051\t정확도: 63.33%\n",
      "61\t검증 세트 손실: 1.150449\t최선의 손실: 1.036051\t정확도: 58.00%\n",
      "62\t검증 세트 손실: 1.110453\t최선의 손실: 1.036051\t정확도: 61.33%\n",
      "63\t검증 세트 손실: 1.047497\t최선의 손실: 1.036051\t정확도: 64.67%\n",
      "64\t검증 세트 손실: 1.059434\t최선의 손실: 1.036051\t정확도: 66.67%\n",
      "65\t검증 세트 손실: 1.171291\t최선의 손실: 1.036051\t정확도: 61.33%\n",
      "66\t검증 세트 손실: 1.025970\t최선의 손실: 1.025970\t정확도: 66.00%\n",
      "67\t검증 세트 손실: 1.099795\t최선의 손실: 1.025970\t정확도: 67.33%\n",
      "68\t검증 세트 손실: 1.100606\t최선의 손실: 1.025970\t정확도: 62.67%\n",
      "69\t검증 세트 손실: 1.111787\t최선의 손실: 1.025970\t정확도: 64.00%\n",
      "70\t검증 세트 손실: 1.182414\t최선의 손실: 1.025970\t정확도: 60.00%\n",
      "71\t검증 세트 손실: 1.016197\t최선의 손실: 1.016197\t정확도: 65.33%\n",
      "72\t검증 세트 손실: 1.064276\t최선의 손실: 1.016197\t정확도: 68.67%\n",
      "73\t검증 세트 손실: 1.024774\t최선의 손실: 1.016197\t정확도: 72.00%\n",
      "74\t검증 세트 손실: 1.038135\t최선의 손실: 1.016197\t정확도: 66.67%\n",
      "75\t검증 세트 손실: 1.059735\t최선의 손실: 1.016197\t정확도: 64.67%\n",
      "76\t검증 세트 손실: 1.065945\t최선의 손실: 1.016197\t정확도: 63.33%\n",
      "77\t검증 세트 손실: 1.060642\t최선의 손실: 1.016197\t정확도: 65.33%\n",
      "78\t검증 세트 손실: 1.015998\t최선의 손실: 1.015998\t정확도: 64.67%\n",
      "79\t검증 세트 손실: 1.138460\t최선의 손실: 1.015998\t정확도: 58.67%\n",
      "80\t검증 세트 손실: 1.158534\t최선의 손실: 1.015998\t정확도: 66.00%\n",
      "81\t검증 세트 손실: 1.171723\t최선의 손실: 1.015998\t정확도: 64.00%\n",
      "82\t검증 세트 손실: 1.028160\t최선의 손실: 1.015998\t정확도: 69.33%\n",
      "83\t검증 세트 손실: 1.147842\t최선의 손실: 1.015998\t정확도: 60.67%\n",
      "84\t검증 세트 손실: 1.109895\t최선의 손실: 1.015998\t정확도: 66.67%\n",
      "85\t검증 세트 손실: 1.128220\t최선의 손실: 1.015998\t정확도: 64.67%\n",
      "86\t검증 세트 손실: 1.134026\t최선의 손실: 1.015998\t정확도: 64.00%\n",
      "87\t검증 세트 손실: 1.138165\t최선의 손실: 1.015998\t정확도: 64.00%\n",
      "88\t검증 세트 손실: 1.076076\t최선의 손실: 1.015998\t정확도: 69.33%\n",
      "89\t검증 세트 손실: 1.067824\t최선의 손실: 1.015998\t정확도: 64.67%\n",
      "90\t검증 세트 손실: 1.049181\t최선의 손실: 1.015998\t정확도: 65.33%\n",
      "91\t검증 세트 손실: 1.056678\t최선의 손실: 1.015998\t정확도: 68.00%\n",
      "92\t검증 세트 손실: 1.200732\t최선의 손실: 1.015998\t정확도: 62.00%\n",
      "93\t검증 세트 손실: 1.076332\t최선의 손실: 1.015998\t정확도: 64.00%\n",
      "94\t검증 세트 손실: 1.040058\t최선의 손실: 1.015998\t정확도: 64.67%\n",
      "95\t검증 세트 손실: 1.184609\t최선의 손실: 1.015998\t정확도: 64.67%\n",
      "96\t검증 세트 손실: 1.167361\t최선의 손실: 1.015998\t정확도: 60.00%\n",
      "97\t검증 세트 손실: 1.069896\t최선의 손실: 1.015998\t정확도: 66.00%\n",
      "98\t검증 세트 손실: 1.042224\t최선의 손실: 1.015998\t정확도: 71.33%\n",
      "조기 종료!\n",
      "전체 훈련 시간: 1.2s\n",
      "INFO:tensorflow:Restoring parameters from models/chap11_exercise_five.ckpt\n",
      "최종 테스트 정확도: 67.43%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"models/chap11_exercise.ckpt\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    hidden5_train = hidden5_out.eval(feed_dict={X: X_train2, y: y_train2})\n",
    "    hidden5_valid = hidden5_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            h5_batch, y_batch = hidden5_train[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={hidden5_out: h5_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={hidden5_out: hidden5_valid, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"models/chap11_exercise_five.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"조기 종료!\")\n",
    "                break\n",
    "        print(\"{}\\t검증 세트 손실: {:.6f}\\t최선의 손실: {:.6f}\\t정확도: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"전체 훈련 시간: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"models/chap11_exercise_five.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"최종 테스트 정확도: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이전과 거의 동일한 코드로m 모델을 훈련합니다. 다른 점은 시작할 때 (훈련 세트와 검증 세트 모두) 동결층의 맨 꼭대기 층의 출력을 계산해서 캐싱하는 것입니다. 이렇게 하면 거의 1.5에서 3배 정도 훈련이 빨라집니다(시스템에 따라 이 수치는 매우 달라집니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"models/chap11_exercise.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden4_out = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden4/Elu:0\")\n",
    "logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"new_logits\")\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "four_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/chap11_exercise.ckpt\n",
      "0\t검증 세트 손실: 1.381814\t최선의 손실: 1.381814\t정확도: 49.33%\n",
      "1\t검증 세트 손실: 1.081485\t최선의 손실: 1.081485\t정확도: 60.67%\n",
      "2\t검증 세트 손실: 1.198649\t최선의 손실: 1.081485\t정확도: 48.67%\n",
      "3\t검증 세트 손실: 1.149890\t최선의 손실: 1.081485\t정확도: 57.33%\n",
      "4\t검증 세트 손실: 1.088721\t최선의 손실: 1.081485\t정확도: 56.00%\n",
      "5\t검증 세트 손실: 1.033008\t최선의 손실: 1.033008\t정확도: 62.67%\n",
      "6\t검증 세트 손실: 1.051053\t최선의 손실: 1.033008\t정확도: 62.67%\n",
      "7\t검증 세트 손실: 1.106173\t최선의 손실: 1.033008\t정확도: 56.67%\n",
      "8\t검증 세트 손실: 1.070067\t최선의 손실: 1.033008\t정확도: 56.67%\n",
      "9\t검증 세트 손실: 1.077142\t최선의 손실: 1.033008\t정확도: 61.33%\n",
      "10\t검증 세트 손실: 1.016346\t최선의 손실: 1.016346\t정확도: 68.00%\n",
      "11\t검증 세트 손실: 0.991142\t최선의 손실: 0.991142\t정확도: 64.00%\n",
      "12\t검증 세트 손실: 0.931112\t최선의 손실: 0.931112\t정확도: 70.67%\n",
      "13\t검증 세트 손실: 1.032602\t최선의 손실: 0.931112\t정확도: 63.33%\n",
      "14\t검증 세트 손실: 1.051768\t최선의 손실: 0.931112\t정확도: 63.33%\n",
      "15\t검증 세트 손실: 0.980843\t최선의 손실: 0.931112\t정확도: 67.33%\n",
      "16\t검증 세트 손실: 0.941269\t최선의 손실: 0.931112\t정확도: 64.00%\n",
      "17\t검증 세트 손실: 1.113663\t최선의 손실: 0.931112\t정확도: 58.67%\n",
      "18\t검증 세트 손실: 1.010767\t최선의 손실: 0.931112\t정확도: 65.33%\n",
      "19\t검증 세트 손실: 1.061080\t최선의 손실: 0.931112\t정확도: 63.33%\n",
      "20\t검증 세트 손실: 1.326104\t최선의 손실: 0.931112\t정확도: 60.00%\n",
      "21\t검증 세트 손실: 0.942782\t최선의 손실: 0.931112\t정확도: 67.33%\n",
      "22\t검증 세트 손실: 0.935243\t최선의 손실: 0.931112\t정확도: 67.33%\n",
      "23\t검증 세트 손실: 0.940078\t최선의 손실: 0.931112\t정확도: 64.67%\n",
      "24\t검증 세트 손실: 0.906241\t최선의 손실: 0.906241\t정확도: 66.00%\n",
      "25\t검증 세트 손실: 0.964877\t최선의 손실: 0.906241\t정확도: 66.67%\n",
      "26\t검증 세트 손실: 0.917144\t최선의 손실: 0.906241\t정확도: 65.33%\n",
      "27\t검증 세트 손실: 0.885746\t최선의 손실: 0.885746\t정확도: 74.67%\n",
      "28\t검증 세트 손실: 0.933813\t최선의 손실: 0.885746\t정확도: 68.67%\n",
      "29\t검증 세트 손실: 0.951269\t최선의 손실: 0.885746\t정확도: 70.00%\n",
      "30\t검증 세트 손실: 0.895169\t최선의 손실: 0.885746\t정확도: 71.33%\n",
      "31\t검증 세트 손실: 0.913047\t최선의 손실: 0.885746\t정확도: 68.67%\n",
      "32\t검증 세트 손실: 0.905084\t최선의 손실: 0.885746\t정확도: 70.67%\n",
      "33\t검증 세트 손실: 1.002810\t최선의 손실: 0.885746\t정확도: 67.33%\n",
      "34\t검증 세트 손실: 0.929417\t최선의 손실: 0.885746\t정확도: 68.67%\n",
      "35\t검증 세트 손실: 0.857732\t최선의 손실: 0.857732\t정확도: 72.67%\n",
      "36\t검증 세트 손실: 0.909070\t최선의 손실: 0.857732\t정확도: 69.33%\n",
      "37\t검증 세트 손실: 0.932977\t최선의 손실: 0.857732\t정확도: 68.67%\n",
      "38\t검증 세트 손실: 0.946103\t최선의 손실: 0.857732\t정확도: 70.00%\n",
      "39\t검증 세트 손실: 0.874127\t최선의 손실: 0.857732\t정확도: 73.33%\n",
      "40\t검증 세트 손실: 0.916748\t최선의 손실: 0.857732\t정확도: 72.67%\n",
      "41\t검증 세트 손실: 0.862812\t최선의 손실: 0.857732\t정확도: 75.33%\n",
      "42\t검증 세트 손실: 0.935563\t최선의 손실: 0.857732\t정확도: 68.00%\n",
      "43\t검증 세트 손실: 0.986369\t최선의 손실: 0.857732\t정확도: 69.33%\n",
      "44\t검증 세트 손실: 0.887203\t최선의 손실: 0.857732\t정확도: 72.00%\n",
      "45\t검증 세트 손실: 0.921927\t최선의 손실: 0.857732\t정확도: 72.67%\n",
      "46\t검증 세트 손실: 0.878990\t최선의 손실: 0.857732\t정확도: 74.00%\n",
      "47\t검증 세트 손실: 0.886848\t최선의 손실: 0.857732\t정확도: 71.33%\n",
      "48\t검증 세트 손실: 0.859870\t최선의 손실: 0.857732\t정확도: 74.67%\n",
      "49\t검증 세트 손실: 0.881313\t최선의 손실: 0.857732\t정확도: 70.67%\n",
      "50\t검증 세트 손실: 1.055257\t최선의 손실: 0.857732\t정확도: 68.00%\n",
      "51\t검증 세트 손실: 0.997174\t최선의 손실: 0.857732\t정확도: 66.67%\n",
      "52\t검증 세트 손실: 0.927526\t최선의 손실: 0.857732\t정확도: 70.00%\n",
      "53\t검증 세트 손실: 0.894129\t최선의 손실: 0.857732\t정확도: 70.67%\n",
      "54\t검증 세트 손실: 0.853470\t최선의 손실: 0.853470\t정확도: 74.67%\n",
      "55\t검증 세트 손실: 0.920769\t최선의 손실: 0.853470\t정확도: 71.33%\n",
      "56\t검증 세트 손실: 1.051759\t최선의 손실: 0.853470\t정확도: 65.33%\n",
      "57\t검증 세트 손실: 0.916084\t최선의 손실: 0.853470\t정확도: 72.67%\n",
      "58\t검증 세트 손실: 0.924257\t최선의 손실: 0.853470\t정확도: 68.67%\n",
      "59\t검증 세트 손실: 0.902358\t최선의 손실: 0.853470\t정확도: 72.00%\n",
      "60\t검증 세트 손실: 0.962678\t최선의 손실: 0.853470\t정확도: 66.67%\n",
      "61\t검증 세트 손실: 0.899528\t최선의 손실: 0.853470\t정확도: 69.33%\n",
      "62\t검증 세트 손실: 0.883389\t최선의 손실: 0.853470\t정확도: 73.33%\n",
      "63\t검증 세트 손실: 0.953333\t최선의 손실: 0.853470\t정확도: 71.33%\n",
      "64\t검증 세트 손실: 0.880488\t최선의 손실: 0.853470\t정확도: 72.00%\n",
      "65\t검증 세트 손실: 0.871852\t최선의 손실: 0.853470\t정확도: 72.67%\n",
      "66\t검증 세트 손실: 0.855803\t최선의 손실: 0.853470\t정확도: 75.33%\n",
      "67\t검증 세트 손실: 0.864328\t최선의 손실: 0.853470\t정확도: 74.67%\n",
      "68\t검증 세트 손실: 0.918677\t최선의 손실: 0.853470\t정확도: 73.33%\n",
      "69\t검증 세트 손실: 0.912282\t최선의 손실: 0.853470\t정확도: 74.67%\n",
      "70\t검증 세트 손실: 0.914230\t최선의 손실: 0.853470\t정확도: 73.33%\n",
      "71\t검증 세트 손실: 0.972987\t최선의 손실: 0.853470\t정확도: 68.00%\n",
      "72\t검증 세트 손실: 0.976183\t최선의 손실: 0.853470\t정확도: 68.00%\n",
      "73\t검증 세트 손실: 0.891446\t최선의 손실: 0.853470\t정확도: 73.33%\n",
      "74\t검증 세트 손실: 0.859270\t최선의 손실: 0.853470\t정확도: 76.67%\n",
      "조기 종료!\n",
      "INFO:tensorflow:Restoring parameters from models/chap11_exercise_four.ckpt\n",
      "최종 테스트 정확도: 72.70%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"models/chap11_exercise.ckpt\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = four_frozen_saver.save(sess, \"models/chap11_exercise_four.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"조기 종료!\")\n",
    "                break\n",
    "        print(\"{}\\t검증 세트 손실: {:.6f}\\t최선의 손실: {:.6f}\\t정확도: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    four_frozen_saver.restore(sess, \"models/chap11_exercise_four.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"최종 테스트 정확도: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 훌륭하지는 않지만 더 나아졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "unfrozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam3\")\n",
    "training_op = optimizer.minimize(loss, var_list=unfrozen_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "two_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/chap11_exercise_four.ckpt\n",
      "0\t검증 세트 손실: 1.560328\t최선의 손실: 1.560328\t정확도: 62.00%\n",
      "1\t검증 세트 손실: 1.122247\t최선의 손실: 1.122247\t정확도: 68.00%\n",
      "2\t검증 세트 손실: 0.939205\t최선의 손실: 0.939205\t정확도: 70.67%\n",
      "3\t검증 세트 손실: 1.047074\t최선의 손실: 0.939205\t정확도: 73.33%\n",
      "4\t검증 세트 손실: 0.942020\t최선의 손실: 0.939205\t정확도: 76.00%\n",
      "5\t검증 세트 손실: 0.827583\t최선의 손실: 0.827583\t정확도: 78.00%\n",
      "6\t검증 세트 손실: 0.896122\t최선의 손실: 0.827583\t정확도: 76.00%\n",
      "7\t검증 세트 손실: 1.104361\t최선의 손실: 0.827583\t정확도: 78.00%\n",
      "8\t검증 세트 손실: 1.336321\t최선의 손실: 0.827583\t정확도: 73.33%\n",
      "9\t검증 세트 손실: 0.991482\t최선의 손실: 0.827583\t정확도: 76.00%\n",
      "10\t검증 세트 손실: 1.123702\t최선의 손실: 0.827583\t정확도: 80.67%\n",
      "11\t검증 세트 손실: 1.009997\t최선의 손실: 0.827583\t정확도: 78.67%\n",
      "12\t검증 세트 손실: 0.994052\t최선의 손실: 0.827583\t정확도: 77.33%\n",
      "13\t검증 세트 손실: 1.200181\t최선의 손실: 0.827583\t정확도: 75.33%\n",
      "14\t검증 세트 손실: 1.038967\t최선의 손실: 0.827583\t정확도: 75.33%\n",
      "15\t검증 세트 손실: 1.067715\t최선의 손실: 0.827583\t정확도: 78.67%\n",
      "16\t검증 세트 손실: 1.306437\t최선의 손실: 0.827583\t정확도: 79.33%\n",
      "17\t검증 세트 손실: 1.115694\t최선의 손실: 0.827583\t정확도: 77.33%\n",
      "18\t검증 세트 손실: 1.637854\t최선의 손실: 0.827583\t정확도: 74.67%\n",
      "19\t검증 세트 손실: 1.420412\t최선의 손실: 0.827583\t정확도: 78.00%\n",
      "20\t검증 세트 손실: 1.033130\t최선의 손실: 0.827583\t정확도: 79.33%\n",
      "21\t검증 세트 손실: 0.923207\t최선의 손실: 0.827583\t정확도: 82.00%\n",
      "22\t검증 세트 손실: 1.185972\t최선의 손실: 0.827583\t정확도: 78.67%\n",
      "23\t검증 세트 손실: 1.477783\t최선의 손실: 0.827583\t정확도: 83.33%\n",
      "24\t검증 세트 손실: 0.881633\t최선의 손실: 0.827583\t정확도: 79.33%\n",
      "25\t검증 세트 손실: 1.821315\t최선의 손실: 0.827583\t정확도: 76.67%\n",
      "조기 종료!\n",
      "INFO:tensorflow:Restoring parameters from models/chap11_exercise_two_frozen.ckpt\n",
      "최종 테스트 정확도: 73.73%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    four_frozen_saver.restore(sess, \"models/chap11_exercise_four.ckpt\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = two_frozen_saver.save(sess, \"models/chap11_exercise_two_frozen.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"조기 종료!\")\n",
    "                break\n",
    "        print(\"{}\\t검증 세트 손실: {:.6f}\\t최선의 손실: {:.6f}\\t정확도: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    two_frozen_saver.restore(sess, \"models/chap11_exercise_two_frozen.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"최종 테스트 정확도: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능이 그렇게 좋아지지는 않았다. 모든 층을 동결 해제하고 정확도를 계산해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam_4\")\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "zero_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/chap11_exercise_two_frozen.ckpt\n",
      "0\t검증 세트 손실: 1.092992\t최선의 손실: 1.092992\t정확도: 82.00%\n",
      "1\t검증 세트 손실: 1.448133\t최선의 손실: 1.092992\t정확도: 80.00%\n",
      "2\t검증 세트 손실: 0.893574\t최선의 손실: 0.893574\t정확도: 90.67%\n",
      "3\t검증 세트 손실: 0.725983\t최선의 손실: 0.725983\t정확도: 91.33%\n",
      "4\t검증 세트 손실: 0.911656\t최선의 손실: 0.725983\t정확도: 88.67%\n",
      "5\t검증 세트 손실: 0.588267\t최선의 손실: 0.588267\t정확도: 92.00%\n",
      "6\t검증 세트 손실: 0.879624\t최선의 손실: 0.588267\t정확도: 91.33%\n",
      "7\t검증 세트 손실: 1.328020\t최선의 손실: 0.588267\t정확도: 86.00%\n",
      "8\t검증 세트 손실: 1.250800\t최선의 손실: 0.588267\t정확도: 88.67%\n",
      "9\t검증 세트 손실: 1.453489\t최선의 손실: 0.588267\t정확도: 92.00%\n",
      "10\t검증 세트 손실: 1.497060\t최선의 손실: 0.588267\t정확도: 91.33%\n",
      "11\t검증 세트 손실: 1.642997\t최선의 손실: 0.588267\t정확도: 91.33%\n",
      "12\t검증 세트 손실: 1.682036\t최선의 손실: 0.588267\t정확도: 91.33%\n",
      "13\t검증 세트 손실: 1.712296\t최선의 손실: 0.588267\t정확도: 91.33%\n",
      "14\t검증 세트 손실: 1.726808\t최선의 손실: 0.588267\t정확도: 91.33%\n",
      "15\t검증 세트 손실: 1.734523\t최선의 손실: 0.588267\t정확도: 91.33%\n",
      "16\t검증 세트 손실: 1.738936\t최선의 손실: 0.588267\t정확도: 91.33%\n",
      "17\t검증 세트 손실: 1.742714\t최선의 손실: 0.588267\t정확도: 91.33%\n",
      "18\t검증 세트 손실: 1.748992\t최선의 손실: 0.588267\t정확도: 92.00%\n",
      "19\t검증 세트 손실: 1.756209\t최선의 손실: 0.588267\t정확도: 92.00%\n",
      "20\t검증 세트 손실: 1.765610\t최선의 손실: 0.588267\t정확도: 92.00%\n",
      "21\t검증 세트 손실: 1.776197\t최선의 손실: 0.588267\t정확도: 92.00%\n",
      "22\t검증 세트 손실: 1.780646\t최선의 손실: 0.588267\t정확도: 92.00%\n",
      "23\t검증 세트 손실: 1.791786\t최선의 손실: 0.588267\t정확도: 92.00%\n",
      "24\t검증 세트 손실: 1.795223\t최선의 손실: 0.588267\t정확도: 92.00%\n",
      "25\t검증 세트 손실: 1.804514\t최선의 손실: 0.588267\t정확도: 92.00%\n",
      "조기 종료!\n",
      "INFO:tensorflow:Restoring parameters from models/chap11_exercise_two_frozen.ckpt\n",
      "최종 테스트 정확도: 90.06%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    two_frozen_saver.restore(sess, \"models/chap11_exercise_two_frozen.ckpt\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = zero_frozen_saver.save(sess, \"models/chap11_exercise_two_frozen.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"조기 종료!\")\n",
    "                break\n",
    "        print(\"{}\\t검증 세트 손실: {:.6f}\\t최선의 손실: {:.6f}\\t정확도: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    zero_frozen_saver.restore(sess, \"models/chap11_exercise_two_frozen.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"최종 테스트 정확도: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음부터 훈련시킨 DNN과 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t검증 세트 손실: 0.472236\t최선의 손실: 0.472236\t정확도: 88.00%\n",
      "1\t검증 세트 손실: 0.657322\t최선의 손실: 0.472236\t정확도: 84.00%\n",
      "2\t검증 세트 손실: 0.555275\t최선의 손실: 0.472236\t정확도: 83.33%\n",
      "3\t검증 세트 손실: 1.002466\t최선의 손실: 0.472236\t정확도: 82.67%\n",
      "4\t검증 세트 손실: 0.618466\t최선의 손실: 0.472236\t정확도: 89.33%\n",
      "5\t검증 세트 손실: 0.724875\t최선의 손실: 0.472236\t정확도: 90.67%\n",
      "6\t검증 세트 손실: 0.924811\t최선의 손실: 0.472236\t정확도: 87.33%\n",
      "7\t검증 세트 손실: 1.568413\t최선의 손실: 0.472236\t정확도: 83.33%\n",
      "8\t검증 세트 손실: 2.048452\t최선의 손실: 0.472236\t정확도: 71.33%\n",
      "9\t검증 세트 손실: 0.855954\t최선의 손실: 0.472236\t정확도: 90.00%\n",
      "10\t검증 세트 손실: 0.706350\t최선의 손실: 0.472236\t정확도: 83.33%\n",
      "11\t검증 세트 손실: 3.180080\t최선의 손실: 0.472236\t정확도: 79.33%\n",
      "12\t검증 세트 손실: 0.630881\t최선의 손실: 0.472236\t정확도: 89.33%\n",
      "13\t검증 세트 손실: 1.021825\t최선의 손실: 0.472236\t정확도: 91.33%\n",
      "14\t검증 세트 손실: 1.657358\t최선의 손실: 0.472236\t정확도: 82.00%\n",
      "15\t검증 세트 손실: 0.927330\t최선의 손실: 0.472236\t정확도: 91.33%\n",
      "16\t검증 세트 손실: 0.989501\t최선의 손실: 0.472236\t정확도: 92.00%\n",
      "17\t검증 세트 손실: 1.000915\t최선의 손실: 0.472236\t정확도: 92.00%\n",
      "18\t검증 세트 손실: 1.016654\t최선의 손실: 0.472236\t정확도: 92.00%\n",
      "19\t검증 세트 손실: 1.018881\t최선의 손실: 0.472236\t정확도: 92.00%\n",
      "20\t검증 세트 손실: 1.030726\t최선의 손실: 0.472236\t정확도: 92.00%\n",
      "21\t검증 세트 손실: 1.046657\t최선의 손실: 0.472236\t정확도: 92.00%\n",
      "조기 종료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ud803\\Anaconda3\\envs\\hands_on_ml\\lib\\site-packages\\sklearn\\base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function elu at 0x00000218F707DF78>,\n",
       "              batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x00000218820A9F48>,\n",
       "              learning_rate=0.01, n_hidden_layers=4, n_neurons=100,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_staet=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_5_to_9 = DNNClassifier(n_hidden_layers=4)\n",
    "dnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=1000, X_valid=X_valid2, y_valid=y_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8559967084961942"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf_5_to_9.predict(X_test2)\n",
    "accuracy_score(y_test2, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래도 어느정도 발전은 있다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
