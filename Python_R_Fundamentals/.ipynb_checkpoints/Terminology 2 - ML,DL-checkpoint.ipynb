{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gradient Descent\n",
    "\n",
    "https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/\n",
    "\n",
    "## - What it is\n",
    "**Gradient descent is an algorithm that minimizes functions. **\n",
    "\n",
    "1. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function.\n",
    "\n",
    "2. Gradient descent is an optimization algorithm often used **for finding the weights or coefficients of machine learning algorithms**, such as ANN and logistic regression.\n",
    "\n",
    "## - How it works\n",
    "\n",
    "1. It works by having the model make predictions on training data and using the error on the predictions to update the model in such a way as to reduce the error. \n",
    "\n",
    "2. The goal of the algorithm is to find model parameters (e.g. coefficients or weights) that minimize the error of the model on the training dataset. It does this **by making changes to the model that move it along a gradient or slope of errors down toward a minimum error value.** This gives the algorithm its name of “gradient descent.”\n",
    "3. This iterative minimization is achieved using calculus, taking steps in the negative direction of the function gradient.\n",
    "\n",
    "## - Other terminologies (Cont)\n",
    "\n",
    "1. Convexity \n",
    "2. Performance\n",
    "3. Convergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/\n",
    "\n",
    "## - Types of Gradient Descent\n",
    "### 1) Stochastic Gradient Descent (SGD)\n",
    "SGD is a variation of the gradient descent algorithm that calculates the error and **updates the model for each example in the training dataset.**\n",
    "\n",
    "The update of the model for each training exaple means that SGD is often called an **online machine learning algorithm.*\n",
    "\n",
    "-**Upsides**\n",
    "\n",
    "- Insight into the performance of the model and the rate of improvement\n",
    "\n",
    "-**Downsides**\n",
    "\n",
    "- Updating the model so frequently is more computationally expensive. \n",
    "\n",
    "\n",
    "### 2) Batch Gradient Descent\n",
    "\n",
    "Batch Gradient Descent **only updates the model after all training examples have been evaluated.**\n",
    "\n",
    "One cycle through the entire training dataset is called a **training epoch.*\n",
    "\n",
    "Therefore, it is often said that batch gradient descent performs model updates at the end of each training epoch.\n",
    "\n",
    "-**Upsides**\n",
    "\n",
    "- More computationally efficient than SGD.\n",
    "\n",
    "- The separation of the calculation of prediction errors and the model update lends the algorithm to parallel processing based implementations.\n",
    "\n",
    "-**Downsides**\n",
    "\n",
    "- Premature convergence of the model to a less optimal set of parameters.\n",
    "\n",
    "- Requires the entire training dataset in memory and available to the algorithm.\n",
    "\n",
    "- Model updates, and training speed may become very slow for large datasets.\n",
    "\n",
    "\n",
    "### 3) Mini-Batch Gradient Descent\n",
    "\n",
    "Splits the dataset into small batches.\n",
    "\n",
    "Seeks to find a **balance between the robustness of SGD and the efficiency of batch gradient descent.**\n",
    "\n",
    "It is the most common implementation of gradient descent.\n",
    "\n",
    "\n",
    "-**Upsides**\n",
    "- Allows both the efficiency of not having all training data in memory and algorithm implementations.\n",
    "\n",
    "-**Downsides**\n",
    "- Requires the configuration of an additional “mini-batch size” hyperparameter.\n",
    "- Configuring Batchsize\n",
    "    - Small values give a learning process that converges quickly at the cost of noise in the training process\n",
    "    - Large values give a learning process that converges slowly with accurate estimates of the error gradient.\n",
    "    - A good default is 32.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
