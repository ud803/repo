{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Classification and logistic regression\n",
    "\n",
    "<br>\n",
    "이제 분류 문제를 다뤄보자. 분류는 $y$ 값이 이산형 값이라는 것만 제외하면 회귀 문제와 같다.\n",
    "\n",
    "일단은 $y$가 0과 1의 값을 갖는 **이진 분류(binary classification)** 문제를 살펴보겠다.\n",
    "\n",
    "예를 들어, 스팸 메일을 거르고 싶을 때, 스팸일 경우 1, 아닐 경우 0의 값을 할당한다.\n",
    "\n",
    "이때 0을 **negative class**라고 부르기도 하고, 1을 **positive class**라고 부르기도 한다.\n",
    "\n",
    "또한 이 $y^{(i)}$ 값들을 트레이닝 예제의 **라벨(label)**이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logistic Regression\n",
    "\n",
    "$y$가 이산형이라는 사실을 무시하고 분류 문제를 해결할 수 있다. 하지만 이렇게 기존의 회귀 방법을 사용하면 모델의 성능이 아주 좋지 않을 것이다.\n",
    "\n",
    "또한 $h_\\theta(x)$의 값이 1보다 크거나 0보다 작은 값을 갖는다는 것은 우리가 $y$의 값이 [0,1]인 것을 안다면 모순적으로 보인다.\n",
    "\n",
    "이 문제를 해결하기 위해 우리의 가설 $h_\\theta(x)$의 형태를 바꿔보자.\n",
    "\n",
    "<br>\n",
    "$$ h_\\theta(x) = g(\\theta^Tx) = \\frac{1}{1+e^{-\\theta^Tx}}$$\n",
    "\n",
    "$$ where, g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "<br>\n",
    "\n",
    "위 함수를 **로지스틱 함수(logistic function) 또는 시그모이드 함수(sigmoid function)**이라고 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4S4ikoUU7iIiWUjhLiKShRTuIiJZ6P8DpSWCrxKetaYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1b02647b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sigmoid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "x = np.arange(-10,10)\n",
    "y = 1/(1+np.exp(-x))\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 함수는 $z$가 무한대로 갈 때 1에 수렴하고, 음의 무한대로 갈 때 0에 수렴한다.\n",
    "\n",
    "그래서 $h(x)$의 값은 0과 1 사이에 존재하게 된다.\n",
    "\n",
    "지금은 시그모이드 함수를 있는 그대로 받아들이지만 나중에는 다른 함수도 살펴볼 것이다.\n",
    "\n",
    "우선 시그모이드 함수의 미분을 살펴보자.\n",
    "\n",
    "<br>\n",
    "$$ g^`(z) = \\frac{d}{dz}\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "$$ = \\frac{1}{(1+e^{-z})^2}(e^{-z})$$\n",
    "\n",
    "$$ = g(z)(1-g(z)$$\n",
    "<br>\n",
    "\n",
    "로지스틱 회귀 모델이 주어졌을 때, 어떻게 $\\theta$를 조율할까?\n",
    "\n",
    "앞서 최소자승법이 여러 가정하에서 최대우도법으로부터 도출되었던 것처럼, 이번에도 여러 확률 가정을 살펴보자.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$P(y = 1| x; \\theta) = h_\\theta(x)$$\n",
    "$$ P(y=0|x; \\theta) = 1 - h_\\theta(x) $$\n",
    "\n",
    "위 성질을 가정하고, $m$개의 트레이닝 예제들이 상호 독립이라고 하자.\n",
    "\n",
    "이때, 파라미터의 우도 함수를 다음과 같이 쓸 수 있다.\n",
    "\n",
    "<br>\n",
    "$ L(\\theta) $\n",
    "\n",
    "$= p(\\vec{y}|X; \\theta) $\n",
    "\n",
    "$= \\prod_{i=1}^mp(y^{(i)}|x^{(i)};\\theta)$\n",
    "\n",
    "$= \\prod_{i=1}^m (h_\\theta(x^{(i)}))^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{1-y^{(i)}}$\n",
    "<br>\n",
    "\n",
    "마찬가지로 로그를 취해서 식을 간단히 해준다.\n",
    "\n",
    "그럼 어떻게 최대 우도값을 구할까? 선형 회귀에서처럼, **경사 상승법(Gradient Ascent)**을 사용하면 된다.\n",
    "\n",
    "<br>\n",
    "$ \\frac{\\partial}{\\partial\\theta_j}l(\\theta) $\n",
    "\n",
    "$ = (y \\frac{1}{g(\\theta^Tx)}-(1-y)\\frac{1}{1-g(\\theta^Tx)})\\frac{\\partial}{\\partial\\theta_j}g(\\theta^Tx) $\n",
    "\n",
    "$ = (y-h_\\theta(x))x_j $\n",
    "\n",
    "\n",
    "결국 아래와 같은 확률 경사 상승법 공식을 얻는다.\n",
    "\n",
    "\n",
    "<br>\n",
    "$$\\theta_j := \\theta_j + \\alpha(y^{(i)}-h_\\theta(x^{(i)}))x_j^{(i)} $$\n",
    "<br>\n",
    "\n",
    "이 식을 LMS 업데이트 식에 비교하면, 식 자체는 동일한 것을 알 수 있다.\n",
    "\n",
    "하지만 $h_\\theta(x^{(i)})$가 이제는 비선형 함수이기 때문에, 같은 알고리즘은 아니다.\n",
    "\n",
    "그럼에도 불구하고 서로 다른 알고리즘과 다른 학습 문제에서 같은 공식이 도출되었다는 점은 놀랍다.\n",
    "\n",
    "단순한 우연일까, 아니면 무언가 더 심오한 이유가 있는 것일까? 나중에 GLM 모델에서 이에 답하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### 5-1. Digression : The perceptron learning algorithm\n",
    "\n",
    "잠깐 여담으로 역사적으로 관심을 받았던 알고리즘에 대해 이야기해보자.\n",
    "\n",
    "앞서 배운 로지스틱 회귀 모델이 정확히 0 또는 1의 값만 산출하도록 강제한다고 해보자. \n",
    "\n",
    "그렇게하기 위해, 다음과 같은 함수를 생각할 수 있다.\n",
    "\n",
    "<br>\n",
    "$$\n",
    "g(z) = \n",
    "\\begin{cases}\n",
    "1 & if & z\\geq 0 \\\\\n",
    "0 & if & z \\lt 0 \n",
    "\\end{cases}$$ <br>\n",
    "\n",
    "여기서 $h_\\theta(x) = g(\\theta^Tx)$로 두면, 다음과 같은 업데이트 공식을 얻는다.\n",
    "\n",
    "<br>\n",
    "$$ \\theta_j := \\theta_j + \\alpha(y^{(i)}-h_\\theta(x^{(i)}))x_j^{(i)}$$\n",
    "<br>\n",
    "\n",
    "그리고 이를 **퍼셉트론 학습 알고리즘(Perceptron learning algorithm)**이라고 부른다.\n",
    "\n",
    "1960년대에 '퍼셉트론'은 뇌에서 하나의 뉴런이 작동하는 대략적인 모델을 의미했다. \n",
    "\n",
    "비록 이 식이 우리가 배운 다른 알고리즘과 비슷하지만, 실은 아주 다른 종류의 알고리즘임을 기억해야 한다. \n",
    "\n",
    "특히 확률적인 해석이나 최대우도추정법등을 통해 퍼셉트론의 예측을 끌어내는 것은 매우 어렵다.\n",
    "\n",
    "<br>\n",
    "### 5-2. Digression : Newton's Method\n",
    "\n",
    "뉴턴이 고안한 함수가 0이 되는 값을 찾는 방법이 있다.\n",
    "\n",
    "하지만 이 방법은 여러 제약 조건 하에서만 응용 가능하고, 단순한 회귀 문제에서 사용할 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./newton.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 함수 $f(x)$가 존재한다.\n",
    "\n",
    "임의의 시작 지점 $X^{0}$에서 함수로 수직선을 그어 함수와 만나는 점 $f(X^{0})$를 찾는다.\n",
    "\n",
    "그 점에서의 접선을 그어 또다시 $X$축과 만나는 점을 찾고, 그 점을 $X^{1}$이라고 한다.\n",
    "\n",
    "위 방법을 반복한다.\n",
    "\n",
    "이때, 접선의 기울기는 해당 접점에서의 미분값이고, $X_n - X_n+1 = \\Delta$라고 하면,\n",
    "\n",
    "$f^\\prime(\\theta^{0}) = \\frac{f(\\theta^0)}{\\Delta} $ 가 된다. 식을 정리하면 다음 일반식을 얻는다.\n",
    "\n",
    "\n",
    "<br>\n",
    "$$ \\theta^{(t+1)} = \\theta^{(t)} - \\frac{f(\\theta^{(t)})}{f^\\prime(\\theta^{(t)})}$$\n",
    "<br>\n",
    "\n",
    "이제 이 사실을 이용하여 우리가 원하는 비용함수 $L(\\theta)$의 미분값이 0이 되는 점을 찾자.\n",
    "\n",
    "<br>\n",
    "$$ \\theta^{(t+1)} = \\theta^{(t)} - \\frac{l^\\prime\\theta^{(t)})}{f^{\\prime\\prime}(\\theta^{(t)})}$$\n",
    "<br>\n",
    "\n",
    "위 과정을 반복하면 $L^\\prime(\\theta)$가 0이 되는 지점(최소 지점)을 찾을 수 있다.\n",
    "\n",
    "$\\theta$가 벡터인 경우, 일반식은 다음과 같다.\n",
    "\n",
    "<br>\n",
    "$$ \\theta^{(t+1)} = \\theta^{(t)} - H^{-1}\\nabla_\\theta L $$\n",
    "$$ H : Hessian Matrix $$\n",
    "$$ H_{ij} = \\frac{\\partial^2L}{\\partial\\theta_i\\partial\\theta_j} $$\n",
    "<br>\n",
    "\n",
    "보통 이 방법은 BGD보다 빠르게 수렴하고, 반복 횟수가 적지만, 연산량은 아주 클 수 있다.\n",
    "\n",
    "왜냐하면 $n-n$ Hessian 행렬을 찾고 역행렬을 계산해야 하기 때문이다.\n",
    "\n",
    "하지만 $n$ 이 그렇게 크지 않다면, 전반적인 속도는 훨씬 빠르다.\n",
    "\n",
    "이 방법이 로지스틱 회귀의 로그 우도함수를 최대화 할 때 쓰일 경우, 그 모형은 **Fisher Scoring** 이라고도 불린다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Part 3. Generalized Linear Models (GLM)\n",
    "\n",
    "이제까지, 우리는 회귀 예제와 분류 예제를 살펴보았다. \n",
    "\n",
    "이번 장에서는 사실 이 두 방법들이 더 큰 모델인 **일반 선형 모델(GLM)**의 한 가족이라는 것을 보일 것이다.\n",
    "\n",
    "또한 GLM에 속한 다른 모델들도 살펴보겠다.\n",
    "\n",
    "<br>\n",
    "### 6. The exponential family\n",
    "\n",
    "우선 지수분포족에 속하는 분포부터 살펴보자. 어떤 분포가 지수족에 속할 때, 다음의 형식으로 쓰일 수 있다.\n",
    "\n",
    "<br>\n",
    "$$ p(y;\\eta) = b(y)exp(\\eta^TT(y)-a(\\eta))$$\n",
    "<br>\n",
    "\n",
    "여기서, $\\eta$는 **자연 파라미터(natural, canonical parameter)**라고 부른다.\n",
    "\n",
    "$T(y)$는 **충분통계량(sufficient statistic)**이 되고, $a(\\eta)$는 **로그분할함수(log partition function)** 이다.\n",
    "\n",
    "$T$, $a$, $b$의 선택이 $\\eta$에 의해 만들어지는 분포군을 정의한다.  마찬가지로 $\\eta$를 변화시키면 같은 분포군 내에서 서로 다른 분포를 얻는다.\n",
    "\n",
    "한 예로 버눌리와 정규분포는 지수 분포족에 속한다는 것을 증명해보자.\n",
    "\n",
    "###### Bernoulli\n",
    "평균이 $\\phi$인 버눌리 분포는,\n",
    "\n",
    "<br>\n",
    "$$ p(y=1; \\phi); p(y=0; \\phi) = 1 - \\phi $$\n",
    "<br>\n",
    "\n",
    "$\\phi$를 변화시키면 평균이 서로 다른 버눌리 분포를 얻게 된다.\n",
    "\n",
    "또한,\n",
    "\n",
    "<br>\n",
    "$ p(y;\\phi) $\n",
    "\n",
    "$= \\phi^y(1-\\phi)^{1-y}$\n",
    "\n",
    "$ = exp(ylog\\phi + (1-y)log(1-\\phi))$\n",
    "\n",
    "$ = exp( ( log(\\frac{\\phi}{1-\\phi}))y + log(1-\\phi))$\n",
    "\n",
    "여기서, 자연 파라미터는 $\\eta = log(\\phi/(1-\\phi))$가 되고, \n",
    "\n",
    "여기서 $T(y) = y$, $a(\\eta) = -log(1-\\phi)$, $b(y) = 1$ 이 된다.\n",
    "\n",
    "그리고 신기하게도 $\\phi$에 대해 정리하면,\n",
    "\n",
    "$$ \\phi = \\frac{1}{1+e^{-\\eta}}$$\n",
    "\n",
    "우리는 앞서 배웠던 시그모이드 함수를 얻게 된다!\n",
    "\n",
    "###### Gaussian \n",
    "\n",
    "명심할 것은 $\\sigma^2$의 값이 $\\theta$와 $h_\\theta(x)$의 선택에 영향을 미치지 않는다는 사실이다.\n",
    "\n",
    "따라서 계산의 간편함을 위해 $\\sigma^2$을 1로 두자.\n",
    "\n",
    "<br>\n",
    "$ p(y; \\mu)$\n",
    "\n",
    "$ = \\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{1}{2}(y-\\mu)^2)$\n",
    "\n",
    "$ = \\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{1}{2}y^2)exp(\\mu y-\\frac{1}{2}\\mu^2) $\n",
    "<br>\n",
    "\n",
    "여기서 $n = \\mu$, $T(y) = y$, $a(\\eta) = \\eta^2/2$, $b(y) = (1/\\sqrt{2\\pi})exp(-y^2/2)$가 된다.\n",
    "\n",
    "<br>\n",
    "이 둘을 제외하고도 지수 분포군에 속하는 것들은 많다.\n",
    "\n",
    "다항분포, 포아송, 감마, 베타, 드리쉴레 등 많은 것들이 포함된다. 이제 다양한 모형들을 만드는 방법을 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### 7. Constructing GLMs\n",
    "\n",
    "당신이 가게에 도착하는 고객들의 수를 추정하는 모형을 만든다고 하자. 사용하는 특징으로는 가게의 홍보 상태, 날씨, 요일 등이 있다.\n",
    "\n",
    "보통 포아송 분포가 방문객의 수에 적합한 모형이라고 알려져 있다. 그리고 다행히도, 포아송은 지수 분포군에 속하기 때문에, GLM을 적용할 수 있다.\n",
    "\n",
    "더 일반적으로, $x$의 함수로서 $y$를 예측하는 분류나 회귀 문제를 생각해보자. 이 문제에 대한 GLM을 구상해보면, $x$가 주어졌을 때 $y$의 분포에 대해 다음과 같은 3가지 가정을 내릴 수 있다.\n",
    "\n",
    "1. $y|x;\\theta ~ ExponentialFamily(\\nu)$. $x$와 $\\theta$가 주어졌을 때 $y$의 분포는 파라미터 $\\nu$를 갖고 지수 분포군에 속한다.\n",
    "\n",
    "2. $x$가 주어졌을 때, 우리의 목표는 $x$가 주어졌을 때 $T(y)$이 기대값을 예측하는 것이다.\n",
    "대부분의 경우에 $T(y) = y$이고, $h(x) = E[y|x]$를 만족하길 바란다.\n",
    "\n",
    "3. 자연 파라미터 $\\nu$와 입력 변수 $x$는 선형 관계에 있다 : $\\nu = \\theta^Tx$\n",
    "\n",
    "이 세가지 가정들은 우리가 학습 알고리즘을 쉽게 도출하고, 학습을 용이하게 해주는 많은 속성들을 갖는다.\n",
    "\n",
    "#### 7-1. Ordinary Least Squares\n",
    "\n",
    "타겟 변수 $y$가 연속형인 경우, $x$가 주어졌을 때 $y$의 분포를 정규분포로 가정하자.\n",
    "\n",
    "이때, $h_\\theta(x) = \\nu = \\theta^Tx $가 된다.\n",
    "\n",
    "\n",
    "#### 7-2. Logistic Regression\n",
    "\n",
    "이번에는 이진 분류 문제이다. $y$의 값이 이진형이기 때문에, 버눌리 분포를 사용하는 것이 자연스러워 보인다. \n",
    "\n",
    "즉, $h_\\theta(x) = \\phi = \\frac{1}{1+e^{-\\theta^Tx}} $가 된다.\n",
    "\n",
    "가우시안 분포군의 canonical response function은 항등 함수이고, 버눌리의 그것은 로지스틱 함수이다.\n",
    "\n",
    "\n",
    "#### 7-3. Softmax Regression\n",
    "\n",
    "이번에는 타겟 변수 $y$가 $k$개의 값을 가지는 분류 문제를 생각해보자.\n",
    "\n",
    "예를 들어, 이메일을 스팸과 아닌 것으로 분류하지 않고 3가지로 분류하고 싶다. 우리는 다항분포를  사용해 모형을 만든다.\n",
    "\n",
    "우선 다항분포를 지수분포군으로 표현해보자.\n",
    "\n",
    "$k$개의 가능한 결과에 대해 다항분포를 만들려면, 각 결과가 나올 확률을 $\\phi_1, ... \\phi_k$로 나타낼 수 있다.\n",
    "\n",
    "하지만 이 파라미터들은 독립이 아니기 때문에, $k-1$개의 인자만을 사용할 것이다. \n",
    "\n",
    "이때 $\\phi_i = p(y=i; \\phi)$, $p(y=k; \\phi) = 1-\\sum_{i=1}^{k-1}\\phi_i $ 이다.\n",
    "\n",
    "또한 $T(y)$는 차원이 $(k-1)$인 벡터이다.\n",
    "\n",
    "$T(1) = \\Bigg[\\begin{array}{c} \n",
    "1\\\\\n",
    "0\\\\\n",
    "...\\\\\n",
    "0\n",
    "\\end{array}\\Bigg] \\hspace{1cm} T(2) = \\Bigg[\\begin{array}{c} \n",
    "0\\\\\n",
    "1\\\\\n",
    "...\\\\\n",
    "0\n",
    "\\end{array}\\Bigg] \\hspace{1cm} ... \\hspace{1cm} T(k-1) = \\Bigg[\\begin{array}{c} \n",
    "0\\\\\n",
    "0\\\\\n",
    "...\\\\\n",
    "1\n",
    "\\end{array}\\Bigg] \\hspace{1cm} $ \n",
    "\n",
    "여기서 지시함수를 사용해 표현하면, $T(y)$의 $y$번째 항이 $i$인 경우에만 1인 벡터이다.\n",
    "\n",
    "<br>\n",
    "$$T(y)_i = 1\\{y=i\\}$$\n",
    "<br>\n",
    "\n",
    "식을 정리하면, \n",
    "\n",
    "<br>\n",
    "$P(y) $\n",
    "\n",
    "$= \\phi_1^{I\\{y=1\\}}...\\phi_k^{I\\{y=k\\}} $\n",
    "\n",
    "$= \\phi_1^{T(y)_1}...\\phi_{k-1}^{T(y)_{k-1}} \\phi_k^{1-\\sum_{j=1}^{k-1}T(y)_j}$\n",
    "\n",
    "$ = b(y) exp(\\nu^TT(y) - a(\\nu)) $\n",
    "\n",
    "<br>\n",
    "$where,$\n",
    "<br>\n",
    "\n",
    "$ \\nu = \\Bigg[ \\begin{array}{c}\n",
    "log(\\phi_1/\\phi_k)\\\\\n",
    "...\\\\\n",
    "log(\\phi_{k-1}/\\phi_k)\n",
    "\\end{array}\\Bigg] \\hspace{1cm} a(\\nu) = -log(\\phi_k) \\hspace{1cm} b(y) = 1$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "이제 $\\phi$를 $\\nu$에 대해 정리하면,\n",
    "\n",
    "<br>\n",
    "$$ \\phi_i = \\frac{e^{\\nu_i}}{1+\\sum_{j=1}^{k-1}e^{\\nu_j}},\\hspace{1cm} (i=1, ..., k-1)$$\n",
    "\n",
    "$$ = \\frac{e^{\\theta_i^TX}}{1+\\sum_{j=1}^{k-1}e^{\\theta_j^TX}}$$\n",
    "<br>\n",
    "\n",
    "결국 학습 알고리즘 $h_\\theta(x)$는 다음이 된다.\n",
    "\n",
    "<br>\n",
    "$ h_\\theta(x) = E[T(y)|x; \\theta] $\n",
    "\n",
    "$ = E \\Bigg[ \\begin{array}{c}\n",
    "I\\{y=1\\}\\\\\n",
    "...\\\\\n",
    "I\\{y=k-1\\} \n",
    "\\end{array} \\Bigg|  x; \\theta\\Bigg] = \\Bigg[ \\begin{array}{c}\n",
    "\\phi_1\\\\\n",
    "...\\\\\n",
    "\\phi_{k-1} \n",
    "\\end{array} \\Bigg]$\n",
    "\n",
    "$ = \\Bigg[ \\begin{array}{c}\n",
    "\\frac{e^{\\theta_1^TX}}{1+\\sum_{j=1}^{k-1}e^{\\theta_j^TX}}\\\\\n",
    "...\\\\\n",
    "\\frac{e^{\\theta_{k-1}^TX}}{1+\\sum_{j=1}^{k-1}e^{\\theta_j^TX}} \n",
    "\\end{array} \\Bigg|  x; \\theta\\Bigg] $\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "본론으로 돌아가서, $ y \\in \\{1, ..., k\\} $ 인 분류 문제를 해결하는 상황을 가정해보자.\n",
    "\n",
    "여기서 해야할 일은 우도함수를 그린 뒤, 그 최대값을 구하는 일이다.\n",
    "\n",
    "<br>\n",
    "$ L(\\theta) $\n",
    "\n",
    "$ = \\prod_{i=1}^m p(y^{(i)} | x^{(i)}; \\theta) $\n",
    "\n",
    "$ = \\prod_{i=1}^m \\phi_1^{I\\{y^{(i)}=1\\}}...\\phi_k^{I\\{y^{(i)}=k\\}} $\n",
    "\n",
    "$ where, \\phi_i = \\frac{e^{\\theta_i^TX}}{1+\\sum_{j=1}^{k-1}e^{\\theta_j^TX}}$\n",
    "<br>\n",
    "\n",
    "이제 앞에서 한 것과 마찬가지로 로그를 취한 후 $\\theta$에 대해 미분하여 *경사상승법 혹은 뉴턴의 방법을* 통해 $\\theta$가 최대값이 되는 지점을 구하면 된다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
